<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Beam – Blogs</title><link>/blog/</link><description>Recent content in Blogs on Apache Beam</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Wed, 09 Jun 2021 09:00:00 -0700</lastBuildDate><atom:link href="/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Apache Beam 2.30.0</title><link>/blog/beam-2.30.0/</link><pubDate>Wed, 09 Jun 2021 09:00:00 -0700</pubDate><guid>/blog/beam-2.30.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.30.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2300-2021-06-09">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.30.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349978">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Legacy Read transform (non-SDF based Read) is used by default for non-FnAPI opensource runners. Use &lt;code>use_sdf_read&lt;/code> experimental flag to re-enable SDF based Read transforms (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10670">BEAM-10670&lt;/a>)&lt;/li>
&lt;li>Upgraded vendored gRPC dependency to 1.36.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11227">BEAM-11227&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Fixed the issue that WriteToBigQuery with batch file loads does not respect schema update options when there are multiple load jobs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11277">BEAM-11277&lt;/a>)&lt;/li>
&lt;li>Fixed the issue that the job didn&amp;rsquo;t properly retry since BigQuery sink swallows HttpErrors when performing streaming inserts (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12362">BEAM-12362&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Added capability to declare resource hints in Java and Python SDKs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-2085">BEAM-2085&lt;/a>)&lt;/li>
&lt;li>Added Spanner IO Performance tests for read and write in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10029">BEAM-10029&lt;/a>)&lt;/li>
&lt;li>Added support for accessing GCP PubSub Message ordering keys, message IDs and message publish timestamp in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7819">BEAM-7819&lt;/a>)&lt;/li>
&lt;li>DataFrame API: Added support for collecting DataFrame objects in interactive Beam (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11855">BEAM-11855&lt;/a>)&lt;/li>
&lt;li>DataFrame API: Added &lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/dataframe">apache_beam.examples.dataframe&lt;/a> module (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12024">BEAM-12024&lt;/a>)&lt;/li>
&lt;li>Upgraded the GCP Libraries BOM version to 20.0.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11205">BEAM-11205&lt;/a>). For Google Cloud client library versions set by this BOM, see &lt;a href="https://storage.googleapis.com/cloud-opensource-java-dashboard/com.google.cloud/libraries-bom/20.0.0/artifact_details.html">this table&lt;/a>&lt;/li>
&lt;li>Added &lt;code>sdkContainerImage&lt;/code> flag to (eventually) replace &lt;code>workerHarnessContainerImage&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12212">BEAM-12212&lt;/a>)&lt;/li>
&lt;li>Added support for Dataflow update when schemas are used (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12198">BEAM-12198&lt;/a>)&lt;/li>
&lt;li>Fixed the issue that &lt;code>ZipFiles.zipDirectory&lt;/code> leaks native JVM memory (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12220">BEAM-12220&lt;/a>)&lt;/li>
&lt;li>Fixed the issue that &lt;code>Reshuffle.withNumBuckets&lt;/code> creates &lt;code>(N*2)-1&lt;/code> buckets (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12361">BEAM-12361&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Drop support for Flink 1.8 and 1.9 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11948">BEAM-11948&lt;/a>)&lt;/li>
&lt;li>MongoDbIO: Read.withFilter() and Read.withProjection() are removed since they are deprecated since Beam 2.12.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12217">BEAM-12217&lt;/a>)&lt;/li>
&lt;li>RedisIO.readAll() was removed since it was deprecated since Beam 2.13.0. Please use RedisIO.readKeyPatterns() for the equivalent functionality (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12214">BEAM-12214&lt;/a>)&lt;/li>
&lt;li>MqttIO.create() with clientId constructor removed because it was deprecated since Beam 2.13.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12216">BEAM-12216&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.30.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to &lt;code>git shortlog&lt;/code>, the following people contributed to the 2.30.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Anant Damle, Andreas Bergmeier, Andrew Pilloud, Ankur Goenka,
Anup D, Artur Khanin, Benjamin Gonzalez, Bipin Upadhyaya, Boyuan Zhang, Brian Hulette, Bulat Shakirzyanov,
Chamikara Jayalath, Chun Yang, Daniel Kulp, Daniel Oliveira, David Cavazos, Elliotte Rusty Harold, Emily Ye,
Eric Roshan-Eisner, Evan Galpin, Fabien Caylus, Fernando Morales, Heejong Lee, Iñigo San Jose Visiers,
Isidro Martínez, Ismaël Mejía, Ke Wu, Kenneth Knowles, KevinGG, Kyle Weaver, Ludovic Post, MATTHEW Ouyang (LCL),
Mackenzie Clark, Masato Nakamura, Matthias Baetens, Max, Nicholas Azar, Ning Kang, Pablo Estrada, Patrick McCaffrey,
Quentin Sommer, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Sam Rohde, Sam Whittle, Shoaib Zafar,
Siyuan Chen, Sruthi Sree Kumar, Steve Niemitz, Sylvain Veyrié, Tomo Suzuki, Udi Meiri, Valentyn Tymofieiev,
Vitaly Terentyev, Wenbing, Xinyu Liu, Yichi Zhang, Yifan Mai, Yueyang Qiu, Yunqing Zhou, ajo thomas, brucearctor,
dmkozh, dpcollins-google, emily, jordan-moore, kileys, lostluck, masahitojp, roger-mike, sychen, tvalentyn,
vachan-shetty, yoshiki.obata&lt;/p></description></item><item><title>Blog: How to validate a Beam Release</title><link>/blog/validate-beam-release/</link><pubDate>Tue, 08 Jun 2021 00:00:01 -0800</pubDate><guid>/blog/validate-beam-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Performing new releases is a core responsibility of any software project.
It is even more important in the culture of Apache projects. Releases are
the main flow of new code / features among the community of a project.&lt;/p>
&lt;p>Beam is no exception: We aspire to keep a release cadence of about 6 weeks,
and try to work with the community to release useful new features, and to
keep Beam useful.&lt;/p>
&lt;h3 id="configure-a-java-build-to-validate-a-beam-release-candidate">Configure a Java build to validate a Beam release candidate&lt;/h3>
&lt;p>First of all, it would be useful to have a single property in your &lt;code>pom.xml&lt;/code>
where you keep the global Beam version that you&amp;rsquo;re using. Something like this
in your &lt;code>pom.xml&lt;/code>:&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">properties&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">...&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">version&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">26&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">0&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">version&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">...&lt;/span>
&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">properties&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">dependencies&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">dependency&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">groupId&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">org&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apache&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">beam&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">groupId&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">artifactId&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">sdks&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">java&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">core&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">artifactId&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">version&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">$&lt;/span>&lt;span class="o">{&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">version&lt;/span>&lt;span class="o">}&amp;lt;/&lt;/span>&lt;span class="n">version&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">dependency&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">...&lt;/span>
&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">dependencies&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Second, you can add a new profile to your &lt;code>pom.xml&lt;/code> file. In this new profile,
add a new repository with the staging repository for the new Beam release. For
Beam 2.27.0, this was &lt;code>https://repository.apache.org/content/repositories/orgapachebeam-1149/&lt;/code>.&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">profile&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">validaterelease&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">repositories&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">repository&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">apache&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">newrelease&lt;/span>&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">$&lt;/span>&lt;span class="o">{&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">release&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">repo&lt;/span>&lt;span class="o">}&amp;lt;/&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">repository&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">repositories&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="o">&amp;lt;/&lt;/span>&lt;span class="n">profile&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Once you have a &lt;code>beam.version&lt;/code> property in your &lt;code>pom.xml&lt;/code>, and a new profile
with the new release, you can run your &lt;code>mvn&lt;/code> command activating the new profile,
and the new Beam version:&lt;/p>
&lt;pre>&lt;code>mvn test -Pvalidaterelease \
-Dbeam.version=2.27.0 \
-Dbeam.release.repo=https://repository.apache.org/content/repositories/orgapachebeam-XXXX/
&lt;/code>&lt;/pre>&lt;p>This should build your project against the new release, and run basic tests.
It will allow you to run basic validations against the new Beam release.
If you find any issues, then you can share them &lt;em>before&lt;/em> the release is
finalized, so your concerns can be addressed by the community.&lt;/p>
&lt;h3 id="configuring-a-python-build-to-validate-a-beam-release-candidate">Configuring a Python build to validate a Beam release candidate&lt;/h3>
&lt;p>For Python SDK releases, you can install SDK from Pypi, by enabling the
installation of pre-release artifacts.&lt;/p>
&lt;p>First, make sure that your &lt;code>requirements.txt&lt;/code> or &lt;code>setup.py&lt;/code> files allow
for Beam versions above the current one. Something like this should install
the latest available version:&lt;/p>
&lt;pre>&lt;code>apache-beam&amp;lt;=3.0.0
&lt;/code>&lt;/pre>&lt;p>With that, you can ask &lt;code>pip&lt;/code> to install pre-release versions of Beam in your
environment:&lt;/p>
&lt;pre>&lt;code>pip install --pre apache-beam
&lt;/code>&lt;/pre>&lt;p>With that, the Beam version in your environment will be the latest release
candidate, and you can go ahead and run your tests to verify that everything
works well.&lt;/p></description></item><item><title>Blog: Apache Beam 2.29.0</title><link>/blog/beam-2.29.0/</link><pubDate>Thu, 29 Apr 2021 09:00:00 -0700</pubDate><guid>/blog/beam-2.29.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.29.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2290-2021-04-15">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.29.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349629">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Spark Classic and Portable runners officially support Spark 3 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7093">BEAM-7093&lt;/a>).&lt;/li>
&lt;li>Official Java 11 support for most runners (Dataflow, Flink, Spark) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-2530">BEAM-2530&lt;/a>).&lt;/li>
&lt;li>DataFrame API now supports GroupBy.apply (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11628">BEAM-11628&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Added support for S3 filesystem on AWS SDK V2 (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7637">BEAM-7637&lt;/a>)&lt;/li>
&lt;li>GCP BigQuery sink (file loads) uses runner determined sharding for unbounded data (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11772">BEAM-11772&lt;/a>)&lt;/li>
&lt;li>KafkaIO now recognizes the &lt;code>partition&lt;/code> property in writing records (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11806">BEAM-11806&lt;/a>)&lt;/li>
&lt;li>Support for Hadoop configuration on ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11913">BEAM-11913&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>DataFrame API now supports pandas 1.2.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11531">BEAM-11531&lt;/a>).&lt;/li>
&lt;li>Multiple DataFrame API bugfixes (&lt;a href="https://issues.apache/jira/browse/BEAM-12071">BEAM-12071&lt;/a>, &lt;a href="https://issues.apache/jira/browse/BEAM-11929">BEAM-11929&lt;/a>)&lt;/li>
&lt;li>DDL supported in SQL transforms (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11850">BEAM-11850&lt;/a>)&lt;/li>
&lt;li>Upgrade Flink runner to Flink version 1.12.2 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11941">BEAM-11941&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Deterministic coding enforced for GroupByKey and Stateful DoFns. Previously non-deterministic coding was allowed, resulting in keys not properly being grouped in some cases. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11719">BEAM-11719&lt;/a>)
To restore the old behavior, one can register &lt;code>FakeDeterministicFastPrimitivesCoder&lt;/code> with
&lt;code>beam.coders.registry.register_fallback_coder(beam.coders.coders.FakeDeterministicFastPrimitivesCoder())&lt;/code>
or use the &lt;code>allow_non_deterministic_key_coders&lt;/code> pipeline option.&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Support for Flink 1.8 and 1.9 will be removed in the next release (2.30.0) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11948">BEAM-11948&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.29.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to &lt;code>git shortlog&lt;/code>, the following people contributed to the 2.29.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alexander Chermenin, Alexey Romanenko,
Allen Pradeep Xavier, Amy Wu, Anant Damle, Andreas Bergmeier, Andrei Balici,
Andrew Pilloud, Andy Xu, Ankur Goenka, Bashir Sadjad, Benjamin Gonzalez, Boyuan
Zhang, Brian Hulette, Chamikara Jayalath, Chinmoy Mandayam, Chuck Yang,
dandy10, Daniel Collins, Daniel Oliveira, David Cavazos, David Huntsperger,
David Moravek, Dmytro Kozhevin, Emily Ye, Esun Kim, Evgeniy Belousov, Filip
Popić, Fokko Driesprong, Gris Cuevas, Heejong Lee, Ihor Indyk, Ismaël Mejía,
Jakub-Sadowski, Jan Lukavský, John Edmonds, Juan Sandoval, 谷口恵輔, Kenneth
Jung, Kenneth Knowles, KevinGG, Kiley Sok, Kyle Weaver, MabelYC, Mackenzie
Clark, Masato Nakamura, Milena Bukal, Miltos, Minbo Bae, Miraç Vuslat Başaran,
mynameborat, Nahian-Al Hasan, Nam Bui, Niel Markwick, Niels Basjes, Ning Kang,
Nir Gazit, Pablo Estrada, Ramazan Yapparov, Raphael Sanamyan, Reuven Lax, Rion
Williams, Robert Bradshaw, Robert Burke, Rui Wang, Sam Rohde, Sam Whittle,
Shehzaad Nakhoda, Shehzaad Nakhoda, Siyuan Chen, Sonam Ramchand, Steve Niemitz,
sychen, Sylvain Veyrié, Tim Robertson, Tobias Kaymak, Tomasz Szerszeń, Tomasz
Szerszeń, Tomo Suzuki, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Yichi
Zhang, Yifan Mai, Yixing Zhang, Yoshiki Obata&lt;/p></description></item><item><title>Blog: Apache Beam 2.28.0</title><link>/blog/beam-2.28.0/</link><pubDate>Mon, 22 Feb 2021 12:00:00 -0800</pubDate><guid>/blog/beam-2.28.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.28.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2280-2021-02-13">download page&lt;/a> for this release.
For more information on changes in 2.28.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349499">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Many improvements related to Parquet support (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11460">BEAM-11460&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8202">BEAM-8202&lt;/a>, and &lt;a href="https://issues.apache.org/jira/browse/BEAM-11526">BEAM-11526&lt;/a>)&lt;/li>
&lt;li>Hash Functions in BeamSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10074">BEAM-10074&lt;/a>)&lt;/li>
&lt;li>Hash functions in ZetaSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11624">BEAM-11624&lt;/a>)&lt;/li>
&lt;li>Create ApproximateDistinct using HLL Impl (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10324">BEAM-10324&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>SpannerIO supports using BigDecimal for Numeric fields (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11643">BEAM-11643&lt;/a>)&lt;/li>
&lt;li>Add Beam schema support to ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11526">BEAM-11526&lt;/a>)&lt;/li>
&lt;li>Support ParquetTable Writer (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8202">BEAM-8202&lt;/a>)&lt;/li>
&lt;li>GCP BigQuery sink (streaming inserts) uses runner determined sharding (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11408">BEAM-11408&lt;/a>)&lt;/li>
&lt;li>PubSub support types: TIMESTAMP, DATE, TIME, DATETIME (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11533">BEAM-11533&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>ParquetIO add methods &lt;em>readGenericRecords&lt;/em> and &lt;em>readFilesGenericRecords&lt;/em> can read files with an unknown schema. See &lt;a href="https://github.com/apache/beam/pull/13554">PR-13554&lt;/a> and (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11460">BEAM-11460&lt;/a>)&lt;/li>
&lt;li>Added support for thrift in KafkaTableProvider (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11482">BEAM-11482&lt;/a>)&lt;/li>
&lt;li>Added support for HadoopFormatIO to skip key/value clone (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11457">BEAM-11457&lt;/a>)&lt;/li>
&lt;li>Support Conversion to GenericRecords in Convert.to transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11571">BEAM-11571&lt;/a>).&lt;/li>
&lt;li>Support writes for Parquet Tables in Beam SQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8202">BEAM-8202&lt;/a>).&lt;/li>
&lt;li>Support reading Parquet files with unknown schema (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11460">BEAM-11460&lt;/a>)&lt;/li>
&lt;li>Support user configurable Hadoop Configuration flags for ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11527">BEAM-11527&lt;/a>)&lt;/li>
&lt;li>Expose commit_offset_in_finalize and timestamp_policy to ReadFromKafka (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11677">BEAM-11677&lt;/a>)&lt;/li>
&lt;li>S3 options does not provided to boto3 client while using FlinkRunner and Beam worker pool container (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11799">BEAM-11799&lt;/a>)&lt;/li>
&lt;li>HDFS not deduplicating identical configuration paths (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11329">BEAM-11329&lt;/a>)&lt;/li>
&lt;li>Hash Functions in BeamSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10074">BEAM-10074&lt;/a>)&lt;/li>
&lt;li>Create ApproximateDistinct using HLL Impl (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10324">BEAM-10324&lt;/a>)&lt;/li>
&lt;li>Add Beam schema support to ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11526">BEAM-11526&lt;/a>)&lt;/li>
&lt;li>Add a Deque Encoder (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11538">BEAM-11538&lt;/a>)&lt;/li>
&lt;li>Hash functions in ZetaSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11624">BEAM-11624&lt;/a>)&lt;/li>
&lt;li>Refactor ParquetTableProvider (&lt;a href="https://issues.apache.org/jira/browse/">&lt;/a>)&lt;/li>
&lt;li>Add JVM properties to JavaJobServer (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8344">BEAM-8344&lt;/a>)&lt;/li>
&lt;li>Single source of truth for supported Flink versions (&lt;a href="https://issues.apache.org/jira/browse/">&lt;/a>)&lt;/li>
&lt;li>Use metric for Python BigQuery streaming insert API latency logging (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11018">BEAM-11018&lt;/a>)&lt;/li>
&lt;li>Use metric for Java BigQuery streaming insert API latency logging (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11032">BEAM-11032&lt;/a>)&lt;/li>
&lt;li>Upgrade Flink runner to Flink versions 1.12.1 and 1.11.3 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11697">BEAM-11697&lt;/a>)&lt;/li>
&lt;li>Upgrade Beam base image to use Tensorflow 2.4.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11762">BEAM-11762&lt;/a>)&lt;/li>
&lt;li>Create Beam GCP BOM (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11665">BEAM-11665&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Java artifacts &amp;ldquo;beam-sdks-java-io-kinesis&amp;rdquo;, &amp;ldquo;beam-sdks-java-io-google-cloud-platform&amp;rdquo;, and
&amp;ldquo;beam-sdks-java-extensions-sql-zetasql&amp;rdquo; declare Guava 30.1-jre dependency (It was 25.1-jre in Beam 2.27.0).
This new Guava version may introduce dependency conflicts if your project or dependencies rely
on removed APIs. If affected, ensure to use an appropriate Guava version via &lt;code>dependencyManagement&lt;/code> in Maven and
&lt;code>force&lt;/code> in Gradle.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.28.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Allen Pradeep Xavier, Anant Damle, Artur Khanin,
Boyuan Zhang, Brian Hulette, Chamikara Jayalath, Chris Roth, Costi Ciudatu, Damon Douglas,
Daniel Collins, Daniel Oliveira, David Cavazos, David Huntsperger, Elliotte Rusty Harold,
Emily Ye, Etienne Chauchot, Etta Rapp, Evan Palmer, Eyal, Filip Krakowski, Fokko Driesprong,
Heejong Lee, Ismaël Mejía, janeliulwq, Jan Lukavský, John Edmonds, Jozef Vilcek, Kenneth Knowles
Ke Wu, kileys, Kyle Weaver, MabelYC, masahitojp, Masato Nakamura, Milena Bukal, Miraç Vuslat Başaran,
Nelson Osacky, Niel Markwick, Ning Kang, omarismail94, Pablo Estrada, Piotr Szuberski,
ramazan-yapparov, Reuven Lax, Reza Rokni, rHermes, Robert Bradshaw, Robert Burke, Robert Gruener,
Romster, Rui Wang, Sam Whittle, shehzaadn-vd, Siyuan Chen, Sonam Ramchand, Tobiasz Kędzierski,
Tomo Suzuki, tszerszen, tvalentyn, Tyson Hamilton, Udi Meiri, Xinbin Huang, Yichi Zhang,
Yifan Mai, yoshiki.obata, Yueyang Qiu, Yusaku Matsuki&lt;/p></description></item><item><title>Blog: Example to ingest data from Apache Kafka to Google Cloud Pub/Sub</title><link>/blog/kafka-to-pubsub-example/</link><pubDate>Fri, 15 Jan 2021 00:00:01 -0800</pubDate><guid>/blog/kafka-to-pubsub-example/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog post we present an example that creates a pipeline to read data from a single topic or
multiple topics from &lt;a href="https://kafka.apache.org/">Apache Kafka&lt;/a> and write data into a topic
in &lt;a href="https://cloud.google.com/pubsub">Google Pub/Sub&lt;/a>. The example provides code samples to implement
simple yet powerful pipelines and also provides an out-of-the-box solution that you can just &lt;em>&amp;rdquo;
plug&amp;rsquo;n&amp;rsquo;play&amp;rdquo;&lt;/em>.&lt;/p>
&lt;p>This end-to-end example is included
in &lt;a href="https://beam.apache.org/blog/beam-2.27.0/">Apache Beam release 2.27&lt;/a>
and can be downloaded &lt;a href="https://beam.apache.org/get-started/downloads/#2270-2020-12-22">here&lt;/a>.&lt;/p>
&lt;p>We hope you will find this example useful for setting up data pipelines between Kafka and Pub/Sub.&lt;/p>
&lt;h1 id="example-specs">Example specs&lt;/h1>
&lt;p>Supported data formats:&lt;/p>
&lt;ul>
&lt;li>Serializable plain text formats, such as JSON&lt;/li>
&lt;li>&lt;a href="https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage">PubSubMessage&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Supported input source configurations:&lt;/p>
&lt;ul>
&lt;li>Single or multiple Apache Kafka bootstrap servers&lt;/li>
&lt;li>Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection&lt;/li>
&lt;li>Secrets vault service &lt;a href="https://www.vaultproject.io/">HashiCorp Vault&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Supported destination configuration:&lt;/p>
&lt;ul>
&lt;li>Single Google Pub/Sub topic&lt;/li>
&lt;/ul>
&lt;p>In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a
source Kafka server with a source topic, and stream the text messages into specified Pub/Sub
destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed
over plaintext or SSL encrypted connection. The example supports using a single Kafka user account
to authenticate in the provided source Kafka servers and topics. To support SASL authentication over
SSL the example will need an SSL certificate location and access to a secrets vault service with
Kafka username and password, currently supporting HashiCorp Vault.&lt;/p>
&lt;h1 id="where-can-i-run-this-example">Where can I run this example?&lt;/h1>
&lt;p>There are two ways to execute the pipeline.&lt;/p>
&lt;ol>
&lt;li>Locally. This way has many options - run directly from your IntelliJ, or create &lt;code>.jar&lt;/code> file and
run it in the terminal, or use your favourite method of running Beam pipelines.&lt;/li>
&lt;li>In &lt;a href="https://cloud.google.com/">Google Cloud&lt;/a> using Google
Cloud &lt;a href="https://cloud.google.com/dataflow">Dataflow&lt;/a>:
&lt;ul>
&lt;li>With &lt;code>gcloud&lt;/code> command-line tool you can create
a &lt;a href="https://cloud.google.com/dataflow/docs/concepts/dataflow-templates">Flex Template&lt;/a>
out of this Beam example and execute it in Google Cloud Platform. &lt;em>This requires corresponding
modifications of the example to turn it into a template.&lt;/em>&lt;/li>
&lt;li>This example exists as
a &lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates/tree/master/v2/kafka-to-pubsub">Flex Template version&lt;/a>
within &lt;a href="https://github.com/GoogleCloudPlatform/DataflowTemplates">Google Cloud Dataflow Template Pipelines&lt;/a>
repository and can be run with no additional code modifications.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>Give this &lt;strong>Beam end-to-end example&lt;/strong> a try. If you are new to Beam, we hope this example will give
you more understanding on how pipelines work and look like. If you are already using Beam, we hope
some code samples in it will be useful for your use cases.&lt;/p>
&lt;p>Please
&lt;a href="https://beam.apache.org/community/contact-us/">let us know&lt;/a> if you encounter any issues.&lt;/p></description></item><item><title>Blog: Apache Beam 2.27.0</title><link>/blog/beam-2.27.0/</link><pubDate>Thu, 07 Jan 2021 12:00:00 -0800</pubDate><guid>/blog/beam-2.27.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.27.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2270-2020-12-22">download page&lt;/a> for this release.
For more information on changes in 2.27.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12349380">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java 11 Containers are now published with all Beam releases.&lt;/li>
&lt;li>There is a new transform &lt;code>ReadAllFromBigQuery&lt;/code> that can receive multiple requests to read data from BigQuery at pipeline runtime. See &lt;a href="https://github.com/apache/beam/pull/13170">PR 13170&lt;/a>, and &lt;a href="https://issues.apache.org/jira/browse/BEAM-9650">BEAM-9650&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>ReadFromMongoDB can now be used with MongoDB Atlas (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11266">BEAM-11266&lt;/a>.)&lt;/li>
&lt;li>ReadFromMongoDB/WriteToMongoDB will mask password in display_data (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11444">BEAM-11444&lt;/a>.)&lt;/li>
&lt;li>There is a new transform &lt;code>ReadAllFromBigQuery&lt;/code> that can receive multiple requests to read data from BigQuery at pipeline runtime. See &lt;a href="https://github.com/apache/beam/pull/13170">PR 13170&lt;/a>, and &lt;a href="https://issues.apache.org/jira/browse/BEAM-9650">BEAM-9650&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Beam modules that depend on Hadoop are now tested for compatibility with Hadoop 3 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8569">BEAM-8569&lt;/a>). (Hive/HCatalog pending)&lt;/li>
&lt;li>Publishing Java 11 SDK container images now supported as part of Apache Beam release process. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8106">BEAM-8106&lt;/a>)&lt;/li>
&lt;li>Added Cloud Bigtable Provider extension to Beam SQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11173">BEAM-11173&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-11373">BEAM-11373&lt;/a>)&lt;/li>
&lt;li>Added a schema provider for thrift data (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11338">BEAM-11338&lt;/a>)&lt;/li>
&lt;li>Added combiner packing pipeline optimization to Dataflow runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10641">BEAM-10641&lt;/a>)&lt;/li>
&lt;li>Added an example to ingest data from Apache Kafka to Google Pub/Sub. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11065">BEAM-11065&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>HBaseIO hbase-shaded-client dependency should be now provided by the users (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9278">BEAM-9278&lt;/a>).&lt;/li>
&lt;li>&lt;code>--region&lt;/code> flag in amazon-web-services2 was replaced by &lt;code>--awsRegion&lt;/code> (&lt;a href="https://issues.apache.org/jira/projects/BEAM/issues/BEAM-11331">BEAM-11331&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.27.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko, Aliraza Nagamia, Allen Pradeep Xavier,
Andrew Pilloud, andreyKaparulin, Ashwin Ramaswami, Boyuan Zhang, Brent Worden, Brian Hulette,
Carlos Marin, Chamikara Jayalath, Costi Ciudatu, Damon Douglas, Daniel Collins,
Daniel Oliveira, David Huntsperger, David Lu, David Moravek, David Wrede,
dennis, Dennis Yung, dpcollins-google, Emily Ye, emkornfield,
Esun Kim, Etienne Chauchot, Eugene Nikolaiev, Frank Zhao, Haizhou Zhao,
Hector Acosta, Heejong Lee, Ilya, Iñigo San Jose Visiers, InigoSJ,
Ismaël Mejía, janeliulwq, Jan Lukavský, Kamil Wasilewski, Kenneth Jung,
Kenneth Knowles, Ke Wu, kileys, Kyle Weaver, lostluck,
Matt Casters, Maximilian Michels, Michal Walenia, Mike Dewar, nehsyc,
Nelson Osacky, Niels Basjes, Ning Kang, Pablo Estrada, palmere-google,
Pawel Pasterz, Piotr Szuberski, purbanow, Reuven Lax, rHermes,
Robert Bradshaw, Robert Burke, Rui Wang, Sam Rohde, Sam Whittle,
Siyuan Chen, Tim Robertson, Tobiasz Kędzierski, tszerszen,
Valentyn Tymofieiev, Tyson Hamilton, Udi Meiri, vachan-shetty, Xinyu Liu,
Yichi Zhang, Yifan Mai, yoshiki.obata, Yueyang Qiu&lt;/p></description></item><item><title>Blog: DataFrame API Preview now Available!</title><link>/blog/dataframe-api-preview-available/</link><pubDate>Wed, 16 Dec 2020 09:09:41 -0800</pubDate><guid>/blog/dataframe-api-preview-available/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We&amp;rsquo;re excited to announce that a preview of the Beam Python SDK&amp;rsquo;s new DataFrame
API is now available in &lt;a href="https://beam.apache.org/blog/beam-2.26.0/">Beam
2.26.0&lt;/a>. Much like &lt;code>SqlTransform&lt;/code>
(&lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/extensions/sql/SqlTransform.html">Java&lt;/a>,
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.sql.html#apache_beam.transforms.sql.SqlTransform">Python&lt;/a>),
the DataFrame API gives Beam users a way to express complex
relational logic much more concisely than previously possible.&lt;/p>
&lt;h2 id="a-more-expressive-api">A more expressive API&lt;/h2>
&lt;p>Beam&amp;rsquo;s new DataFrame API aims to be compatible with the well known
&lt;a href="https://pandas.pydata.org/pandas-docs/stable/index.html">Pandas&lt;/a>
DataFrame API, with a few caveats detailed below. With this new API a simple
pipeline that reads NYC taxiride data from a CSV, performs a grouped
aggregation, and writes the output to CSV, can be expressed very concisely:&lt;/p>
&lt;pre>&lt;code>from apache_beam.dataframe.io import read_csv
with beam.Pipeline() as p:
df = p | read_csv(&amp;quot;gs://apache-beam-samples/nyc_taxi/2019/*.csv&amp;quot;,
use_ncols=['passenger_count' , 'DOLocationID'])
# Count the number of passengers dropped off per LocationID
agg = df.groupby('DOLocationID').sum()
agg.to_csv(output)
&lt;/code>&lt;/pre>&lt;p>Compare this to the same logic implemented as a conventional Beam python
pipeline with a &lt;code>CombinePerKey&lt;/code>:&lt;/p>
&lt;pre>&lt;code>with beam.Pipeline() as p:
(p | beam.io.ReadFromText(&amp;quot;gs://apache-beam-samples/nyc_taxi/2019/*.csv&amp;quot;,
skip_header_lines=1)
| beam.Map(lambda line: line.split(','))
# Parse CSV, create key - value pairs
| beam.Map(lambda splits: (int(splits[8] or 0), # DOLocationID
int(splits[3] or 0))) # passenger_count
# Sum values per key
| beam.CombinePerKey(sum)
| beam.MapTuple(lambda loc_id, pc: f'{loc_id},{pc}')
| beam.io.WriteToText(known_args.output))
&lt;/code>&lt;/pre>&lt;p>The DataFrame example is much easier to quickly inspect and understand, as it
allows you to concisely express grouped aggregations without using the low-level
&lt;code>CombinePerKey&lt;/code>.&lt;/p>
&lt;p>In addition to being more expressive, a pipeline written with the DataFrame API
can often be more efficient than a conventional Beam pipeline. This is because
the DataFrame API defers to the very efficient, columnar Pandas implementation
as much as possible.&lt;/p>
&lt;h2 id="dataframes-as-a-dsl">DataFrames as a DSL&lt;/h2>
&lt;p>You may already be aware of &lt;a href="https://beam.apache.org/documentation/dsls/sql/overview/">Beam
SQL&lt;/a>, which is
a Domain-Specific Language (DSL) built with Beam&amp;rsquo;s Java SDK. SQL is
considered a DSL because it&amp;rsquo;s possible to express a full pipeline, including IOs
and complex operations, entirely with SQL. &lt;/p>
&lt;p>Similarly, the DataFrame API is a DSL built with the Python SDK. You can see
that the above example is written without traditional Beam constructs like IOs,
ParDo, or CombinePerKey. In fact the only traditional Beam type is the Pipeline
instance! Otherwise this pipeline is written completely using the DataFrame API.
This is possible because the DataFrame API doesn&amp;rsquo;t just implement Pandas&amp;rsquo;
computation operations, it also includes IOs based on the Pandas native
implementations (&lt;code>pd.read_{csv,parquet,...}&lt;/code> and &lt;code>pd.DataFrame.to_{csv,parquet,...}&lt;/code>).&lt;/p>
&lt;p>Like SQL, it&amp;rsquo;s also possible to embed the DataFrame API into a larger pipeline
by using
&lt;a href="https://beam.apache.org/documentation/programming-guide/#what-is-a-schema">schemas&lt;/a>.
A schema-aware PCollection can be converted to a DataFrame, processed, and the
result converted back to another schema-aware PCollection. For example, if you
wanted to use traditional Beam IOs rather than one of the DataFrame IOs you
could rewrite the above pipeline like this:&lt;/p>
&lt;pre>&lt;code>from apache_beam.dataframe.convert import to_dataframe
from apache_beam.dataframe.convert import to_pcollection
with beam.Pipeline() as p:
...
schema_pc = (p | beam.ReadFromText(..)
# Use beam.Select to assign a schema
| beam.Select(DOLocationID=lambda line: int(...),
passenger_count=lambda line: int(...)))
df = to_dataframe(schema_pc)
agg = df.groupby('DOLocationID').sum()
agg_pc = to_pcollection(pc)
# agg_pc has a schema based on the structure of agg
(agg_pc | beam.Map(lambda row: f'{row.DOLocationID},{row.passenger_count}')
| beam.WriteToText(..))
&lt;/code>&lt;/pre>&lt;p>It&amp;rsquo;s also possible to use the DataFrame API by passing a function to
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.dataframe.transforms.html#apache_beam.dataframe.transforms.DataframeTransform">&lt;code>DataframeTransform&lt;/code>&lt;/a>:&lt;/p>
&lt;pre>&lt;code>from apache_beam.dataframe.transforms import DataframeTransform
with beam.Pipeline() as p:
...
| beam.Select(DOLocationID=lambda line: int(..),
passenger_count=lambda line: int(..))
| DataframeTransform(lambda df: df.groupby('DOLocationID').sum())
| beam.Map(lambda row: f'{row.DOLocationID},{row.passenger_count}')
...
&lt;/code>&lt;/pre>&lt;h2 id="caveats">Caveats&lt;/h2>
&lt;p>As hinted above, there are some differences between Beam&amp;rsquo;s DataFrame API and the
Pandas API. The most significant difference is that the Beam DataFrame API is
&lt;em>deferred&lt;/em>, just like the rest of the Beam API. This means that you can&amp;rsquo;t
&lt;code>print()&lt;/code> a DataFrame instance in order to inspect the data, because we haven&amp;rsquo;t
computed the data yet! The computation doesn&amp;rsquo;t take place until the pipeline is
&lt;code>run()&lt;/code>. Before that, we only know about the shape/schema of the result (i.e.
the names and types of the columns), and not the result itself.&lt;/p>
&lt;p>There are a few common exceptions you will likely see when attempting to use
certain Pandas operations:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>NotImplementedError:&lt;/strong> Indicates this is an operation or argument that we
haven&amp;rsquo;t had time to look at yet. We&amp;rsquo;ve tried to make as many Pandas operations
as possible available in the Preview offering of this new API, but there&amp;rsquo;s
still a long tail of operations to go.&lt;/li>
&lt;li>&lt;strong>WontImplementError:&lt;/strong> Indicates this is an operation or argument we do not
intend to support in the near-term because it&amp;rsquo;s incompatible with the Beam
model. The largest class of operations that raise this error are those that
are order sensitive (e.g. shift, cummax, cummin, head, tail, etc..). These
cannot be trivially mapped to Beam because PCollections, representing
distributed datasets, are unordered. Note that even some of these operations
&lt;em>may&lt;/em> get implemented in the future - we actually have some ideas for how we
might support order sensitive operations - but it&amp;rsquo;s a ways off.&lt;/li>
&lt;/ul>
&lt;p>Finally, it&amp;rsquo;s important to note that this is a preview of a new feature that
will get hardened over the next few Beam releases. We would love for you to try
it out now and give us some feedback, but we do not yet recommend it for use in
production workloads.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved&lt;/h2>
&lt;p>The easiest way to get involved with this effort is to try out DataFrames and
let us know what you think! You can send questions to &lt;a href="mailto:user@beam.apache.org">user@beam.apache.org&lt;/a>, or
file bug reports and feature requests in &lt;a href="https://issues.apache.org/jira">jira&lt;/a>.
In particular, it would be really helpful to know if there&amp;rsquo;s an operation we
haven&amp;rsquo;t implemented yet that you&amp;rsquo;d find useful, so that we can prioritize it.&lt;/p>
&lt;p>If you&amp;rsquo;d like to learn more about how the DataFrame API works under the hood and
get involved with the development we recommend you take a look at the
&lt;a href="http://s.apache.org/beam-dataframes">design doc&lt;/a>
and our &lt;a href="https://2020.beamsummit.org/sessions/simpler-python-pipelines/">Beam summit
presentation&lt;/a>.
From there the best way to help is to knock out some of those not implemented
operations. We&amp;rsquo;re coordinating that work in
&lt;a href="https://issues.apache.org/jira/browse/BEAM-9547">BEAM-9547&lt;/a>.&lt;/p></description></item><item><title>Blog: Splittable DoFn in Apache Beam is Ready to Use</title><link>/blog/splittable-do-fn-is-available/</link><pubDate>Mon, 14 Dec 2020 00:00:01 -0800</pubDate><guid>/blog/splittable-do-fn-is-available/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are pleased to announce that Splittable DoFn (SDF) is ready for use in the Beam Python, Java,
and Go SDKs for versions 2.25.0 and later.&lt;/p>
&lt;p>In 2017, &lt;a href="https://beam.apache.org/blog/splittable-do-fn/">Splittable DoFn Blog Post&lt;/a> proposed
to build &lt;a href="https://s.apache.org/splittable-do-fn">Splittable DoFn&lt;/a> APIs as the new recommended way of
building I/O connectors. Splittable DoFn is a generalization of &lt;code>DoFn&lt;/code> that gives it the core
capabilities of &lt;code>Source&lt;/code> while retaining &lt;code>DoFn&lt;/code>'s syntax, flexibility, modularity, and ease of
coding. Thus, it becomes much easier to develop complex I/O connectors with simpler and reusable
code.&lt;/p>
&lt;p>SDF has three advantages over the existing &lt;code>UnboundedSource&lt;/code> and &lt;code>BoundedSource&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>SDF provides a unified set of APIs to handle both unbounded and bounded cases.&lt;/li>
&lt;li>SDF enables reading from source descriptors dynamically.
&lt;ul>
&lt;li>Taking KafkaIO as an example, within &lt;code>UnboundedSource&lt;/code>/&lt;code>BoundedSource&lt;/code> API, you must specify
the topic and partition you want to read from during pipeline construction time. There is no way
for &lt;code>UnboundedSource&lt;/code>/&lt;code>BoundedSource&lt;/code> to accept topics and partitions as inputs during execution
time. But it&amp;rsquo;s built-in to SDF.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>SDF fits in as any node on a pipeline freely with the ability of splitting.
&lt;ul>
&lt;li>&lt;code>UnboundedSource&lt;/code>/&lt;code>BoundedSource&lt;/code> has to be the root node of the pipeline to gain performance
benefits from splitting strategies, which limits many real-world usages. This is no longer a limit
for an SDF.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>As SDF is now ready to use with all the mentioned improvements, it is the recommended
way to build the new I/O connectors. Try out building your own Splittable DoFn by following the
&lt;a href="https://beam.apache.org/documentation/programming-guide/#splittable-dofns">programming guide&lt;/a>. We
have provided tonnes of common utility classes such as common types of &lt;code>RestrictionTracker&lt;/code> and
&lt;code>WatermarkEstimator&lt;/code> in Beam SDK, which will help you onboard easily. As for the existing I/O
connectors, we have wrapped &lt;code>UnboundedSource&lt;/code> and &lt;code>BoundedSource&lt;/code> implementations into Splittable
DoFns, yet we still encourage developers to convert &lt;code>UnboundedSource&lt;/code>/&lt;code>BoundedSource&lt;/code> into actual
Splittable DoFn implementation to gain more performance benefits.&lt;/p>
&lt;p>Many thanks to every contributor who brought this highly anticipated design into the data processing
world. We are really excited to see that users benefit from SDF.&lt;/p>
&lt;p>Below are some real-world SDF examples for you to explore.&lt;/p>
&lt;h2 id="real-world-splittable-dofn-examples">Real world Splittable DoFn examples&lt;/h2>
&lt;p>&lt;strong>Java Examples&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java#L118">Kafka&lt;/a>:
An I/O connector for &lt;a href="https://kafka.apache.org/">Apache Kafka&lt;/a>
(an open-source distributed event streaming platform).&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Watch.java#L787">Watch&lt;/a>:
Uses a polling function producing a growing set of outputs for each input until a per-input
termination condition is met.&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java#L365">Parquet&lt;/a>:
An I/O connector for &lt;a href="https://parquet.apache.org/">Apache Parquet&lt;/a>
(an open-source columnar storage format).&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/6fdde4f4eab72b49b10a8bb1cb3be263c5c416b5/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/healthcare/HL7v2IO.java#L493">HL7v2&lt;/a>:
An I/O connector for HL7v2 messages (a clinical messaging format that provides data about events
that occur inside an organization) part of
&lt;a href="https://cloud.google.com/healthcare">Google’s Cloud Healthcare API&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Read.java#L248">BoundedSource wrapper&lt;/a>:
A wrapper which converts an existing &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/BoundedSource.html">BoundedSource&lt;/a>
implementation to a splittable DoFn.&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/java/core/src/main/java/org/apache/beam/sdk/io/Read.java#L432">UnboundedSource wrapper&lt;/a>:
A wrapper which converts an existing &lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/UnboundedSource.html">UnboundedSource&lt;/a>
implementation to a splittable DoFn.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Python Examples&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/571338b0cc96e2e80f23620fe86de5c92dffaccc/sdks/python/apache_beam/io/iobase.py#L1375">BoundedSourceWrapper&lt;/a>:
A wrapper which converts an existing &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.io.iobase.html#apache_beam.io.iobase.BoundedSource">BoundedSource&lt;/a>
implementation to a splittable DoFn.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Go Examples&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/apache/beam/blob/ce190e11332469ea59b6c9acf16ee7c673ccefdd/sdks/go/pkg/beam/io/textio/sdf.go#L40">textio.ReadSdf&lt;/a> implements reading from text files using a splittable DoFn.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Apache Beam 2.26.0</title><link>/blog/beam-2.26.0/</link><pubDate>Fri, 11 Dec 2020 12:00:00 -0800</pubDate><guid>/blog/beam-2.26.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.26.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2260-2020-12-11">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.26.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12348833">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Splittable DoFn is now the default for executing the Read transform for Java based runners (Spark with bounded pipelines) in addition to existing runners from the 2.25.0 release (Direct, Flink, Jet, Samza, Twister2). The expected output of the Read transform is unchanged. Users can opt-out using &lt;code>--experiments=use_deprecated_read&lt;/code>. The Apache Beam community is looking for feedback for this change as the community is planning to make this change permanent with no opt-out. If you run into an issue requiring the opt-out, please send an e-mail to &lt;a href="mailto:user@beam.apache.org">user@beam.apache.org&lt;/a> specifically referencing BEAM-10670 in the subject line and why you needed to opt-out. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10670">BEAM-10670&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Java BigQuery streaming inserts now have timeouts enabled by default. Pass &lt;code>--HTTPWriteTimeout=0&lt;/code> to revert to the old behavior. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6103">BEAM-6103&lt;/a>)&lt;/li>
&lt;li>Added support for Contextual Text IO (Java), a version of text IO that provides metadata about the records (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10124">BEAM-10124&lt;/a>). Support for this IO is currently experimental. Specifically, &lt;strong>there are no update-compatibility guarantees&lt;/strong> for streaming jobs with this IO between current future verisons of Apache Beam SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for avro payload format in Beam SQL Kafka Table (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10885">BEAM-10885&lt;/a>)&lt;/li>
&lt;li>Added support for json payload format in Beam SQL Kafka Table (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10893">BEAM-10893&lt;/a>)&lt;/li>
&lt;li>Added support for protobuf payload format in Beam SQL Kafka Table (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10892">BEAM-10892&lt;/a>)&lt;/li>
&lt;li>Added support for avro payload format in Beam SQL Pubsub Table (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5504">BEAM-5504&lt;/a>)&lt;/li>
&lt;li>Added option to disable unnecessary copying between operators in Flink Runner (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11146">BEAM-11146&lt;/a>)&lt;/li>
&lt;li>Added CombineFn.setup and CombineFn.teardown to Python SDK. These methods let you initialize the CombineFn&amp;rsquo;s state before any of the other methods of the CombineFn is executed and clean that state up later on. If you are using Dataflow, you need to enable Dataflow Runner V2 by passing &lt;code>--experiments=use_runner_v2&lt;/code> before using this feature. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3736">BEAM-3736&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>BigQuery&amp;rsquo;s DATETIME type now maps to Beam logical type org.apache.beam.sdk.schemas.logicaltypes.SqlTypes.DATETIME&lt;/li>
&lt;li>Pandas 1.x is now required for dataframe operations.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.26.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abhishek Yadav, AbhiY98, Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko,
Andrew Pilloud, Ankur Goenka, Boyuan Zhang, Brian Hulette, Chad Dombrova,
Chamikara Jayalath, Curtis &amp;ldquo;Fjord&amp;rdquo; Hawthorne, Damon Douglas, dandy10, Daniel Oliveira,
David Cavazos, dennis, Derrick Qin, dpcollins-google, Dylan Hercher, emily, Esun Kim,
Gleb Kanterov, Heejong Lee, Ismaël Mejía, Jan Lukavský, Jean-Baptiste Onofré, Jing,
Jozef Vilcek, Justin White, Kamil Wasilewski, Kenneth Knowles, kileys, Kyle Weaver,
lostluck, Luke Cwik, Mark, Maximilian Michels, Milan Cermak, Mohammad Hossein Sekhavat,
Nelson Osacky, Neville Li, Ning Kang, pabloem, Pablo Estrada, pawelpasterz,
Pawel Pasterz, Piotr Szuberski, PoojaChandak, purbanow, rarokni, Ravi Magham,
Reuben van Ammers, Reuven Lax, Reza Rokni, Robert Bradshaw, Robert Burke,
Romain Manni-Bucau, Rui Wang, rworley-monster, Sam Rohde, Sam Whittle, shollyman,
Simone Primarosa, Siyuan Chen, Steve Niemitz, Steven van Rossum, sychen, Teodor Spæren,
Tim Clemons, Tim Robertson, Tobiasz Kędzierski, tszerszen, Tudor Marian, tvalentyn,
Tyson Hamilton, Udi Meiri, Vasu Gupta, xasm83, Yichi Zhang, yichuan66, Yifan Mai,
yoshiki.obata, Yueyang Qiu, yukihira1992&lt;/p></description></item><item><title>Blog: Apache Beam 2.25.0</title><link>/blog/beam-2.25.0/</link><pubDate>Fri, 23 Oct 2020 14:00:00 -0800</pubDate><guid>/blog/beam-2.25.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.25.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2250-2020-10-23">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.25.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347147">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Splittable DoFn is now the default for executing the Read transform for Java based runners (Direct, Flink, Jet, Samza, Twister2). The expected output of the Read transform is unchanged. Users can opt-out using &lt;code>--experiments=use_deprecated_read&lt;/code>. The Apache Beam community is looking for feedback for this change as the community is planning to make this change permanent with no opt-out. If you run into an issue requiring the opt-out, please send an e-mail to &lt;a href="mailto:user@beam.apache.org">user@beam.apache.org&lt;/a> specifically referencing BEAM-10670 in the subject line and why you needed to opt-out. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10670">BEAM-10670&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Added cross-language support to Java&amp;rsquo;s KinesisIO, now available in the Python module &lt;code>apache_beam.io.kinesis&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10138">BEAM-10138&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-10137">BEAM-10137&lt;/a>).&lt;/li>
&lt;li>Update Snowflake JDBC dependency for SnowflakeIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10864">BEAM-10864&lt;/a>)&lt;/li>
&lt;li>Added cross-language support to Java&amp;rsquo;s SnowflakeIO.Write, now available in the Python module &lt;code>apache_beam.io.snowflake&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9898">BEAM-9898&lt;/a>).&lt;/li>
&lt;li>Added delete function to Java&amp;rsquo;s &lt;code>ElasticsearchIO#Write&lt;/code>. Now, Java&amp;rsquo;s ElasticsearchIO can be used to selectively delete documents using &lt;code>withIsDeleteFn&lt;/code> function (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5757">BEAM-5757&lt;/a>).&lt;/li>
&lt;li>Java SDK: Added new IO connector for InfluxDB - InfluxDbIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-2546">BEAM-2546&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Support for repeatable fields in JSON decoder for &lt;code>ReadFromBigQuery&lt;/code> added. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10524">BEAM-10524&lt;/a>)&lt;/li>
&lt;li>Added an opt-in, performance-driven runtime type checking system for the Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10549">BEAM-10549&lt;/a>).
More details will be in an upcoming &lt;a href="https://beam.apache.org/blog/python-performance-runtime-type-checking/index.html">blog post&lt;/a>.&lt;/li>
&lt;li>Added support for Python 3 type annotations on PTransforms using typed PCollections (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10258">BEAM-10258&lt;/a>).
More details will be in an upcoming &lt;a href="https://beam.apache.org/blog/python-improved-annotations/index.html">blog post&lt;/a>.&lt;/li>
&lt;li>Improved the Interactive Beam API where recording streaming jobs now start a long running background recording job. Running ib.show() or ib.collect() samples from the recording (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10603">BEAM-10603&lt;/a>).&lt;/li>
&lt;li>In Interactive Beam, ib.show() and ib.collect() now have &amp;ldquo;n&amp;rdquo; and &amp;ldquo;duration&amp;rdquo; as parameters. These mean read only up to &amp;ldquo;n&amp;rdquo; elements and up to &amp;ldquo;duration&amp;rdquo; seconds of data read from the recording (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10603">BEAM-10603&lt;/a>).&lt;/li>
&lt;li>Initial preview of &lt;a href="https://s.apache.org/simpler-python-pipelines-2020#slide=id.g905ac9257b_1_21">Dataframes&lt;/a> support.
See also example at apache_beam/examples/wordcount_dataframe.py&lt;/li>
&lt;li>Fixed support for type hints on &lt;code>@ptransform_fn&lt;/code> decorators in the Python SDK.
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-4091">BEAM-4091&lt;/a>)
This has not enabled by default to preserve backwards compatibility; use the
&lt;code>--type_check_additional=ptransform_fn&lt;/code> flag to enable. It may be enabled by
default in future versions of Beam.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Python 2 and Python 3.5 support dropped (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10644">BEAM-10644&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-9372">BEAM-9372&lt;/a>).&lt;/li>
&lt;li>Pandas 1.x allowed. Older version of Pandas may still be used, but may not be as well tested.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Python transform ReadFromSnowflake has been moved from &lt;code>apache_beam.io.external.snowflake&lt;/code> to &lt;code>apache_beam.io.snowflake&lt;/code>. The previous path will be removed in the future versions.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Dataflow streaming timers once against not strictly time ordered when set earlier mid-bundle, as the fix for &lt;a href="https://issues.apache.org/jira/browse/BEAM-8543">BEAM-8543&lt;/a> introduced more severe bugs and has been rolled back.&lt;/li>
&lt;li>Default compressor change breaks dataflow python streaming job update compatibility. Please use python SDK version &amp;lt;= 2.23.0 or &amp;gt; 2.25.0 if job update is critical.(&lt;a href="https://issues.apache.org/jira/browse/BEAM-11113">BEAM-11113&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.25.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Aldair Coronel Ruiz, Alexey Romanenko, Andrew Pilloud, Ankur Goenka,
Ayoub ENNASSIRI, Bipin Upadhyaya, Boyuan Zhang, Brian Hulette, Brian Michalski, Chad Dombrova,
Chamikara Jayalath, Damon Douglas, Daniel Oliveira, David Cavazos, David Janicek, Doug Roeper, Eric
Roshan-Eisner, Etta Rapp, Eugene Kirpichov, Filipe Regadas, Heejong Lee, Ihor Indyk, Irvi Firqotul
Aini, Ismaël Mejía, Jan Lukavský, Jayendra, Jiadai Xia, Jithin Sukumar, Jozsef Bartok, Kamil
Gałuszka, Kamil Wasilewski, Kasia Kucharczyk, Kenneth Jung, Kenneth Knowles, Kevin Puthusseri, Kevin
Sijo Puthusseri, KevinGG, Kyle Weaver, Leiyi Zhang, Lourens Naudé, Luke Cwik, Matthew Ouyang,
Maximilian Michels, Michal Walenia, Milan Cermak, Monica Song, Nelson Osacky, Neville Li, Ning Kang,
Pablo Estrada, Piotr Szuberski, Qihang, Rehman, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang,
Saavan Nanavati, Sam Bourne, Sam Rohde, Sam Whittle, Sergiy Kolesnikov, Sindy Li, Siyuan Chen, Steve
Niemitz, Terry Xian, Thomas Weise, Tobiasz Kędzierski, Truc Le, Tyson Hamilton, Udi Meiri, Valentyn
Tymofieiev, Yichi Zhang, Yifan Mai, Yueyang Qiu, annaqin418, danielxjd, dennis, dp, fuyuwei,
lostluck, nehsyc, odeshpande, odidev, pulasthi, purbanow, rworley-monster, sclukas77, terryxian78,
tvalentyn, yoshiki.obata&lt;/p></description></item><item><title>Blog: Apache Beam 2.24.0</title><link>/blog/beam-2.24.0/</link><pubDate>Fri, 18 Sep 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.24.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.24.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2240-2020-09-18">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.24.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347146">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Apache Beam 2.24.0 is the last release with Python 2 and Python 3.5
support.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>New overloads for BigtableIO.Read.withKeyRange() and BigtableIO.Read.withRowFilter()
methods that take ValueProvider as a parameter (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10283">BEAM-10283&lt;/a>).&lt;/li>
&lt;li>The WriteToBigQuery transform (Python) in Dataflow Batch no longer relies on BigQuerySink by default. It relies on
a new, fully-featured transform based on file loads into BigQuery. To revert the behavior to the old implementation,
you may use &lt;code>--experiments=use_legacy_bq_sink&lt;/code>.&lt;/li>
&lt;li>Add cross-language support to Java&amp;rsquo;s JdbcIO, now available in the Python module &lt;code>apache_beam.io.jdbc&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10135">BEAM-10135&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-10136">BEAM-10136&lt;/a>).&lt;/li>
&lt;li>Add support of AWS SDK v2 for KinesisIO.Read (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9702">BEAM-9702&lt;/a>).&lt;/li>
&lt;li>Add streaming support to SnowflakeIO in Java SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9896">BEAM-9896&lt;/a>)&lt;/li>
&lt;li>Support reading and writing to Google Healthcare DICOM APIs in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10601">BEAM-10601&lt;/a>)&lt;/li>
&lt;li>Add dispositions for SnowflakeIO.write (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10343">BEAM-10343&lt;/a>)&lt;/li>
&lt;li>Add cross-language support to SnowflakeIO.Read now available in the Python module &lt;code>apache_beam.io.external.snowflake&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9897">BEAM-9897&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Shared library for simplifying management of large shared objects added to Python SDK. Example use case is sharing a large TF model object across threads (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10417">BEAM-10417&lt;/a>).&lt;/li>
&lt;li>Dataflow streaming timers are not strictly time ordered when set earlier mid-bundle (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8543">BEAM-8543&lt;/a>).&lt;/li>
&lt;li>OnTimerContext should not create a new one when processing each element/timer in FnApiDoFnRunner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9839">BEAM-9839&lt;/a>)&lt;/li>
&lt;li>Key should be available in @OnTimer methods (Spark Runner) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9850">BEAM-9850&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>WriteToBigQuery transforms now require a GCS location to be provided through either
custom_gcs_temp_location in the constructor of WriteToBigQuery or the fallback option
&amp;ndash;temp_location, or pass method=&amp;quot;STREAMING_INSERTS&amp;rdquo; to WriteToBigQuery (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6928">BEAM-6928&lt;/a>).&lt;/li>
&lt;li>Python SDK now understands &lt;code>typing.FrozenSet&lt;/code> type hints, which are not interchangeable with &lt;code>typing.Set&lt;/code>. You may need to update your pipelines if type checking fails. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10197">BEAM-10197&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Default compressor change breaks dataflow python streaming job update compatibility. Please use python SDK version &amp;lt;= 2.23.0 or &amp;gt; 2.25.0 if job update is critical.(&lt;a href="https://issues.apache.org/jira/browse/BEAM-11113">BEAM-11113&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.24.0 release. Thank you to all contributors!&lt;/p>
&lt;p>adesormi, Ahmet Altay, Alex Amato, Alexey Romanenko, Andrew Pilloud, Ashwin Ramaswami, Borzoo,
Boyuan Zhang, Brian Hulette, Brian M, Bu Sun Kim, Chamikara Jayalath, Colm O hEigeartaigh,
Corvin Deboeser, Damian Gadomski, Damon Douglas, Daniel Oliveira, Dariusz Aniszewski,
davidak09, David Cavazos, David Moravek, David Yan, dhodun, Doug Roeper, Emil Hessman, Emily Ye,
Etienne Chauchot, Etta Rapp, Eugene Kirpichov, fuyuwei, Gleb Kanterov,
Harrison Green, Heejong Lee, Henry Suryawirawan, InigoSJ, Ismaël Mejía, Israel Herraiz,
Jacob Ferriero, Jan Lukavský, Jayendra, jfarr, jhnmora000, Jiadai Xia, JIahao wu, Jie Fan,
Jiyong Jung, Julius Almeida, Kamil Gałuszka, Kamil Wasilewski, Kasia Kucharczyk, Kenneth Knowles,
Kevin Puthusseri, Kyle Weaver, Łukasz Gajowy, Luke Cwik, Mark-Zeng, Maximilian Michels,
Michal Walenia, Niel Markwick, Ning Kang, Pablo Estrada, pawel.urbanowicz, Piotr Szuberski,
Rafi Kamal, rarokni, Rehman Murad Ali, Reuben van Ammers, Reuven Lax, Ricardo Bordon,
Robert Bradshaw, Robert Burke, Robin Qiu, Rui Wang, Saavan Nanavati, sabhyankar, Sam Rohde,
Scott Lukas, Siddhartha Thota, Simone Primarosa, Sławomir Andrian,
Steve Niemitz, Tobiasz Kędzierski, Tomo Suzuki, Tyson Hamilton, Udi Meiri,
Valentyn Tymofieiev, viktorjonsson, Xinyu Liu, Yichi Zhang, Yixing Zhang, yoshiki.obata,
Yueyang Qiu, zijiesong&lt;/p></description></item><item><title>Blog: Pattern Matching with Beam SQL</title><link>/blog/pattern-match-beam-sql/</link><pubDate>Thu, 27 Aug 2020 00:00:01 +0800</pubDate><guid>/blog/pattern-match-beam-sql/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>SQL is becoming increasingly powerful and useful in the field of data analysis. MATCH_RECOGNIZE,
a new SQL component introduced in 2016, brings extra analytical functionality. This project,
as part of Google Summer of Code, aims to support basic MATCH_RECOGNIZE functionality. A basic MATCH_RECOGNIZE
query would be something like this:
&lt;div class='language-sql snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="k">SELECT&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">aid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">bid&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">cid&lt;/span>
&lt;span class="k">FROM&lt;/span> &lt;span class="n">MyTable&lt;/span>
&lt;span class="n">MATCH_RECOGNIZE&lt;/span> &lt;span class="p">(&lt;/span>
&lt;span class="n">PARTITION&lt;/span> &lt;span class="k">BY&lt;/span> &lt;span class="n">userid&lt;/span>
&lt;span class="k">ORDER&lt;/span> &lt;span class="k">BY&lt;/span> &lt;span class="n">proctime&lt;/span>
&lt;span class="n">MEASURES&lt;/span>
&lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">aid&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">B&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">bid&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="k">C&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">cid&lt;/span>
&lt;span class="n">PATTERN&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span> &lt;span class="n">B&lt;/span> &lt;span class="k">C&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">DEFINE&lt;/span>
&lt;span class="n">A&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;a&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">B&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;b&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="k">C&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;c&amp;#39;&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">T&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;/p>
&lt;p>The above query finds out ordered sets of events that have names &amp;lsquo;a&amp;rsquo;, &amp;lsquo;b&amp;rsquo; and &amp;lsquo;c&amp;rsquo;. Apart from this basic usage of
MATCH_RECOGNIZE, I supported a few of other crucial features such as quantifiers and row pattern navigation. I will spell out
the details in later sections.&lt;/p>
&lt;h2 id="approach--discussion">Approach &amp;amp; Discussion&lt;/h2>
&lt;p>The implementation is strongly based on BEAM core transforms. Specifically, one MATCH_RECOGNIZE execution composes the
following series of transforms:&lt;/p>
&lt;ol>
&lt;li>A &lt;code>ParDo&lt;/code> transform and then a &lt;code>GroupByKey&lt;/code> transform that build up the partitions (PARTITION BY).&lt;/li>
&lt;li>A &lt;code>ParDo&lt;/code> transform that sorts within each partition (ORDER BY).&lt;/li>
&lt;li>A &lt;code>ParDo&lt;/code> transform that applies pattern-match in each sorted partition.&lt;/li>
&lt;/ol>
&lt;p>A pattern-match operation was first done with the java regex library. That is, I first transform rows within a partition into
a string and then apply regex pattern-match routines. If a row satisfies a condition, then I output the corresponding pattern variable.
This is ok under the assumption that the pattern definitions are mutually exclusive. That is, a pattern definition like &lt;code>A AS A.price &amp;gt; 0, B AS b.price &amp;lt; 0&lt;/code> is allowed while
a pattern definition like &lt;code>A AS A.price &amp;gt; 0, B AS B.proctime &amp;gt; 0&lt;/code> might results in an incomplete match. For the latter case,
an event can satisfy the conditions A and B at the same time. Mutually exclusive conditions gives deterministic pattern-match:
each event can only belong to at most one pattern class.&lt;/p>
&lt;p>As specified in the SQL 2016 document, MATCH_RECOGNIZE defines a richer set of expression than regular expression. Specifically,
it introduces &lt;em>Row Pattern Navigation Operations&lt;/em> such as &lt;code>PREV&lt;/code> and &lt;code>NEXT&lt;/code>. This is perhaps one of the most intriguing feature of
MATCH_RECOGNIZE. A regex library would no longer suffice the need since the pattern definition could be back-referencing (&lt;code>PREV&lt;/code>) or
forward-referencing (&lt;code>NEXT&lt;/code>). So for the second version of implementation, we chose to use an NFA regex engine. An NFA brings more flexibility
in terms of non-determinism (see Chapter 6 of SQL 2016 Part 5 for a more thorough discussion). My proposed NFA is based on a paper of UMASS.&lt;/p>
&lt;p>This is a working project. Many of the components are still not supported. I will list some unimplemented work in the section
of future work.&lt;/p>
&lt;h2 id="usages">Usages&lt;/h2>
&lt;p>For now, the components I supported are:&lt;/p>
&lt;ul>
&lt;li>PARTITION BY&lt;/li>
&lt;li>ORDER BY&lt;/li>
&lt;li>MEASURES
&lt;ol>
&lt;li>LAST&lt;/li>
&lt;li>FIRST&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>ONE ROW PER MATCH/ALL ROWS PER MATCH&lt;/li>
&lt;li>DEFINE
&lt;ol>
&lt;li>Left side of the condition
&lt;ol>
&lt;li>LAST&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Right side of the condition
&lt;ol>
&lt;li>PREV&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Quantifier
&lt;ol>
&lt;li>Kleene plus&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ul>
&lt;p>The pattern definition evaluation is hard coded. To be more specific, it expects the column reference of the incoming row
to be on the left side of a comparator. Additionally, PREV function can only appear on the right side of the comparator.&lt;/p>
&lt;p>With these limited tools, we could already write some slightly more complicated queries. Imagine we have the following
table:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th align="center">transTime&lt;/th>
&lt;th align="center">price&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td align="center">1&lt;/td>
&lt;td align="center">3&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">2&lt;/td>
&lt;td align="center">2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">3&lt;/td>
&lt;td align="center">1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">4&lt;/td>
&lt;td align="center">5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">5&lt;/td>
&lt;td align="center">6&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>This table reflects the price changes of a product with respect to the transaction time. We could write the following
query:&lt;/p>
&lt;div class='language-sql snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="k">SELECT&lt;/span> &lt;span class="o">*&lt;/span>
&lt;span class="k">FROM&lt;/span> &lt;span class="n">MyTable&lt;/span>
&lt;span class="n">MATCH_RECOGNIZE&lt;/span> &lt;span class="p">(&lt;/span>
&lt;span class="k">ORDER&lt;/span> &lt;span class="k">BY&lt;/span> &lt;span class="n">transTime&lt;/span>
&lt;span class="n">MEASURES&lt;/span>
&lt;span class="k">LAST&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">beforePrice&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="k">FIRST&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">afterPrice&lt;/span>
&lt;span class="n">PATTERN&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="o">+&lt;/span> &lt;span class="n">B&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">DEFINE&lt;/span>
&lt;span class="n">A&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">price&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">PREV&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">B&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">price&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">PREV&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">price&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="p">)&lt;/span> &lt;span class="k">AS&lt;/span> &lt;span class="n">T&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>This will find the local minimum price and the price after it. For the example dataset, the first 3 rows will be
mapped to A and the rest of the rows will be mapped to B. Thus, we will have (1, 5) as the result.&lt;/p>
&lt;blockquote>
&lt;p>Very important: For my NFA implementation, it slightly breaks the rule in the SQL standard. Since the buffered NFA
only stores an event to the buffer if the event is a match to some pattern class, There would be no way to get the
previous event back if the previous row is discarded. So the first row would always be a match (different from the standard)
if PREV is used.&lt;/p>
&lt;/blockquote>
&lt;h2 id="progress">Progress&lt;/h2>
&lt;ol>
&lt;li>PRs
&lt;ol>
&lt;li>&lt;a href="https://github.com/apache/beam/pull/12232">Support MATCH_RECOGNIZE using regex library&lt;/a> (merged)&lt;/li>
&lt;li>&lt;a href="https://github.com/apache/beam/pull/12532">Support MATCH_RECOGNIZE using NFA&lt;/a> (pending)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Commits
&lt;ol>
&lt;li>partition by: &lt;a href="https://github.com/apache/beam/pull/12232/commits/064ada7257970bcb1d35530be1b88cb3830f242b">commit 064ada7&lt;/a>&lt;/li>
&lt;li>order by: &lt;a href="https://github.com/apache/beam/pull/12232/commits/9cd1a82bec7b2f7c44aacfbd72f5f775bb58b650">commit 9cd1a82&lt;/a>&lt;/li>
&lt;li>regex pattern match: &lt;a href="https://github.com/apache/beam/pull/12232/commits/8d6ffcc213e30999fc495c119b68da4f62fad258">commit 8d6ffcc&lt;/a>&lt;/li>
&lt;li>support quantifiers: &lt;a href="https://github.com/apache/beam/pull/12232/commits/f529b876a2c2e43d012c71b3a83ebd55eb16f4ff">commit f529b87&lt;/a>&lt;/li>
&lt;li>measures: &lt;a href="https://github.com/apache/beam/pull/12232/commits/87935746647611aa139d664ebed10c8e638bb024">commit 8793574&lt;/a>&lt;/li>
&lt;li>added NFA implementation: &lt;a href="https://github.com/apache/beam/pull/12532/commits/fc731f2b0699d11853e7b76da86456427d434a2a">commit fc731f2&lt;/a>&lt;/li>
&lt;li>implemented functions PREV and LAST: &lt;a href="https://github.com/apache/beam/pull/12532/commits/fc731f2b0699d11853e7b76da86456427d434a2a">commit 35323da&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h2 id="future-work">Future Work&lt;/h2>
&lt;ul>
&lt;li>Support FINAL/RUNNING keywords.&lt;/li>
&lt;li>Support more quantifiers.&lt;/li>
&lt;li>Add optimization to the NFA.&lt;/li>
&lt;li>A better way to realize MATCH_RECOGNIZE might be having a Complex Event Processing library at BEAM core (rather than using BEAM transforms).&lt;/li>
&lt;/ul>
&lt;!-- Related Documents:
- proposal
- design doc
- SQL 2016 standard
- UMASS NFA^b paper
-->
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/file/d/1ZuFZV4dCFVPZW_-RiqbU0w-vShaZh_jX/view?usp=sharing">Project Proposal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://s.apache.org/beam-sql-pattern-recognization">Design Documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.iso.org/standard/65143.html">SQL 2016 documentation Part 5&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dl.acm.org/doi/10.1145/1376616.1376634">UMASS paper on NFA with shared buffer&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Improved Annotation Support for the Python SDK</title><link>/blog/python-improved-annotations/</link><pubDate>Fri, 21 Aug 2020 00:00:01 -0800</pubDate><guid>/blog/python-improved-annotations/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The importance of static type checking in a dynamically
typed language like Python is not up for debate. Type hints
allow developers to leverage a strong typing system to:&lt;/p>
&lt;ul>
&lt;li>write better code,&lt;/li>
&lt;li>self-document ambiguous programming logic, and&lt;/li>
&lt;li>inform intelligent code completion in IDEs like PyCharm.&lt;/li>
&lt;/ul>
&lt;p>This is why we&amp;rsquo;re excited to announce upcoming improvements to
the &lt;code>typehints&lt;/code> module of Beam&amp;rsquo;s Python SDK, including support
for typed PCollections and Python 3 style annotations on PTransforms.&lt;/p>
&lt;h1 id="improved-annotations">Improved Annotations&lt;/h1>
&lt;p>Today, you have the option to declare type hints on PTransforms using either
class decorators or inline functions.&lt;/p>
&lt;p>For instance, a PTransform with decorated type hints might look like this:&lt;/p>
&lt;pre>&lt;code>@beam.typehints.with_input_types(int)
@beam.typehints.with_output_types(str)
class IntToStr(beam.PTransform):
def expand(self, pcoll):
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr())
&lt;/code>&lt;/pre>&lt;p>Using inline functions instead, the same transform would look like this:&lt;/p>
&lt;pre>&lt;code>class IntToStr(beam.PTransform):
def expand(self, pcoll):
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr()).with_input_types(int).with_output_types(str)
&lt;/code>&lt;/pre>&lt;p>Both methods have problems. Class decorators are syntax-heavy,
requiring two additional lines of code, whereas inline functions provide type hints
that aren&amp;rsquo;t reusable across other instances of the same transform. Additionally, both
methods are incompatible with static type checkers like MyPy.&lt;/p>
&lt;p>With Python 3 annotations however, we can subvert these problems to provide a
clean and reusable type hint experience. Our previous transform now looks like this:&lt;/p>
&lt;pre>&lt;code>class IntToStr(beam.PTransform):
def expand(self, pcoll: PCollection[int]) -&amp;gt; PCollection[str]:
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr())
&lt;/code>&lt;/pre>&lt;p>These type hints will actively hook into the internal Beam typing system to
play a role in pipeline type checking, and runtime type checking.&lt;/p>
&lt;p>So how does this work?&lt;/p>
&lt;h2 id="typed-pcollections">Typed PCollections&lt;/h2>
&lt;p>You guessed it! The PCollection class inherits from &lt;code>typing.Generic&lt;/code>, allowing it to be
parameterized with either zero types (denoted &lt;code>PCollection&lt;/code>) or one type (denoted &lt;code>PCollection[T]&lt;/code>).&lt;/p>
&lt;ul>
&lt;li>A PCollection with zero types is implicitly converted to &lt;code>PCollection[Any]&lt;/code>.&lt;/li>
&lt;li>A PCollection with one type can have any nested type (e.g. &lt;code>Union[int, str]&lt;/code>).&lt;/li>
&lt;/ul>
&lt;p>Internally, Beam&amp;rsquo;s typing system makes these annotations compatible with other
type hints by removing the outer PCollection container.&lt;/p>
&lt;h2 id="pbegin-pdone-none">PBegin, PDone, None&lt;/h2>
&lt;p>Finally, besides PCollection, a valid annotation on the &lt;code>expand(...)&lt;/code> method of a PTransform is
&lt;code>PBegin&lt;/code> or &lt;code>None&lt;/code>. These are generally used for PTransforms that begin or end with an I/O operation.&lt;/p>
&lt;p>For instance, when saving data, your transform&amp;rsquo;s output type should be &lt;code>None&lt;/code>.&lt;/p>
&lt;pre>&lt;code>class SaveResults(beam.PTransform):
def expand(self, pcoll: PCollection[str]) -&amp;gt; None:
return pcoll | beam.io.WriteToBigQuery(...)
&lt;/code>&lt;/pre>&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>What are you waiting for.. start using annotations on your transforms!&lt;/p>
&lt;p>For more background on type hints in Python, see:
&lt;a href="https://beam.apache.org/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>.&lt;/p>
&lt;p>Finally, please
&lt;a href="https://beam.apache.org/community/contact-us/">let us know&lt;/a>
if you encounter any issues.&lt;/p></description></item><item><title>Blog: Performance-Driven Runtime Type Checking for the Python SDK</title><link>/blog/python-performance-runtime-type-checking/</link><pubDate>Fri, 21 Aug 2020 00:00:01 -0800</pubDate><guid>/blog/python-performance-runtime-type-checking/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog post, we&amp;rsquo;re announcing the upcoming release of a new, opt-in
runtime type checking system for Beam&amp;rsquo;s Python SDK that&amp;rsquo;s optimized for performance
in both development and production environments.&lt;/p>
&lt;p>But let&amp;rsquo;s take a step back - why do we even care about runtime type checking
in the first place? Let&amp;rsquo;s look at an example.&lt;/p>
&lt;pre>&lt;code>class MultiplyNumberByTwo(beam.DoFn):
def process(self, element: int):
return element * 2
p = Pipeline()
p | beam.Create(['1', '2'] | beam.ParDo(MultiplyNumberByTwo())
&lt;/code>&lt;/pre>&lt;p>In this code, we passed a list of strings to a DoFn that&amp;rsquo;s clearly intended for use with
integers. Luckily, this code will throw an error during pipeline construction because
the inferred output type of &lt;code>beam.Create(['1', '2'])&lt;/code> is &lt;code>str&lt;/code> which is incompatible with
the declared input type of &lt;code>MultiplyNumberByTwo.process&lt;/code> which is &lt;code>int&lt;/code>.&lt;/p>
&lt;p>However, what if we turned pipeline type checking off using the &lt;code>no_pipeline_type_check&lt;/code>
flag? Or more realistically, what if the input PCollection to &lt;code>MultiplyNumberByTwo&lt;/code> arrived
from a database, meaning that the output data type can only be known at runtime?&lt;/p>
&lt;p>In either case, no error would be thrown during pipeline construction.
And even at runtime, this code works. Each string would be multiplied by 2,
yielding a result of &lt;code>['11', '22']&lt;/code>, but that&amp;rsquo;s certainly not the outcome we want.&lt;/p>
&lt;p>So how do you debug this breed of &amp;ldquo;hidden&amp;rdquo; errors? More broadly speaking, how do you debug
any typing or serialization error in Beam?&lt;/p>
&lt;p>The answer is to use runtime type checking.&lt;/p>
&lt;h1 id="runtime-type-checking-rtc">Runtime Type Checking (RTC)&lt;/h1>
&lt;p>This feature works by checking that actual input and output values satisfy the declared
type constraints during pipeline execution. If you ran the code from before with
&lt;code>runtime_type_check&lt;/code> on, you would receive the following error message:&lt;/p>
&lt;pre>&lt;code>Type hint violation for 'ParDo(MultiplyByTwo)': requires &amp;lt;class 'int'&amp;gt; but got &amp;lt;class 'str'&amp;gt; for element
&lt;/code>&lt;/pre>&lt;p>This is an actionable error message - it tells you that either your code has a bug
or that your declared type hints are incorrect. Sounds simple enough, so what&amp;rsquo;s the catch?&lt;/p>
&lt;p>&lt;em>It is soooo slowwwwww.&lt;/em> See for yourself.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Element Size&lt;/th>
&lt;th>Normal Pipeline&lt;/th>
&lt;th>Runtime Type Checking Pipeline&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>5.3 sec&lt;/td>
&lt;td>5.6 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2,001&lt;/td>
&lt;td>9.4 sec&lt;/td>
&lt;td>57.2 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10,001&lt;/td>
&lt;td>24.5 sec&lt;/td>
&lt;td>259.8 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18,001&lt;/td>
&lt;td>38.7 sec&lt;/td>
&lt;td>450.5 sec&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In this micro-benchmark, the pipeline with runtime type checking was over 10x slower,
with the gap only increasing as our input PCollection increased in size.&lt;/p>
&lt;p>So, is there any production-friendly alternative?&lt;/p>
&lt;h1 id="performance-runtime-type-check">Performance Runtime Type Check&lt;/h1>
&lt;p>There is! We developed a new flag called &lt;code>performance_runtime_type_check&lt;/code> that
minimizes its footprint on the pipeline&amp;rsquo;s time complexity using a combination of&lt;/p>
&lt;ul>
&lt;li>efficient Cython code,&lt;/li>
&lt;li>smart sampling techniques, and&lt;/li>
&lt;li>optimized mega type-hints.&lt;/li>
&lt;/ul>
&lt;p>So what do the new numbers look like?&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Element Size&lt;/th>
&lt;th>Normal&lt;/th>
&lt;th>RTC&lt;/th>
&lt;th>Performance RTC&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>5.3 sec&lt;/td>
&lt;td>5.6 sec&lt;/td>
&lt;td>5.4 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2,001&lt;/td>
&lt;td>9.4 sec&lt;/td>
&lt;td>57.2 sec&lt;/td>
&lt;td>11.2 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10,001&lt;/td>
&lt;td>24.5 sec&lt;/td>
&lt;td>259.8 sec&lt;/td>
&lt;td>25.5 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18,001&lt;/td>
&lt;td>38.7 sec&lt;/td>
&lt;td>450.5 sec&lt;/td>
&lt;td>39.4 sec&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>On average, the new Performance RTC is 4.4% slower than a normal pipeline whereas the old RTC
is over 900% slower! Additionally, as the size of the input PCollection increases, the fixed cost
of setting up the Performance RTC system is spread across each element, decreasing the relative
impact on the overall pipeline. With 18,001 elements, the difference is less than 1 second.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>There are three key factors responsible for this upgrade in performance.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Instead of type checking all values, we only type check a subset of values, known as
a sample in statistics. Initially, we sample a substantial number of elements, but as our
confidence that the element type won&amp;rsquo;t change over time increases, we reduce our
sampling rate (up to a fixed minimum).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Whereas the old RTC system used heavy wrappers to perform the type check, the new RTC system
moves the type check to a Cython-optimized, non-decorated portion of the codebase. For reference,
Cython is a programming language that gives C-like performance to Python code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Finally, we use a single mega type hint to type-check only the output values of transforms
instead of type-checking both the input and output values separately. This mega typehint is composed of
the original transform&amp;rsquo;s output type constraints along with all consumer transforms&amp;rsquo; input type
constraints. Using this mega type hint allows us to reduce overhead while simultaneously allowing
us to throw &lt;em>more actionable errors&lt;/em>. For instance, consider the following error (which was
generated from the old RTC system):&lt;/p>
&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>Runtime type violation detected within ParDo(DownstreamDoFn): Type-hint for argument: 'element' violated. Expected an instance of &amp;lt;class ‘str’&amp;gt;, instead found 9, an instance of &amp;lt;class ‘int’&amp;gt;.
&lt;/code>&lt;/pre>&lt;p>This error tells us that the &lt;code>DownstreamDoFn&lt;/code> received an &lt;code>int&lt;/code> when it was expecting a &lt;code>str&lt;/code>, but doesn&amp;rsquo;t tell us
who created that &lt;code>int&lt;/code> in the first place. Who is the offending upstream transform that&amp;rsquo;s responsible for
this &lt;code>int&lt;/code>? Presumably, &lt;em>that&lt;/em> transform&amp;rsquo;s output type hints were too expansive (e.g. &lt;code>Any&lt;/code>) or otherwise non-existent because
no error was thrown during the runtime type check of its output.&lt;/p>
&lt;p>The problem here boils down to a lack of context. If we knew who our consumers were when type
checking our output, we could simultaneously type check our output value against our output type
constraints and every consumers&amp;rsquo; input type constraints to know whether there is &lt;em>any&lt;/em> possibility
for a mismatch. This is exactly what the mega type hint does, and it allows us to throw errors
at the point of declaration rather than the point of exception, saving you valuable time
while providing higher quality error messages.&lt;/p>
&lt;p>So what would the same error look like using Performance RTC? It&amp;rsquo;s the exact same string but with one additional line:&lt;/p>
&lt;pre>&lt;code>[while running 'ParDo(UpstreamDoFn)']
&lt;/code>&lt;/pre>&lt;p>And that&amp;rsquo;s much more actionable for an investigation :)&lt;/p>
&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>Go play with the new &lt;code>performance_runtime_type_check&lt;/code> feature!&lt;/p>
&lt;p>It&amp;rsquo;s in an experimental state so please
&lt;a href="https://beam.apache.org/community/contact-us/">let us know&lt;/a>
if you encounter any issues.&lt;/p></description></item><item><title>Blog: Apache Beam 2.23.0</title><link>/blog/beam-2.23.0/</link><pubDate>Wed, 29 Jul 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.23.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.23.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2230-2020-07-29">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.23.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347145">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Twister2 Runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7304">BEAM-7304&lt;/a>).&lt;/li>
&lt;li>Python 3.8 support (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8494">BEAM-8494&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for reading from Snowflake added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9722">BEAM-9722&lt;/a>).&lt;/li>
&lt;li>Support for writing to Splunk added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8596">BEAM-8596&lt;/a>).&lt;/li>
&lt;li>Support for assume role added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10335">BEAM-10335&lt;/a>).&lt;/li>
&lt;li>A new transform to read from BigQuery has been added: &lt;code>apache_beam.io.gcp.bigquery.ReadFromBigQuery&lt;/code>. This transform
is experimental. It reads data from BigQuery by exporting data to Avro files, and reading those files. It also supports
reading data by exporting to JSON files. This has small differences in behavior for Time and Date-related fields. See
Pydoc for more information.&lt;/li>
&lt;li>Add dispositions for SnowflakeIO.write (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10343">BEAM-10343&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Update Snowflake JDBC dependency and add application=beam to connection URL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10383">BEAM-10383&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>RowJson.RowJsonDeserializer&lt;/code>, &lt;code>JsonToRow&lt;/code>, and &lt;code>PubsubJsonTableProvider&lt;/code> now accept &amp;ldquo;implicit
nulls&amp;rdquo; by default when deserializing JSON (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10220">BEAM-10220&lt;/a>).
Previously nulls could only be represented with explicit null values, as in
&lt;code>{&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;, &amp;quot;baz&amp;quot;: null}&lt;/code>, whereas an implicit null like &lt;code>{&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;}&lt;/code> would raise an
exception. Now both JSON strings will yield the same result by default. This behavior can be
overridden with &lt;code>RowJson.RowJsonDeserializer#withNullBehavior&lt;/code>.&lt;/li>
&lt;li>Fixed a bug in &lt;code>GroupIntoBatches&lt;/code> experimental transform in Python to actually group batches by key.
This changes the output type for this transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6696">BEAM-6696&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Gearpump runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9999">BEAM-9999&lt;/a>)&lt;/li>
&lt;li>Remove Apex runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9999">BEAM-9999&lt;/a>)&lt;/li>
&lt;li>RedisIO.readAll() is deprecated and will be removed in 2 versions, users must use RedisIO.readKeyPatterns() as a replacement (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9747">BEAM-9747&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.23.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron, Abhishek Yadav, Ahmet Altay, aiyangar, Aizhamal Nurmamat kyzy, Ajo Thomas, Akshay-Iyangar, Alan Pryor, Alex Amato, Alexey Romanenko, Allen Pradeep Xavier, Andrew Crites, Andrew Pilloud, Ankur Goenka, Anna Qin, Ashwin Ramaswami, bntnam, Borzoo Esmailloo, Boyuan Zhang, Brian Hulette, Brian Michalski, brucearctor, Chamikara Jayalath, chi-chi weng, Chuck Yang, Chun Yang, Colm O hEigeartaigh, Corvin Deboeser, Craig Chambers, Damian Gadomski, Damon Douglas, Daniel Oliveira, Dariusz Aniszewski, darshanj, darshan jani, David Cavazos, David Moravek, David Yan, Esun Kim, Etienne Chauchot, Filipe Regadas, fuyuwei, Graeme Morgan, Hannah-Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, InigoSJ, Ismaël Mejía, Israel Herraiz, Jacob Ferriero, Jan Lukavský, Jie Fan, John Mora, Jozef Vilcek, Julien Phalip, Justine Koa, Kamil Gabryjelski, Kamil Wasilewski, Kasia Kucharczyk, Kenneth Jung, Kenneth Knowles, kevingg, Kevin Sijo Puthusseri, kshivvy, Kyle Weaver, Kyoungha Min, Kyungwon Jo, Luke Cwik, Mark Liu, Mark-Zeng, Matthias Baetens, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Nam Bui, Nathan Fisher, Niel Markwick, Ning Kang, Omar Ismail, Pablo Estrada, paul fisher, Pawel Pasterz, perkss, Piotr Szuberski, pulasthi, purbanow, Rahul Patwari, Rajat Mittal, Rehman, Rehman Murad Ali, Reuben van Ammers, Reuven Lax, Reza Rokni, Rion Williams, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, sabhyankar, Sam Rohde, Sam Whittle, sclukas77, Sebastian Graca, Shoaib Zafar, Sruthi Sree Kumar, Stephen O&amp;rsquo;Kennedy, Steve Koonce, Steve Niemitz, Steven van Rossum, Ted Romer, Tesio, Thinh Ha, Thomas Weise, Tobias Kaymak, tobiaslieber-cognitedata, Tobiasz Kędzierski, Tomo Suzuki, Tudor Marian, tvs, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, xuelianhan, Yichi Zhang, Yifan Zou, Yixing Zhang, yoshiki.obata, Yueyang Qiu, Yu Feng, Yuwei Fu, Zhuo Peng, ZijieSong946.&lt;/p></description></item><item><title>Blog: Apache Beam 2.22.0</title><link>/blog/beam-2.22.0/</link><pubDate>Mon, 08 Jun 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.22.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.22.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2220-2020-06-08">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.22.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347144">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Basic Kafka read/write support for DataflowRunner (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8019">BEAM-8019&lt;/a>).&lt;/li>
&lt;li>Sources and sinks for Google Healthcare APIs (Java)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9468">BEAM-9468&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>&lt;code>--workerCacheMB&lt;/code> flag is supported in Dataflow streaming pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9964">BEAM-9964&lt;/a>)&lt;/li>
&lt;li>&lt;code>--direct_num_workers=0&lt;/code> is supported for FnApi runner. It will set the number of threads/subprocesses to number of cores of the machine executing the pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9443">BEAM-9443&lt;/a>).&lt;/li>
&lt;li>Python SDK now has experimental support for SqlTransform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8603">BEAM-8603&lt;/a>).&lt;/li>
&lt;li>Add OnWindowExpiration method to Stateful DoFn (&lt;a href="https://issues.apache.org/jira/browse/BEAM-1589">BEAM-1589&lt;/a>).&lt;/li>
&lt;li>Added PTransforms for Google Cloud DLP (Data Loss Prevention) services integration (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9723">BEAM-9723&lt;/a>):
&lt;ul>
&lt;li>Inspection of data,&lt;/li>
&lt;li>Deidentification of data,&lt;/li>
&lt;li>Reidentification of data.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Add a more complete I/O support matrix in the documentation site (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9916">BEAM-9916&lt;/a>).&lt;/li>
&lt;li>Upgrade Sphinx to 3.0.3 for building PyDoc.&lt;/li>
&lt;li>Added a PTransform for image annotation using Google Cloud AI image processing service
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9646">BEAM-9646&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Python SDK now requires &lt;code>--job_endpoint&lt;/code> to be set when using &lt;code>--runner=PortableRunner&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9860">BEAM-9860&lt;/a>). Users seeking the old default behavior should set &lt;code>--runner=FlinkRunner&lt;/code> instead.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.22.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, aiyangar, Ajo Thomas, Akshay-Iyangar, Alan Pryor, Alexey Romanenko, Allen Pradeep Xavier, amaliujia, Andrew Pilloud, Ankur Goenka, Ashwin Ramaswami, bntnam, Borzoo Esmailloo, Boyuan Zhang, Brian Hulette, Chamikara Jayalath, Colm O hEigeartaigh, Craig Chambers, Damon Douglas, Daniel Oliveira, David Cavazos, David Moravek, Esun Kim, Etienne Chauchot, Filipe Regadas, Graeme Morgan, Hannah Jiang, Hannah-Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, Ismaël Mejía, Israel Herraiz, Jacob Ferriero, Jan Lukavský, John Mora, Kamil Wasilewski, Kenneth Jung, Kenneth Knowles, kevingg, Kyle Weaver, Kyoungha Min, Kyungwon Jo, Luke Cwik, Mark Liu, Matthias Baetens, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Nam Bui, Niel Markwick, Ning Kang, Omar Ismail, omarismail94, Pablo Estrada, paul fisher, pawelpasterz, Pawel Pasterz, Piotr Szuberski, Rahul Patwari, rarokni, Rehman, Rehman Murad Ali, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Sam Rohde, Sam Whittle, Sebastian Graca, Shoaib Zafar, Sruthi Sree Kumar, Stephen O&amp;rsquo;Kennedy, Steve Koonce, Steve Niemitz, Steven van Rossum, Tesio, Thomas Weise, tobiaslieber-cognitedata, Tomo Suzuki, Tudor Marian, tvalentyn, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, xuelianhan, Yichi Zhang, Yifan Zou, yoshiki.obata, Yueyang Qiu, Zhuo Peng&lt;/p></description></item><item><title>Blog: Announcing Beam Katas for Kotlin</title><link>/blog/beam-katas-kotlin-release/</link><pubDate>Mon, 01 Jun 2020 00:00:01 -0800</pubDate><guid>/blog/beam-katas-kotlin-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Today, we are happy to announce a new addition to the Beam Katas family: Kotlin!&lt;/p>
&lt;p>&lt;img src="/images/blog/beam-katas-kotlin-release/beam-and-kotlin.png" alt="Apache Beam and Kotlin Shaking Hands" height="330" width="800" >&lt;/p>
&lt;p>You may remember &lt;a href="https://beam.apache.org/blog/beam-kata-release">a post from last year&lt;/a> that informed everyone of the wonderful Beam Katas available on &lt;a href="https://stepik.org">Stepik&lt;/a>
for learning more about writing Apache Beam applications, working with its various APIs and programming model
hands-on, all from the comfort of your favorite IDEs. As of today, you can now work through all of the progressive
exercises to learn about the fundamentals of Beam in Kotlin.&lt;/p>
&lt;p>&lt;a href="https://kotlinlang.org">Kotlin&lt;/a> is a modern, open-source, statically typed language that targets the JVM. It is most commonly used by
Android developers, however it has recently risen in popularity due to its extensive feature set that enables
more concise and cleaner code than Java, without sacrificing performance or type safety. It recently was &lt;a href="https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved">ranked
as one of the most beloved programming languages in the annual Stack Overflow Developer Survey&lt;/a>, so don&amp;rsquo;t take
just our word for it.&lt;/p>
&lt;p>The relationship between Apache Beam and Kotlin isn&amp;rsquo;t a new one. You can find examples scattered across the web
of engineering teams embracing the two technologies including &lt;a href="https://beam.apache.org/blog/beam-kotlin/">a series of samples announced on this very blog&lt;/a>.
If you are new to Beam or are an experienced veteran looking for a change of pace, we&amp;rsquo;d encourage you to give
Kotlin a try.&lt;/p>
&lt;p>You can find the Kotlin and the other excellent Beam Katas below (or by just searching for &amp;ldquo;Beam Katas&amp;rdquo; within
&lt;a href="https://www.jetbrains.com/education/download/#section=idea">IntelliJ&lt;/a> or &lt;a href="https://www.jetbrains.com/education/download/#section=pycharm-edu">PyCharm&lt;/a> through &lt;a href="https://plugins.jetbrains.com/plugin/10081-edutools">the EduTools plugin&lt;/a>):&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://stepik.org/course/72488">&lt;strong>Kotlin&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/54530">&lt;strong>Java&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/54532">&lt;strong>Python&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/70387">&lt;strong>Go (in development)&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;d like to extend a very special thanks to &lt;a href="https://twitter.com/henry_ken">Henry Suryawirawan&lt;/a> for his creation of the original series of Katas
and his support during the review process and making this effort a reality.&lt;/p>
&lt;p>&lt;br />&lt;/p>
&lt;p>&lt;img src="/images/blog/beam-katas-kotlin-release/beam-katas-in-edutools.png" alt="Access Beam Katas Kotlin through a JetBrains Educational Product" height="252" width="800" >&lt;/p></description></item><item><title>Blog: Python SDK Typing Changes</title><link>/blog/python-typing/</link><pubDate>Thu, 28 May 2020 00:00:01 -0800</pubDate><guid>/blog/python-typing/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Beam Python has recently increased its support and integration of Python 3 type
annotations for improved code clarity and type correctness checks.
Read on to find out what&amp;rsquo;s new.&lt;/p>
&lt;p>Python supports type annotations on functions (PEP 484). Static type checkers,
such as mypy, are used to verify adherence to these types.
For example:&lt;/p>
&lt;pre>&lt;code>def f(v: int) -&amp;gt; int:
return v[0]
&lt;/code>&lt;/pre>&lt;p>Running mypy on the above code will give the error:
&lt;code>Value of type &amp;quot;int&amp;quot; is not indexable&lt;/code>.&lt;/p>
&lt;p>We&amp;rsquo;ve recently made changes to Beam in 2 areas:&lt;/p>
&lt;p>Adding type annotations throughout Beam. Type annotations make a large and
sophisticated codebase like Beam easier to comprehend and navigate in your
favorite IDE.&lt;/p>
&lt;p>Second, we&amp;rsquo;ve added support for Python 3 type annotations. This allows SDK
users to specify a DoFn&amp;rsquo;s type hints in one place.
We&amp;rsquo;ve also expanded Beam&amp;rsquo;s support of &lt;code>typing&lt;/code> module types.&lt;/p>
&lt;p>For more background see:
&lt;a href="https://beam.apache.org/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>.&lt;/p>
&lt;h1 id="beam-is-typed">Beam Is Typed&lt;/h1>
&lt;p>In tandem with the new type annotation support within DoFns, we&amp;rsquo;ve invested a
great deal of time adding type annotations to the Beam python code itself.
With this in place, we have begun using mypy, a static type
checker, as part of Beam&amp;rsquo;s code review process, which ensures higher quality
contributions and fewer bugs.
The added context and insight that type annotations add throughout Beam is
useful for all Beam developers, contributors and end users alike, but
it is especially beneficial for developers who are new to the project.
If you use an IDE that understands type annotations, it will provide richer
type completions and warnings than before.
You&amp;rsquo;ll also be able to use your IDE to inspect the types of Beam functions and
transforms to better understand how they work, which will ease your own
development.
Finally, once Beam is fully annotated, end users will be able to benefit from
the use of static type analysis on their own pipelines and custom transforms.&lt;/p>
&lt;h1 id="new-ways-to-annotate">New Ways to Annotate&lt;/h1>
&lt;h2 id="python-3-syntax-annotations">Python 3 Syntax Annotations&lt;/h2>
&lt;p>Coming in Beam 2.21 (BEAM-8280), you will be able to use Python annotation
syntax to specify input and output types.&lt;/p>
&lt;p>For example, this new form:&lt;/p>
&lt;pre>&lt;code>class MyDoFn(beam.DoFn):
def process(self, element: int) -&amp;gt; typing.Text:
yield str(element)
&lt;/code>&lt;/pre>&lt;p>is equivalent to this:&lt;/p>
&lt;pre>&lt;code>@apache_beam.typehints.with_input_types(int)
@apache_beam.typehints.with_output_types(typing.Text)
class MyDoFn(beam.DoFn):
def process(self, element):
yield str(element)
&lt;/code>&lt;/pre>&lt;p>One of the advantages of the new form is that you may already be using it
in tandem with a static type checker such as mypy, thus getting additional
runtime type checking for free.&lt;/p>
&lt;p>This feature will be enabled by default, and there will be 2 mechanisms in
place to disable it:&lt;/p>
&lt;ol>
&lt;li>Calling &lt;code>apache_beam.typehints.disable_type_annotations()&lt;/code> before pipeline
construction will disable the new feature completely.&lt;/li>
&lt;li>Decorating a function with &lt;code>@apache_beam.typehints.no_annotations&lt;/code> will
tell Beam to ignore annotations for it.&lt;/li>
&lt;/ol>
&lt;p>Uses of Beam&amp;rsquo;s &lt;code>with_input_type&lt;/code>, &lt;code>with_output_type&lt;/code> methods and decorators will
still work and take precedence over annotations.&lt;/p>
&lt;h3 id="sidebar">Sidebar&lt;/h3>
&lt;p>You might ask: couldn&amp;rsquo;t we use mypy to type check Beam pipelines?
There are several reasons why this is not the case.&lt;/p>
&lt;ul>
&lt;li>Pipelines are constructed at runtime and may depend on information that is
only known at that time, such as a config file or database table schema.&lt;/li>
&lt;li>PCollections don&amp;rsquo;t have the necessary type information, so mypy sees them as
effectively containing any element type.
This may change in in the future.&lt;/li>
&lt;li>Transforms using lambdas (ex: &lt;code>beam.Map(lambda x: (1, x)&lt;/code>) cannot be
annotated properly using PEP 484.
However, Beam does a best-effort attempt to analyze the output type
from the bytecode.&lt;/li>
&lt;/ul>
&lt;h2 id="typing-module-support">Typing Module Support&lt;/h2>
&lt;p>Python&amp;rsquo;s &lt;a href="https://docs.python.org/3/library/typing.html">typing&lt;/a> module defines
types used in type annotations. This is what we call &amp;ldquo;native&amp;rdquo; types.
While Beam has its own typing types, it also supports native types.
While both Beam and native types are supported, for new code we encourage using
native typing types. Native types have as these are supported by additional tools.&lt;/p>
&lt;p>While working on Python 3 annotations syntax support, we&amp;rsquo;ve also discovered and
fixed issues with native type support. There may still be bugs and unsupported
native types. Please
&lt;a href="https://beam.apache.org/community/contact-us/">let us know&lt;/a> if you encounter
issues.&lt;/p></description></item><item><title>Blog: Apache Beam 2.21.0</title><link>/blog/beam-2.21.0/</link><pubDate>Wed, 27 May 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.21.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.21.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2210-2020-05-27">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.21.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347143">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Python: Deprecated module &lt;code>apache_beam.io.gcp.datastore.v1&lt;/code> has been removed
as the client it uses is out of date and does not support Python 3
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9529">BEAM-9529&lt;/a>).
Please migrate your code to use
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.datastore.v1new.datastoreio.html">apache_beam.io.gcp.datastore.&lt;strong>v1new&lt;/strong>&lt;/a>.
See the updated
&lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py">datastore_wordcount&lt;/a>
for example usage.&lt;/li>
&lt;li>Python SDK: Added integration tests and updated batch write functionality for Google Cloud Spanner transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8949">BEAM-8949&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Python SDK will now use Python 3 type annotations as pipeline type hints.
(&lt;a href="https://github.com/apache/beam/pull/10717">#10717&lt;/a>)&lt;/p>
&lt;p>If you suspect that this feature is causing your pipeline to fail, calling
&lt;code>apache_beam.typehints.disable_type_annotations()&lt;/code> before pipeline creation
will disable is completely, and decorating specific functions (such as
&lt;code>process()&lt;/code>) with &lt;code>@apache_beam.typehints.no_annotations&lt;/code> will disable it
for that function.&lt;/p>
&lt;p>More details can be found in
&lt;a href="https://beam.apache.org/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>
and the Python SDK Typing Changes
&lt;a href="https://beam.apache.org/blog/python-typing/">blog post&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Java SDK: Introducing the concept of options in Beam Schema’s. These options add extra
context to fields and schemas. This replaces the current Beam metadata that is present
in a FieldType only, options are available in fields and row schemas. Schema options are
fully typed and can contain complex rows. &lt;em>Remark: Schema aware is still experimental.&lt;/em>
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9035">BEAM-9035&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Java SDK: The protobuf extension is fully schema aware and also includes protobuf option
conversion to beam schema options. &lt;em>Remark: Schema aware is still experimental.&lt;/em>
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9044">BEAM-9044&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added ability to write to BigQuery via Avro file loads (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8841">BEAM-8841&lt;/a>)&lt;/p>
&lt;p>By default, file loads will be done using JSON, but it is possible to
specify the temp_file_format parameter to perform file exports with AVRO.
AVRO-based file loads work by exporting Python types into Avro types, so
to switch to Avro-based loads, you will need to change your data types
from Json-compatible types (string-type dates and timestamp, long numeric
values as strings) into Python native types that are written to Avro
(Python&amp;rsquo;s date, datetime types, decimal, etc). For more information
see &lt;a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added integration of Java SDK with Google Cloud AI VideoIntelligence service
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9147">BEAM-9147&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added integration of Java SDK with Google Cloud AI natural language processing API
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9634">BEAM-9634&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>docker-pull-licenses&lt;/code> tag was introduced. Licenses/notices of third party dependencies will be added to the docker images when &lt;code>docker-pull-licenses&lt;/code> was set.
The files are added to &lt;code>/opt/apache/beam/third_party_licenses/&lt;/code>.
By default, no licenses/notices are added to the docker images. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9136">BEAM-9136&lt;/a>)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Dataflow runner now requires the &lt;code>--region&lt;/code> option to be set, unless a default value is set in the environment (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9199">BEAM-9199&lt;/a>). See &lt;a href="https://cloud.google.com/dataflow/docs/concepts/regional-endpoints">here&lt;/a> for more details.&lt;/li>
&lt;li>HBaseIO.ReadAll now requires a PCollection of HBaseIO.Read objects instead of HBaseQuery objects (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9279">BEAM-9279&lt;/a>).&lt;/li>
&lt;li>ProcessContext.updateWatermark has been removed in favor of using a WatermarkEstimator (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9430">BEAM-9430&lt;/a>).&lt;/li>
&lt;li>Coder inference for PCollection of Row objects has been disabled (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9569">BEAM-9569&lt;/a>).&lt;/li>
&lt;li>Go SDK docker images are no longer released until further notice.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Java SDK: Beam Schema FieldType.getMetadata is now deprecated and is replaced by the Beam
Schema Options, it will be removed in version &lt;code>2.23.0&lt;/code>. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9704">BEAM-9704&lt;/a>)&lt;/li>
&lt;li>The &lt;code>--zone&lt;/code> option in the Dataflow runner is now deprecated. Please use &lt;code>--worker_zone&lt;/code> instead. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9716">BEAM-9716&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.21.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron Meihm, Adrian Eka, Ahmet Altay, AldairCoronel, Alex Van Boxel, Alexey Romanenko, Andrew Crites, Andrew Pilloud, Ankur Goenka, Badrul (Taki) Chowdhury, Bartok Jozsef, Boyuan Zhang, Brian Hulette, brucearctor, bumblebee-coming, Chad Dombrova, Chamikara Jayalath, Chie Hayashida, Chris Gorgolewski, Chuck Yang, Colm O hEigeartaigh, Curtis &amp;ldquo;Fjord&amp;rdquo; Hawthorne, Daniel Mills, Daniel Oliveira, David Yan, Elias Djurfeldt, Emiliano Capoccia, Etienne Chauchot, Fernando Diaz, Filipe Regadas, Gleb Kanterov, Hai Lu, Hannah Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, Hk-tang, Ismaël Mejía, Jacoby, Jan Lukavský, Jeroen Van Goey, jfarr, Jozef Vilcek, Kai Jiang, Kamil Wasilewski, Kenneth Knowles, KevinGG, Kyle Weaver, Kyoungha Min, Luke Cwik, Maximilian Michels, Michal Walenia, Ning Kang, Pablo Estrada, paul fisher, Piotr Szuberski, Reuven Lax, Robert Bradshaw, Robert Burke, Rose Nguyen, Rui Wang, Sam Rohde, Sam Whittle, Spoorti Kundargi, Steve Koonce, sunjincheng121, Ted Yun, Tesio, Thomas Weise, Tomo Suzuki, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, Yichi Zhang, yoshiki.obata, Yueyang Qiu&lt;/p></description></item><item><title>Blog: Beam Summit Digital Is Coming - Register Now!</title><link>/blog/beam-summit-digital-2020/</link><pubDate>Fri, 08 May 2020 00:00:01 -0800</pubDate><guid>/blog/beam-summit-digital-2020/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>As some of you are already aware, the 2020 edition of the Beam Summit will be completely &lt;strong>digital and free&lt;/strong>. Beam Summit Digital will take place from &lt;strong>August 24th to 28th&lt;/strong>. The conference will be spread across the course of one week with a couple of hours of program each day.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beamsummit/beamsummit-digital-2020.png"
alt="Beam Summit Digital 2020, August 24-28">&lt;/p>
&lt;p>While we would have loved to see all of you in person, we have to accept that 2020 will not be the year for that. So, we are looking at this as an opportunity to have a bigger and more inclusive event, where people who would normally not be able to travel to the summit will now be able to join, learn and share with the rest of the community.&lt;/p>
&lt;h2 id="providing-you-the-best-experience-possible">Providing you the best experience possible&lt;/h2>
&lt;p>We are going to great lengths to ensure that we provide the Beam community with the best possible experience in an online event. From audio/video quality, to an adequate schedule for our community, to making it as easy as possible to register to the event and join the sessions, to setting up ways for the community to interact and network with each other. The team behind the organization of the Beam Summit has been working on these things, and we are also teaming up with an event production company with experience in online events who are bringing in their knowledge.&lt;/p>
&lt;p>So, what we want to say with this is: We will have a great event! And if you have any ideas on how to make it better, please let us know.&lt;/p>
&lt;h2 id="ways-to-participate-and-help">Ways to participate and help&lt;/h2>
&lt;p>As all things Beam, this is a community effort. The door is open for participation:&lt;/p>
&lt;ol>
&lt;li>Submit a proposal to talk. Please check out the &lt;strong>&lt;a href="https://sessionize.com/beam-digital-summit-2020/">Call for Papers&lt;/a>&lt;/strong> and submit a talk. The deadline for submissions is &lt;em>June 15th&lt;/em>!&lt;/li>
&lt;li>Register to join as an attendee. Registration is now open at the &lt;strong>&lt;a href="https://crowdcast.io/e/beamsummit">registration page&lt;/a>&lt;/strong>. Registration is free!&lt;/li>
&lt;li>Consider sponsoring the event. If your company is interested in engaging with members of the community please check out our &lt;a href="https://drive.google.com/open?id=1EbijvZKpkWwWyMryLY9sJfyZzZk1k44v">sponsoring prospectus&lt;/a>.&lt;/li>
&lt;li>Help us get the word out. Please make sure to let your colleagues and friends in the data engineering field (and beyond!) know about the Beam Summit.&lt;/li>
&lt;/ol>
&lt;h2 id="follow-up-and-more-information">Follow up and more information&lt;/h2>
&lt;p>While we will use the Crowdcast platform to broadcast the event, we will still have a full event website at &lt;a href="https://beamsummit.org">beamsummit.org&lt;/a> with details about the schedule, speakers, FAQ and everything else you need from an event. We are currently working on updating the website and will publish all event details in the next couple of weeks.&lt;/p>
&lt;p>Please also follow us on &lt;a href="https://twitter.com/beamsummit">Twitter&lt;/a> or &lt;a href="https://www.linkedin.com/company/beam-summit/">LinkedIn&lt;/a> to get event updates.&lt;/p></description></item><item><title>Blog: Apache Beam 2.20.0</title><link>/blog/beam-2.20.0/</link><pubDate>Wed, 15 Apr 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.20.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.20.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2190-2020-02-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.20.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12346780">detailed release notes&lt;/a>.&lt;/p>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;p>Python SDK: . (#10223).&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8561">BEAM-8561&lt;/a> Adds support for Thrift encoded data via ThriftIO&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7310">BEAM-7310&lt;/a> KafkaIO supports schema resolution using Confluent Schema Registry&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7246">BEAM-7246&lt;/a> Support for Google Cloud Spanner. This is an experimental module for reading and writing data from Google Cloud Spanner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8399">BEAM-8399&lt;/a> Adds support for standard HDFS URLs (with server name)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9146">BEAM-9146&lt;/a> New AnnotateVideo &amp;amp; AnnotateVideoWithContext PTransform&amp;rsquo;s that integrates GCP Video Intelligence functionality&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9247">BEAM-9247&lt;/a> New AnnotateImage &amp;amp; AnnotateImageWithContext PTransform&amp;rsquo;s for element-wise &amp;amp; batch image annotation using Google Cloud Vision API&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9258">BEAM-9258&lt;/a> Added a PTransform for inspection and deidentification of text using Google Cloud DLP&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9248">BEAM-9248&lt;/a> New AnnotateText PTransform that integrates Google Cloud Natural Language functionality&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9305">BEAM-9305&lt;/a> ReadFromBigQuery now supports value providers for the query string&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8841">BEAM-8841&lt;/a> Added ability to write to BigQuery via Avro file loads&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9228">BEAM-9228&lt;/a> Direct runner for FnApi supports further parallelism&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8550">BEAM-8550&lt;/a> Support for @RequiresTimeSortedInput in Flink and Spark&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6857">BEAM-6857&lt;/a> Added support for dynamic timers&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-3453">BEAM-3453&lt;/a> Backwards incompatible change in ReadFromPubSub(topic=) in Python&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9310">BEAM-9310&lt;/a> SpannerAccessor in Java is now package-private to reduce API surface&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8616">BEAM-8616&lt;/a> ParquetIO hadoop dependency should be now provided by the users&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9063">BEAM-9063&lt;/a> Docker images will be deployed to apache/beam repositories from 2.20&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9579">BEAM-9579&lt;/a> Fixed numpy operators in ApproximateQuantiles&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9277">BEAM-9277&lt;/a> Fixed exception when running in IPython notebook&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-1833">BEAM-1833&lt;/a> Restructure Python pipeline construction to better follow the Runner API&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9225">BEAM-9225&lt;/a> Fixed Flink uberjar job termination bug&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9503">BEAM-9503&lt;/a> Fixed SyntaxError in process worker startup&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9322">BEAM-9322&lt;/a> Python SDK ignores manually set PCollection tags&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9445">BEAM-9445&lt;/a> Python SDK pre_optimize=all experiment may cause error&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9725">BEAM-9725&lt;/a> Python SDK performance regression for reshuffle transform&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.20.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Andrew Pilloud, Ankur Goenka, Anton Kedin, Boyuan Zhang, Brian Hulette, Brian Martin, Chamikara Jayalath
, Charles Chen, Craig Chambers, Daniel Oliveira, David Moravek, David Rieber, Dustin Rhodes, Etienne Chauchot, Gleb Kanterov, Hai Lu, Heejong Lee
, Ismaël Mejía, J Ross Thomson, Jan Lukavský, Jason Kuster, Jean-Baptiste Onofré, Jeff Klukas, João Cabrita, Juan Rael, Juta, Kasia Kucharczyk
, Kengo Seki, Kenneth Jung, Kenneth Knowles, Kyle Weaver, Kyle Winkelman, Lukas Drbal, Marek Simunek, Mark Liu, Maximilian Michels, Melissa Pashniak
, Michael Luckey, Michal Walenia, Mike Pedersen, Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Pascal Gula, Rehman Murad Ali, Reuven Lax, Rob, Robbe Sneyders
, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Williams, Sam Rohde, Sam Whittle, Scott Wegner, Shoaib Zafar, Thomas Weise, Tianyang Hu, Tyler Akidau
, Udi Meiri, Valentyn Tymofieiev, Xinyu Liu, XuMingmin, ttanay, tvalentyn, Łukasz Gajowy&lt;/p></description></item><item><title>Blog: Apache Beam 2.19.0</title><link>/blog/beam-2.19.0/</link><pubDate>Tue, 04 Feb 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.19.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.19.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2190-2020-02-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.19.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12346582">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Multiple improvements made into Python SDK harness:
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8624">BEAM-8624&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8623">BEAM-8623&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-7949">BEAM-7949&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8935">BEAM-8935&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8816">BEAM-8816&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-1440">BEAM-1440&lt;/a> Create a BigQuery source (that implements iobase.BoundedSource) for Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-2572">BEAM-2572&lt;/a> Implement an S3 filesystem for Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5192">BEAM-5192&lt;/a> Support Elasticsearch 7.x&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8745">BEAM-8745&lt;/a> More fine-grained controls for the size of a BigQuery Load job&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8801">BEAM-8801&lt;/a> PubsubMessageToRow should not check useFlatSchema() in processElement&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8953">BEAM-8953&lt;/a> Extend ParquetIO.Read/ReadFiles.Builder to support Avro GenericData model&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8946">BEAM-8946&lt;/a> Report collection size from MongoDBIOIT&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8978">BEAM-8978&lt;/a> Report saved data size from HadoopFormatIOIT&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6008">BEAM-6008&lt;/a> Improve error reporting in Java/Python PortableRunner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8296">BEAM-8296&lt;/a> Containerize the Spark job server&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8746">BEAM-8746&lt;/a> Allow the local job service to work from inside docker&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8837">BEAM-8837&lt;/a> PCollectionVisualizationTest: possible bug&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8139">BEAM-8139&lt;/a> Execute portable Spark application jar&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9019">BEAM-9019&lt;/a> Improve Spark Encoders (wrappers of beam coders)&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9053">BEAM-9053&lt;/a> Improve error message when unable to get the correct filesystem for specified path in Python SDK) Improve error message when unable to get the correct filesystem for specified path in Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9055">BEAM-9055&lt;/a> Unify the config names of Fn Data API across languages&lt;/li>
&lt;/ul>
&lt;h3 id="sql">SQL&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5690">BEAM-5690&lt;/a> Issue with GroupByKey in BeamSql using SparkRunner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8993">BEAM-8993&lt;/a> [SQL] MongoDb should use predicate push-down&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8844">BEAM-8844&lt;/a> [SQL] Create performance tests for BigQueryTable&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9023">BEAM-9023&lt;/a> Upgrade to ZetaSQL 2019.12.1&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8989">BEAM-8989&lt;/a> Backwards incompatible change in ParDo.getSideInputs (caught by failure when running Apache Nemo quickstart)&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8402">BEAM-8402&lt;/a> Backwards incompatible change related to how Environments are represented in Python &lt;code>DirectRunner&lt;/code>.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9218">BEAM-9218&lt;/a> Template staging broken on Beam 2.18.0&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8696">BEAM-8696&lt;/a> Beam Dependency Update Request: com.google.protobuf:protobuf-java&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8701">BEAM-8701&lt;/a> Beam Dependency Update Request: commons-io:commons-io&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8716">BEAM-8716&lt;/a> Beam Dependency Update Request: org.apache.commons:commons-csv&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8717">BEAM-8717&lt;/a> Beam Dependency Update Request: org.apache.commons:commons-lang3&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8749">BEAM-8749&lt;/a> Beam Dependency Update Request: com.datastax.cassandra:cassandra-driver-mapping&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5546">BEAM-5546&lt;/a> Beam Dependency Update Request: commons-codec:commons-codec&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9123">BEAM-9123&lt;/a> HadoopResourceId returns wrong directory name&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8962">BEAM-8962&lt;/a> FlinkMetricContainer causes churn in the JobManager and lets the web frontend malfunction&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5495">BEAM-5495&lt;/a> PipelineResources algorithm is not working in most environments&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8025">BEAM-8025&lt;/a> Cassandra IO classMethod test is flaky&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8577">BEAM-8577&lt;/a> FileSystems may have not be initialized during ResourceId deserialization&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8582">BEAM-8582&lt;/a> Python SDK emits duplicate records for Default and AfterWatermark triggers&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8943">BEAM-8943&lt;/a> SDK harness servers don&amp;rsquo;t shut down properly when SDK harness environment cleanup fails&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8995">BEAM-8995&lt;/a> apache_beam.io.gcp.bigquery_read_it_test failing on Py3.5 PC with: TypeError: the JSON object must be str, not &amp;lsquo;bytes&amp;rsquo;&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8999">BEAM-8999&lt;/a> PGBKCVOperation does not respect timestamp combiners&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9050">BEAM-9050&lt;/a> Beam pickler doesn&amp;rsquo;t pickle classes that have &lt;strong>module&lt;/strong> set to None.&lt;/li>
&lt;li>&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.19.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Andrew Pilloud, Ankur Goenka, Anton Kedin, Boyuan Zhang, Brian Hulette, Brian Martin, Chamikara Jayalath, Charles Chen, Craig Chambers, Daniel Oliveira, David Moravek, David Rieber, Dustin Rhodes, Etienne Chauchot, Gleb Kanterov, Hai Lu, Heejong Lee, Ismaël Mejía, Jan Lukavský, Jason Kuster, Jean-Baptiste Onofré, Jeff Klukas, João Cabrita, J Ross Thomson, Juan Rael, Juta, Kasia Kucharczyk, Kengo Seki, Kenneth Jung, Kenneth Knowles, Kyle Weaver, Kyle Winkelman, Lukas Drbal, Łukasz Gajowy, Marek Simunek, Mark Liu, Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Pedersen, Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Pascal Gula, Reuven Lax, Rob, Robbe Sneyders, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Williams, Sam Rohde, Sam Whittle, Scott Wegner, Thomas Weise, Tianyang Hu, ttanay, tvalentyn, Tyler Akidau, Udi Meiri, Valentyn Tymofieiev, Xinyu Liu, XuMingmin&lt;/p></description></item><item><title>Blog: Apache Beam 2.18.0</title><link>/blog/beam-2.18.0/</link><pubDate>Thu, 23 Jan 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.18.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.18.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2180-2020-01-23">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.18.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12346383&amp;amp;projectId=12319527">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8470">BEAM-8470&lt;/a> - Create a new Spark runner based on Spark Structured streaming framework&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7636">BEAM-7636&lt;/a> - Added SqsIO v2 support.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8513">BEAM-8513&lt;/a> - RabbitMqIO: Allow reads from exchange-bound queue without declaring the exchange.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8540">BEAM-8540&lt;/a> - Fix CSVSink example in FileIO docs&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5878">BEAM-5878&lt;/a> - Added support DoFns with Keyword-only arguments in Python 3.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6756">BEAM-6756&lt;/a> - Improved support for lazy iterables in schemas (Java).&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-4776">BEAM-4776&lt;/a> AND &lt;a href="https://issues.apache.org/jira/browse/BEAM-4777">BEAM-4777&lt;/a> - Added metrics supports to portable runners.&lt;/li>
&lt;li>Various improvements to Interactive Beam: &lt;a href="https://issues.apache.org/jira/browse/BEAM-7760">BEAM-7760&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8379">BEAM-8379&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8016">BEAM-8016&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8016">BEAM-8016&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8658">BEAM-8658&lt;/a> - Optionally set artifact staging port in FlinkUberJarJobServer.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8660">BEAM-8660&lt;/a> - Override returned artifact staging endpoint&lt;/li>
&lt;/ul>
&lt;h3 id="sql">SQL&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8343">BEAM-8343&lt;/a> - [SQL] Add means for IO APIs to support predicate and/or project push-down when running SQL pipelines. And &lt;a href="https://issues.apache.org/jira/browse/BEAM-8468">BEAM-8468&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8365">BEAM-8365&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-8508">BEAM-8508&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8427">BEAM-8427&lt;/a> - [SQL] Add support for MongoDB source.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8456">BEAM-8456&lt;/a> - Add pipeline option to control truncate of BigQuery data processed by Beam SQL.&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8814">BEAM-8814&lt;/a> - &amp;ndash;no_auth flag changed to boolean type.&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8252">BEAM-8252&lt;/a> AND &lt;a href="https://issues.apache.org/jira/browse/BEAM-8254">BEAM-8254&lt;/a> Add worker_region and worker_zone options. Deprecated &amp;ndash;zone flag and &amp;ndash;worker_region experiment argument.&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7078">BEAM-7078&lt;/a> - com.amazonaws:amazon-kinesis-client updated to 1.13.0.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8822">BEAM-8822&lt;/a> - Upgrade Hadoop dependencies to version 2.8.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7917">BEAM-7917&lt;/a> - Python datastore v1new fails on retry.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7981">BEAM-7981&lt;/a> - ParDo function wrapper doesn&amp;rsquo;t support Iterable output types.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8146">BEAM-8146&lt;/a> - SchemaCoder/RowCoder have no equals() function.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8347">BEAM-8347&lt;/a> - UnboundedRabbitMqReader can fail to advance watermark if no new data comes in.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8352">BEAM-8352&lt;/a> - Reading records in background may lead to OOM errors&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8480">BEAM-8480&lt;/a> - Explicitly set restriction coder for bounded reader wrapper SDF.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8515">BEAM-8515&lt;/a> - Ensure that ValueProvider types have equals/hashCode implemented for comparison reasons.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8579">BEAM-8579&lt;/a> - Strip UTF-8 BOM bytes (if present) in TextSource.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8657">BEAM-8657&lt;/a> - Not doing Combiner lifting for data-driven triggers.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8663">BEAM-8663&lt;/a> - BundleBasedRunner Stacked Bundles don&amp;rsquo;t respect PaneInfo.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8667">BEAM-8667&lt;/a> - Data channel should to avoid unlimited buffering in Python SDK.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8802">BEAM-8802&lt;/a> - Timestamp combiner not respected across bundles in streaming mode.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8803">BEAM-8803&lt;/a> - Default behaviour for Python BQ Streaming inserts sink should be to retry always.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8825">BEAM-8825&lt;/a> - OOM when writing large numbers of &amp;lsquo;narrow&amp;rsquo; rows.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8835">BEAM-8835&lt;/a> - Artifact retrieval fails with FlinkUberJarJobServer&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8836">BEAM-8836&lt;/a> - ExternalTransform is not providing a unique name&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8884">BEAM-8884&lt;/a> - Python MongoDBIO TypeError when splitting.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9041">BEAM-9041&lt;/a> - SchemaCoder equals should not rely on from/toRowFunction equality.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9042">BEAM-9042&lt;/a> - AvroUtils.schemaCoder(schema) produces a not serializable SchemaCoder.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9065">BEAM-9065&lt;/a> - Spark runner accumulates metrics (incorrectly) between runs.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6303">BEAM-6303&lt;/a> - Add .parquet extension to files in ParquetIO.&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8882">BEAM-8882&lt;/a> - Python: &lt;code>beam.Create&lt;/code> no longer preserves order unless &lt;code>reshuffle=False&lt;/code> is passed in as an argument.&lt;/p>
&lt;p>You may encounter this issue when using DirectRunner.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9065">BEAM-9065&lt;/a> - Spark runner accumulates metrics (incorrectly) between runs&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9123">BEAM-9123&lt;/a> - HadoopResourceId returns wrong directory name&lt;/p>
&lt;/li>
&lt;li>
&lt;p>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.18.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9144">BEAM-9144&lt;/a> - If you are using Avro 1.9.x with Beam you should not upgrade to this version. There is an issue with timestamp conversions. A fix will be available in the next release.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.18.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Aizhamal Nurmamat kyzy, Alan Myrvold, Alexey Romanenko, Alex Van Boxel, Andre Araujo, Andrew Crites, Andrew Pilloud, Aryan Naraghi, Boyuan Zhang, Brian Hulette, bumblebee-coming, Cerny Ondrej, Chad Dombrova, Chamikara Jayalath, Changming Ma, Chun Yang, cmachgodaddy, Colm O hEigeartaigh, Craig Chambers, Daniel Oliveira, Daniel Robert, David Cavazos, David Moravek, David Song, dependabot[bot], Derek, Dmytro Sadovnychyi, Elliotte Rusty Harold, Etienne Chauchot, Hai Lu, Henry Suryawirawan, Ismaël Mejía, Jack Whelpton, Jan Lukavský, Jean-Baptiste Onofré, Jeff Klukas, Jincheng Sun, Jing, Jing Chen, Joe Tsai, Jonathan Alvarez-Gutierrez, Kamil Wasilewski, KangZhiDong, Kasia Kucharczyk, Kenneth Knowles, kirillkozlov, Kirill Kozlov, Kyle Weaver, liumomo315, lostluck, Łukasz Gajowy, Luke Cwik, Mark Liu, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Niel Markwick, Ning Kang, nlofeudo, pabloem, Pablo Estrada, Pankaj Gudlani, Piotr Szczepanik, Primevenn, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, RusOr10n, Ryan Skraba, Saikat Maitra, sambvfx, Sam Rohde, Samuel Husso, Stefano, Steve Koonce, Steve Niemitz, sunjincheng121, Thomas Weise, Tianyang Hu, Tim Robertson, Tomo Suzuki, tvalentyn, Udi Meiri, Valentyn Tymofieiev, Viola Lyu, Wenjia Liu, Yichi Zhang, Yifan Zou, yoshiki.obata, Yueyang Qiu, ziel, 康智冬&lt;/p></description></item><item><title>Blog: Apache Beam 2.17.0</title><link>/blog/beam-2.17.0/</link><pubDate>Mon, 06 Jan 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.17.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.17.0 release of Beam. This release includes both improvements and new functionality.
Users of the MongoDbIO connector are encouraged to upgrade to this release to address a &lt;a href="/security/CVE-2020-1929/">security vulnerability&lt;/a>.&lt;/p>
&lt;p>See the &lt;a href="/get-started/downloads/#2170-2020-01-06">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.17.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12345970&amp;amp;projectId=12319527">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7962">BEAM-7962&lt;/a> - Drop support for Flink 1.5 and 1.6&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7635">BEAM-7635&lt;/a> - Migrate SnsIO to AWS SDK for Java 2&lt;/li>
&lt;li>Improved usability for portable Flink Runner
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8183">BEAM-8183&lt;/a> - Optionally bundle multiple pipelines into a single Flink jar.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8372">BEAM-8372&lt;/a> - Allow submission of Flink UberJar directly to flink cluster.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8471">BEAM-8471&lt;/a> - Flink native job submission for portable pipelines.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8312">BEAM-8312&lt;/a> - Flink portable pipeline jars do not need to stage artifacts remotely.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7730">BEAM-7730&lt;/a> - Add Flink 1.9 build target and Make FlinkRunner compatible with Flink 1.9.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7990">BEAM-7990&lt;/a> - Add ability to read parquet files into PCollection of pyarrow.Table.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8355">BEAM-8355&lt;/a> - Make BooleanCoder a standard coder.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8394">BEAM-8394&lt;/a> - Add withDataSourceConfiguration() method in JdbcIO.ReadRows class.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5428">BEAM-5428&lt;/a> - Implement cross-bundle state caching.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5967">BEAM-5967&lt;/a> - Add handling of DynamicMessage in ProtoCoder.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7473">BEAM-7473&lt;/a> - Update RestrictionTracker within Python to not be required to be thread safe.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7920">BEAM-7920&lt;/a> - Added AvroTableProvider to Beam SQL.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8098">BEAM-8098&lt;/a> - Improve documentation on BigQueryIO.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8100">BEAM-8100&lt;/a> - Add exception handling to Json transforms in Java SDK.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8306">BEAM-8306&lt;/a> - Improve estimation of data byte size reading from source in ElasticsearchIO.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8351">BEAM-8351&lt;/a> - Support passing in arbitrary KV pairs to sdk worker via external environment config.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8396">BEAM-8396&lt;/a> - Default to LOOPBACK mode for local flink (spark, &amp;hellip;) runner.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8410">BEAM-8410&lt;/a> - JdbcIO should support setConnectionInitSqls in its DataSource.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8609">BEAM-8609&lt;/a> - Add HllCount to Java transform catalog.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8861">BEAM-8861&lt;/a> - Disallow self-signed certificates by default in ElasticsearchIO.&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8285">BEAM-8285&lt;/a> - Upgrade ZetaSQL to 2019.09.1.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8392">BEAM-8392&lt;/a> - Upgrade pyarrow version bounds: 0.15.1&amp;lt;= to &amp;lt;0.16.0.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5895">BEAM-5895&lt;/a> - Upgrade com.rabbitmq:amqp-client to 5.7.3.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6896">BEAM-6896&lt;/a> - Upgrade PyYAML version bounds: 3.12&amp;lt;= to &amp;lt;6.0.0.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>[BEAM-8819] - AvroCoder for SpecificRecords is not serialized correctly since 2.13.0&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8989">BEAM-8989&lt;/a> Apache Nemo
runner broken due to backwards incompatible change since 2.16.0.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.17.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alexey Romanenko, Andre-Philippe Paquet, Andrew
Pilloud, angulartist, Ankit Jhalaria, Ankur Goenka, Anton Kedin, Aryan Naraghi,
Aurélien Geron, B M VISHWAS, Bartok Jozsef, Boyuan Zhang, Brian Hulette, Cerny
Ondrej, Chad Dombrova, Chamikara Jayalath, ChethanU, cmach, Colm O hEigeartaigh,
Cyrus Maden, Daniel Oliveira, Daniel Robert, Dante, David Cavazos, David
Moravek, David Yan, Enrico Canzonieri, Etienne Chauchot, gxercavins, Hai Lu,
Hannah Jiang, Ian Lance Taylor, Ismaël Mejía, Israel Herraiz, James Wen, Jan
Lukavský, Jean-Baptiste Onofré, Jeff Klukas, jesusrv1103, Jofre, Kai Jiang,
Kamil Wasilewski, Kasia Kucharczyk, Kenneth Knowles, Kirill Kozlov,
kirillkozlov, Kohki YAMAGIWA, Kyle Weaver, Leonardo Alves Miguel, lloigor,
lostluck, Luis Enrique Ortíz Ramirez, Luke Cwik, Mark Liu, Maximilian Michels,
Michal Walenia, Mikhail Gryzykhin, mrociorg, Nicolas Delsaux, Ning Kang, NING
KANG, Pablo Estrada, pabloem, Piotr Szczepanik, rahul8383, Rakesh Kumar, Renat
Nasyrov, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ruslan Altynnikov,
Ryan Skraba, Salman Raza, Saul Chavez, Sebastian Jambor, sunjincheng121, Tatu
Saloranta, tchiarato, Thomas Weise, Tomo Suzuki, Tudor Marian, tvalentyn, Udi
Meiri, Valentyn Tymofieiev, Viola Lyu, Vishwas, Yichi Zhang, Yifan Zou, Yueyang
Qiu, Łukasz Gajowy&lt;/p></description></item><item><title>Blog: Apache Beam 2.16.0</title><link>/blog/beam-2.16.0/</link><pubDate>Mon, 07 Oct 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.16.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.16.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2160-2019-10-07">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.16.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12345494">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Customizable Docker container images released and supported by Beam portable runners on Python 2.7, 3.5, 3.6, 3.7. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7907">BEAM-7907&lt;/a>)&lt;/li>
&lt;li>Integration improvements for Python Streaming on Dataflow including service features like autoscaling, drain, update, streaming engine and counter updates.&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>A new count distinct transform based on BigQuery compatible HyperLogLog++ implementation. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7013">BEAM-7013&lt;/a>)&lt;/li>
&lt;li>Element counters in the Web UI graph representations for transforms for Python streaming jobs in Google Cloud Dataflow. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7045">BEAM-7045&lt;/a>)&lt;/li>
&lt;li>Add SetState in Python sdk. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7741">BEAM-7741&lt;/a>)&lt;/li>
&lt;li>Add hot key detection to Dataflow Runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7820">BEAM-7820&lt;/a>)&lt;/li>
&lt;li>Add ability to get the list of submitted jobs from gRPC JobService. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7927">BEAM-7927&lt;/a>)&lt;/li>
&lt;li>Portable Flink pipelines can now be bundled into executable jars. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7966">BEAM-7966&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-7967">BEAM-7967&lt;/a>)&lt;/li>
&lt;li>SQL join selection should be done in planner, not in expansion to PTransform. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6114">BEAM-6114&lt;/a>)&lt;/li>
&lt;li>A Python Sink for BigQuery with File Loads in Streaming. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6611">BEAM-6611&lt;/a>)&lt;/li>
&lt;li>Python BigQuery sink should be able to handle 15TB load job quota. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7588">BEAM-7588&lt;/a>)&lt;/li>
&lt;li>Spark portable runner: reuse SDK harness. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7600">BEAM-7600&lt;/a>)&lt;/li>
&lt;li>BigQuery File Loads to work well with load job size limits. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7742">BEAM-7742&lt;/a>)&lt;/li>
&lt;li>External environment with containerized worker pool. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7980">BEAM-7980&lt;/a>)&lt;/li>
&lt;li>Use OffsetRange as restriction for OffsetRestrictionTracker. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8014">BEAM-8014&lt;/a>)&lt;/li>
&lt;li>Get logs for SDK worker Docker containers. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8015">BEAM-8015&lt;/a>)&lt;/li>
&lt;li>PCollection boundedness is tracked and propagated in python sdk. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8088">BEAM-8088&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>Upgrade &amp;ldquo;com.amazonaws:amazon-kinesis-producer&amp;rdquo; to version 0.13.1. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7894">BEAM-7894&lt;/a>)&lt;/li>
&lt;li>Upgrade to joda time 2.10.3 to get updated TZDB. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8161">BEAM-8161&lt;/a>)&lt;/li>
&lt;li>Upgrade Jackson to version 2.9.10. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8299">BEAM-8299&lt;/a>)&lt;/li>
&lt;li>Upgrade grpcio minimum required version to 1.12.1. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7986">BEAM-7986&lt;/a>)&lt;/li>
&lt;li>Upgrade funcsigs minimum required version to 1.0.2 in Python2. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7060">BEAM-7060&lt;/a>)&lt;/li>
&lt;li>Upgrade google-cloud-pubsub maximum required version to 1.0.0. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5539">BEAM-5539&lt;/a>)&lt;/li>
&lt;li>Upgrade google-cloud-bigtable maximum required version to 1.0.0. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5539">BEAM-5539&lt;/a>)&lt;/li>
&lt;li>Upgrade dill version to 0.3.0. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8324">BEAM-8324&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Given that Python 2 will reach EOL on Jan 1 2020, Python 2 users of Beam will now receive a warning that new releases of Apache Beam will soon support Python 3 only.&lt;/li>
&lt;li>Filesystems not properly registered using FileIO.write in FlinkRunner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8303">BEAM-8303&lt;/a>)&lt;/li>
&lt;li>Performance regression in Java DirectRunner in streaming mode. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8363">BEAM-8363&lt;/a>)&lt;/li>
&lt;li>Can&amp;rsquo;t install the Python SDK on macOS 10.15. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8368">BEAM-8368&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.16.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Van Boxel, Alexey Romanenko, Alexey Strokach, Alireza Samadian,
Andre-Philippe Paquet, Andrew Pilloud, Ankur Goenka, Anton Kedin, Aryan Naraghi,
B M VISHWAS, Bartok Jozsef, Bill Neubauer, Boyuan Zhang, Brian Hulette, Bruno Volpato,
Chad Dombrova, Chamikara Jayalath, Charith Ellawala, Charles Chen, Claire McGinty,
Cyrus Maden, Daniel Oliveira, Dante, David Cavazos, David Moravek, David Yan,
Dominic Mitchell, Elias Djurfeldt, Enrico Canzonieri, Etienne Chauchot, Gleb Kanterov,
Hai Lu, Hannah Jiang, Heejong Lee, Ian Lance Taylor, Ismaël Mejía, Jack Whelpton,
James Wen, Jan Lukavský, Jean-Baptiste Onofré, Jofre, Kai Jiang, Kamil Wasilewski,
Kasia Kucharczyk, Kenneth Jung, Kenneth Knowles, Kirill Kozlov, Kohki YAMAGIWA,
Kyle Weaver, Kyle Winkelman, Ludovic Post, Luis Enrique Ortíz Ramirez, Luke Cwik,
Mark Liu, Maximilian Michels, Michal Walenia, Mike Kaplinskiy, Mikhail Gryzykhin,
NING KANG, Oliver Henlich, Pablo Estrada, Rakesh Kumar, Renat Nasyrov, Reuven Lax,
Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Skraba, Sahith Nallapareddy,
Salman Raza, Sam Rohde, Saul Chavez, Shoaib, Shoaib Zafar, Slava Chernyak, Tanay Tummalapalli,
Thinh Ha, Thomas Weise, Tianzi Cai, Tim van der Lippe, Tomer Zeltzer, Tudor Marian,
Udi Meiri, Valentyn Tymofieiev, Yichi Zhang, Yifan Zou, Yueyang Qiu, gxercavins,
jesusrv1103, lostluck, matt-darwin, mrociorg, ostrokach, parahul, rahul8383, rosetn,
sunjincheng121, the1plummie, ttanay, tvalentyn, venn001, yoshiki.obata, Łukasz Gajowy&lt;/p></description></item><item><title>Blog: Google Summer of Code '19</title><link>/blog/gsoc-19/</link><pubDate>Wed, 04 Sep 2019 00:00:01 -0800</pubDate><guid>/blog/gsoc-19/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Google Summer of Code was an amazing learning experience for me.
I contributed to open source, learned about Apache Beam’s internals and worked with the best engineers in the world.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Two of my friends had participated in GSoC in 2018. I was intrigued by their experience.
The idea of working on open-source software that could potentially be used by developers across the world, while being mentored by the best people in a field was exciting!
So, I decided to give Google Summer of Code a shot this year.&lt;/p>
&lt;h2 id="what-is-google-summer-of-code">What is Google Summer of Code?&lt;/h2>
&lt;p>&lt;a href="https://summerofcode.withgoogle.com/">Google Summer of Code&lt;/a> is a global program hosted by Google focused on introducing students to open source software development.
Students work on a 3 month programming project with an open source organization during their break from university.&lt;/p>
&lt;h2 id="why-apache-beam">Why Apache Beam?&lt;/h2>
&lt;p>While interning at &lt;a href="https://atlan.com/">Atlan&lt;/a>, I discovered the field of Data Engineering. I found the challenges and the discussions of the engineers there interesting. While researching for my internship project, I came across the Streaming Systems book. It introduced me to the unified model of Apache Beam for Batch and Streaming Systems, which I was fascinated by.
I wanted to explore Data Engineering, so for GSoC, I wanted to work on a project in that field. Towards the end of my internship, I started contributing to Apache Airflow(very cool project) and Apache Beam, hoping one of them would participate in GSoC. I got lucky!&lt;/p>
&lt;p>&lt;a href="https://youtu.be/U2eWLb-LD44">Also, Spotify’s Discover Weekly uses Apache Beam!&lt;/a>&lt;/p>
&lt;h2 id="preparation">Preparation&lt;/h2>
&lt;p>I had already read the &lt;a href="http://streamingsystems.net/">Streaming Systems book&lt;/a>. So, I had an idea of the concepts that Beam is built on, but had never actually used Beam.
Before actually submitting a proposal, I went through a bunch of resources to make sure I had a concrete understanding of Beam.
I read the &lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">Streaming 101&lt;/a> and &lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102">Streaming 102&lt;/a> blogs by Tyler Akidau. They are the perfect introduction to Beam’s unified model for Batch and Streaming.
In addition, I watched all Beam talks on YouTube. You can find them on the &lt;a href="https://beam.apache.org/documentation/resources/videos-and-podcasts/">Beam Website&lt;/a>.
Beam has really good documentation. The &lt;a href="https://beam.apache.org/documentation/programming-guide/">Programming Guide&lt;/a> lays out all of Beam’s concepts really well. &lt;a href="https://beam.apache.org/documentation/runtime/model">Beam’s execution model&lt;/a> is also documented well and is a must-read to understand how Beam processes data.
&lt;a href="https://www.waitingforcode.com/apache-beam">waitingforcode.com&lt;/a> also has good blog posts about Beam concepts.
To get a better sense of the Beam codebase, I played around with it and worked on some PRs to understand Beam better and got familiar with the test suite and workflows.&lt;/p>
&lt;h2 id="gsoc-journey">GSoC Journey&lt;/h2>
&lt;p>GSoC has 2 phases. The first is the Community Bonding period in which students get familiar with the project and the community. The other is the actual Coding Period in which students work on their projects. Since the Coding Period has three evaluations spaced out by a month, I divided my project into three parts focusing on the implementation, tests, and documentation or improvements.&lt;/p>
&lt;h3 id="project">Project&lt;/h3>
&lt;p>My project(&lt;a href="https://issues.apache.org/jira/browse/BEAM-6611">BEAM-6611&lt;/a>) added support for File Loads method of inserting data into BigQuery for streaming pipelines. It builds on PR - &lt;a href="https://github.com/apache/beam/pull/7655">#7655&lt;/a> for &lt;a href="https://issues.apache.org/jira/browse/BEAM-6553">BEAM-6553&lt;/a> that added support in the Python SDK for writing to BigQuery using File Loads method for Batch pipelines. Streaming pipelines with non-default Windowing, Triggering and Accumulation mode can write data to BigQuery using file loads method. In case of failure, the pipeline will fail atomically. This means that each record will be loaded into BigQuery at-most-once.
You can find my proposal &lt;a href="https://docs.google.com/document/d/15Peyd3Z_wu5rvGWw8lMLpZuTyyreM_JOAEFFWvF97YY/edit?usp=sharing">here&lt;/a>.&lt;/p>
&lt;h3 id="community-bonding">Community Bonding&lt;/h3>
&lt;p>When GSoC started, my semester end exams had not yet finished. As a result, I couldn’t get much done. I worked on three PTransforms for the Python SDK - Latest, WithKeys and Reify.&lt;/p>
&lt;h3 id="coding-period-i">Coding Period I&lt;/h3>
&lt;p>In this period, I wrote some Integration Tests for the BigQuery sink using Streaming Inserts in streaming mode. I worked on a failing integration test for my project. I also finished the implementation of my project. But, one PostCommit test didn’t pass. I realized that the matcher for the Integration Test that queried BigQuery for the results was intended to be used in Batch mode. So, I wrote a version of the matcher to work in streaming mode.&lt;/p>
&lt;h3 id="coding-period-ii">Coding Period II&lt;/h3>
&lt;p>Even after I had added the matcher for streaming mode, the PostComit tests did not pass. A test was being run even though it was not specified. I isolated the failure to a &lt;a href="https://nose.readthedocs.io/en/latest/doc_tests/test_multiprocess/multiprocess.html#other-differences-in-test-running">limitation&lt;/a> of the multiprocess plugin for &lt;a href="https://nose.readthedocs.io/en/latest/">nose(a Python test framework)&lt;/a> due to which it found more tests than had been specified. It took me a while to figure this out. In this period, changes for my project got merged.
I also worked on small issues related to testing.&lt;/p>
&lt;p>This period was marked by a few exciting events:&lt;/p>
&lt;ul>
&lt;li>Ending up in the top #100 contributors to apache/beam.&lt;/li>
&lt;li>My first ever PR Review on an open source project.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://pbs.twimg.com/media/D_XNSC-UIAUmswG?format=png&amp;name=small" alt="Weird flex but ok" />&lt;/p>
&lt;h3 id="coding-period-iii">Coding Period III&lt;/h3>
&lt;p>This was the final coding period before the program ended. Since my project was merged earlier than expected, my mentor suggested another issue(&lt;a href="https://issues.apache.org/jira/browse/BEAM-7742">BEAM-7742&lt;/a>) in the same area - BigQueryIO, that I found interesting. So, I worked on partitioning written files in BigQuery to ensure that all load jobs triggered adhere to the load job size limitations specified for BigQuery.
While working on my project, I was using a pipeline that uses PubSub as a source and BigQuery as a sink to validate my changes. My mentor suggested we add them to the Beam test suite as it would be the ultimate test for BigQueryIO. I also worked on adding this test to Beam.&lt;/p>
&lt;p>You can find the list of PRs I worked on &lt;a href="https://github.com/apache/beam/pulls?utf8=%E2%9C%93&amp;amp;q=is%3Apr+author%3Attanay">here&lt;/a>.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>GSoC has been a lesson in discipline and goal-setting for me. Deciding what I wanted to work on and how much I wanted to get done each week was an important lesson.
I had never worked remotely, so this was a new experience. Although I struggled with it initially, I appreciate the flexibility that it comes with.
I also had a lot of fun learning about Apache Beam’s internals, and other tools in the same ecosystem.
This was also the first time I had written code with a test-first approach.&lt;/p>
&lt;p>I thank my mentor - Pablo Estrada, Apache Beam, The Apache Software Foundation and Google Summer of Code for this opportunity. I am also grateful to my mentor for helping me with everything I needed and more, and the Apache Beam community for being supportive and encouraging.&lt;/p>
&lt;p>With the right effort, perseverance, conviction, and a plan, anything is possible. Anything.&lt;/p></description></item><item><title>Blog: Apache Beam 2.15.0</title><link>/blog/beam-2.15.0/</link><pubDate>Thu, 22 Aug 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.15.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.15.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2150-2019-08-22">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.15.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12345489">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Vendored Guava was upgraded to version 26.0.&lt;/li>
&lt;li>Support multi-process execution on the FnApiRunner for Python. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3645">BEAM-3645&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Add AvroIO.sink for IndexedRecord (FileIO compatible). (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6480">BEAM-6480&lt;/a>)&lt;/li>
&lt;li>Add support for writing to BigQuery clustered tables. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5191">BEAM-5191&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Support ParquetTable in SQL. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7728">BEAM-7728&lt;/a>)&lt;/li>
&lt;li>Add hot key detection to Dataflow Runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7820">BEAM-7820&lt;/a>)&lt;/li>
&lt;li>Support schemas in the JDBC sink. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6675">BEAM-6675&lt;/a>)&lt;/li>
&lt;li>Report GCS throttling time to Dataflow autoscaler for better autoscaling. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7667">BEAM-7667&lt;/a>)&lt;/li>
&lt;li>Support transform_name_mapping option in Python SDK for &lt;code>--update&lt;/code> use. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7761">BEAM-7761&lt;/a>)&lt;/li>
&lt;li>Dependency: Upgrade Jackson databind to version 2.9.9.3 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7880">BEAM-7880&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7616">BEAM-7616&lt;/a> urlopen calls may get stuck. (Regression from 2.14.0)&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8111">BEAM-8111&lt;/a> SchemaCoder fails on Dataflow, preventing the use of SqlTransform and schema-aware transforms. (Regression from 2.14.0)&lt;/li>
&lt;li>(&lt;a href="https://issues.apache.org/jira/browse/BEAM-8368">BEAM-8368&lt;/a>) Can&amp;rsquo;t install the Python SDK on macOS 10.15.&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;code>--region&lt;/code> flag will be a required flag in the future for Dataflow. A warning is added to warn for this future change. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7833">BEAM-7833&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.15.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alexey Romanenko, Alex Goos, Alireza Samadian, Andrew Pilloud, Ankur Goenka,
Anton Kedin, Aryan Naraghi, Bartok Jozsef, bmv126, B M VISHWAS, Boyuan Zhang,
Brian Hulette, brucearctor, Cade Markegard, Cam Mach, Chad Dombrova,
Chaim Turkel, Chamikara Jayalath, Charith Ellawala, Claire McGinty, Craig Chambers,
Daniel Oliveira, David Cavazos, David Moravek, Dominic Mitchell, Dustin Rhodes,
Etienne Chauchot, Filipe Regadas, Gleb Kanterov, Gunnar Schulze, Hannah Jiang,
Heejong Lee, Henry Suryawirawan, Ismaël Mejía, Ivo Galic, Jan Lukavský,
Jawad, Juta, Juta Staes, Kai Jiang, Kamil Wasilewski, Kasia Kucharczyk,
Kenneth Jung, Kenneth Knowles, Kyle Weaver, Lily Li, Logan HAUSPIE, lostluck,
Łukasz Gajowy, Luke Cwik, Mark Liu, Matt Helm, Maximilian Michels,
Michael Luckey, Mikhail Gryzykhin, Neville Li, Nicholas Rucci, pabloem,
Pablo Estrada, Paul King, Paul Suganthan, Raheel Khan, Rakesh Kumar,
Reza Rokni, Robert Bradshaw, Robert Burke, rosetn, Rui Wang, Ryan Skraba, RyanSkraba,
Sahith Nallapareddy, Sam Rohde, Sam Whittle, Steve Niemitz, Tanay Tummalapalli, Thomas Weise,
Tianyang Hu, ttanay, tvalentyn, Udi Meiri, Valentyn Tymofieiev, Wout Scheepers,
yanzhi, Yekut, Yichi Zhang, Yifan Zou, yoshiki.obata, Yueyang Qiu, Yunqing Zhou&lt;/p></description></item><item><title>Blog: Apache Beam 2.14.0</title><link>/blog/beam-2.14.0/</link><pubDate>Wed, 31 Jul 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.14.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.14.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2140-2019-08-01">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.14.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12345431">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python 3 support is extended to Python 3.6 and 3.7; in addition to various other Python 3 &lt;a href="https://issues.apache.org/jira/browse/BEAM-1251?focusedCommentId=16890504&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16890504">improvements&lt;/a>.&lt;/li>
&lt;li>Spark portable runner (batch) now &lt;a href="https://lists.apache.org/thread.html/c43678fc24c9a1dc9f48c51c51950aedcb9bc0fd3b633df16c3d595a@%3Cuser.beam.apache.org%3E">available&lt;/a> for Java, Python, Go.&lt;/li>
&lt;li>Added new runner: Hazelcast Jet Runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7305">BEAM-7305&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Schema support added to BigQuery reads. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6673">BEAM-6673&lt;/a>)&lt;/li>
&lt;li>Schema support added to JDBC source. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6674">BEAM-6674&lt;/a>)&lt;/li>
&lt;li>BigQuery support for &lt;code>bytes&lt;/code> is fixed. (Python 3) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6769">BEAM-6769&lt;/a>)&lt;/li>
&lt;li>Added DynamoDB IO. (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7043">BEAM-7043&lt;/a>)&lt;/li>
&lt;li>Added support unbounded reads with HCatalogIO (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7450">BEAM-7450&lt;/a>)&lt;/li>
&lt;li>Added BoundedSource wrapper for SDF. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7443">BEAM-7443&lt;/a>)&lt;/li>
&lt;li>Added support for INCRBY/DECRBY operations in RedisIO. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7286">BEAM-7286&lt;/a>)&lt;/li>
&lt;li>Added Support for ValueProvider defined GCS Location for WriteToBigQuery with File Loads. (Java) ((&lt;a href="https://issues.apache.org/jira/browse/BEAM-7603">BEAM-7603&lt;/a>))&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Python SDK add support for DoFn &lt;code>setup&lt;/code> and &lt;code>teardown&lt;/code> methods. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-562">BEAM-562&lt;/a>)&lt;/li>
&lt;li>Python SDK adds new transforms: &lt;a href="https://issues.apache.org/jira/browse/BEAM-6693">ApproximateUnique&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-6695">Latest&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-7019">Reify&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-7021">ToString&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-7023">WithKeys&lt;/a>.&lt;/li>
&lt;li>Added hook for user-defined JVM initialization in workers. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6872">BEAM-6872&lt;/a>)&lt;/li>
&lt;li>Added support for SQL Row Estimation for BigQueryTable. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7513">BEAM-7513&lt;/a>)&lt;/li>
&lt;li>Auto sharding of streaming sinks in FlinkRunner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5865">BEAM-5865&lt;/a>)&lt;/li>
&lt;li>Removed the Hadoop dependency from the external sorter. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7268">BEAM-7268&lt;/a>)&lt;/li>
&lt;li>Added option to expire portable SDK worker environments. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7348">BEAM-7348&lt;/a>)&lt;/li>
&lt;li>Beam does not relocate Guava anymore and depends only on its own vendored version of Guava. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6620">BEAM-6620&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>Deprecated set/getClientConfiguration in Jdbc IO. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7263">BEAM-7263&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Fixed reading of concatenated compressed files. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6952">BEAM-6952&lt;/a>)&lt;/li>
&lt;li>Fixed re-scaling issues on Flink &amp;gt;= 1.6 versions. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7144">BEAM-7144&lt;/a>)&lt;/li>
&lt;li>Fixed SQL EXCEPT DISTINCT behavior. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7194">BEAM-7194&lt;/a>)&lt;/li>
&lt;li>Fixed OOM issues with bounded Reads for Flink Runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7442">BEAM-7442&lt;/a>)&lt;/li>
&lt;li>Fixed HdfsFileSystem to correctly match directories. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7561">BEAM-7561&lt;/a>)&lt;/li>
&lt;li>Upgraded Spark runner to use spark version 2.4.3. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7265">BEAM-7265&lt;/a>)&lt;/li>
&lt;li>Upgraded Jackson to version 2.9.9. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7465">BEAM-7465&lt;/a>)&lt;/li>
&lt;li>Various other bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Do &lt;strong>NOT&lt;/strong> use Python MongoDB source in this release. Python MongoDB source &lt;a href="https://issues.apache.org/jira/browse/BEAM-5148">added&lt;/a> in this release has a known issue that can result in data loss. See (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7866">BEAM-7866&lt;/a>) for details.&lt;/li>
&lt;li>Can&amp;rsquo;t install the Python SDK on macOS 10.15. See (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8368">BEAM-8368&lt;/a>) for details.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.14.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Aizhamal Nurmamat kyzy, Ajo Thomas, Alex Amato, Alexey Romanenko,
Alexey Strokach, Alex Van Boxel, Alireza Samadian, Andrew Pilloud,
Ankit Jhalaria, Ankur Goenka, Anton Kedin, Aryan Naraghi, Bartok Jozsef,
Bora Kaplan, Boyuan Zhang, Brian Hulette, Cam Mach, Chamikara Jayalath,
Charith Ellawala, Charles Chen, Colm O hEigeartaigh, Cyrus Maden,
Daniel Mills, Daniel Oliveira, David Cavazos, David Moravek, David Yan,
Daniel Lescohier, Elwin Arens, Etienne Chauchot, Fábio Franco Uechi,
Finch Keung, Frederik Bode, Gregory Kovelman, Graham Polley, Hai Lu, Hannah Jiang,
Harshit Dwivedi, Harsh Vardhan, Heejong Lee, Henry Suryawirawan,
Ismaël Mejía, Jan Lukavský, Jean-Baptiste Onofré, Jozef Vilcek, Juta, Kai Jiang,
Kamil Wu, Kasia Kucharczyk, Kenneth Knowles, Kyle Weaver, Lara Schmidt,
Łukasz Gajowy, Luke Cwik, Manu Zhang, Mark Liu, Matthias Baetens,
Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia,
Mikhail Gryzykhin, Ming Liang, Neville Li, Pablo Estrada, Paul Suganthan,
Peter Backx, Rakesh Kumar, Rasmi Elasmar, Reuven Lax, Reza Rokni, Robbe Sneyders,
Robert Bradshaw, Robert Burke, Rose Nguyen, Rui Wang, Ruoyun Huang,
Shoaib Zafar, Slava Chernyak, Steve Niemitz, Tanay Tummalapalli, Thomas Weise,
Tim Robertson, Tim van der Lippe, Udi Meiri, Valentyn Tymofieiev, Varun Dhussa,
Viktor Gerdin, Yichi Zhang, Yifan Mai, Yifan Zou, Yueyang Qiu.&lt;/p></description></item><item><title>Blog: Looping timers in Apache Beam</title><link>/blog/looping-timers/</link><pubDate>Tue, 11 Jun 2019 00:00:01 -0800</pubDate><guid>/blog/looping-timers/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Apache Beam’s primitives let you build expressive data pipelines, suitable for a
variety of use cases. One specific use case is the analysis of time series data
in which continuous sequences across window boundaries are important. A few fun
challenges arise as you tackle this type of data and in this blog we will
explore one of those in more detail and make use of the Timer API
(&lt;a href="/blog/2017/08/28/timely-processing.html">blog post&lt;/a>)
using the &amp;ldquo;looping timer&amp;rdquo; pattern.&lt;/p>
&lt;p>With Beam in streaming mode, you can take streams of data and build analytical
transforms to produce results on the data. But for time series data, the absence
of data is useful information. So how can we produce results in the absence of
data?&lt;/p>
&lt;p>Let&amp;rsquo;s use a more concrete example to illustrate the requirement. Imagine you
have a simple pipeline that sums the number of events coming from an IoT device
every minute. We would like to produce the value 0 when no data has been seen
within a specific time interval. So why can this get tricky? Well it is easy to
build a simple pipeline that counts events as they arrive, but when there is no
event, there is nothing to count!&lt;/p>
&lt;p>Let&amp;rsquo;s build a simple pipeline to work with:&lt;/p>
&lt;pre>&lt;code> // We will start our timer at 1 sec from the fixed upper boundary of our
// minute window
Instant now = Instant.parse(&amp;quot;2000-01-01T00:00:59Z&amp;quot;);
// ----- Create some dummy data
// Create 3 elements, incrementing by 1 minute and leaving a time gap between
// element 2 and element 3
TimestampedValue&amp;lt;KV&amp;lt;String, Integer&amp;gt;&amp;gt; time_1 =
TimestampedValue.of(KV.of(&amp;quot;Key_A&amp;quot;, 1), now);
TimestampedValue&amp;lt;KV&amp;lt;String, Integer&amp;gt;&amp;gt; time_2 =
TimestampedValue.of(KV.of(&amp;quot;Key_A&amp;quot;, 2),
now.plus(Duration.standardMinutes(1)));
// No Value for start time + 2 mins
TimestampedValue&amp;lt;KV&amp;lt;String, Integer&amp;gt;&amp;gt; time_3 =
TimestampedValue.of(KV.of(&amp;quot;Key_A&amp;quot;, 3),
now.plus(Duration.standardMinutes(3)));
// Create pipeline
PipelineOptions options = PipelineOptionsFactory.fromArgs(args).withValidation()
.as(PipelineOptions.class);
Pipeline p = Pipeline.create(options);
// Apply a fixed window of duration 1 min and Sum the results
p.apply(Create.timestamped(time_1, time_2, time_3))
.apply(
Window.&amp;lt;KV&amp;lt;String,Integer&amp;gt;&amp;gt;into(
FixedWindows.&amp;lt;Integer&amp;gt;of(Duration.standardMinutes(1))))
.apply(Sum.integersPerKey())
.apply(ParDo.of(new DoFn&amp;lt;KV&amp;lt;String, Integer&amp;gt;, KV&amp;lt;String, Integer&amp;gt;&amp;gt;() {
@ProcessElement public void process(ProcessContext c) {
LOG.info(&amp;quot;Value is {} timestamp is {}&amp;quot;, c.element(), c.timestamp());
}
}));
p.run();
&lt;/code>&lt;/pre>&lt;p>Running that pipeline will result in the following output:&lt;/p>
&lt;pre>&lt;code>INFO LoopingTimer - Value is KV{Key_A, 1} timestamp is 2000-01-01T00:00:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 3} timestamp is 2000-01-01T00:03:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 2} timestamp is 2000-01-01T00:01:59.999Z
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>Note: The lack of order in the output should be expected, however the
key-window tuple is correctly computed.&lt;/p>
&lt;/blockquote>
&lt;p>As expected, we see output in each of the interval windows which had a data
point with a timestamp between the minimum and maximum value of the window.
There was a data point at timestamps 00:00:59, 00:01:59 and 00:03:59, which
fell into the following interval windows.&lt;/p>
&lt;ul>
&lt;li>[00:00:00, 00:00:59.999)&lt;/li>
&lt;li>[00:01:00, 00:01:59.999)&lt;/li>
&lt;li>[00:03:00, 00:03:59.999)&lt;/li>
&lt;/ul>
&lt;p>But as there was no data between 00:02:00 and 00:02:59, no value is produced
for interval window [00:02:00,00:02:59.999).&lt;/p>
&lt;p>How can we get Beam to output values for that missing window? First, let’s walk
through some options that do not make use of the Timer API.&lt;/p>
&lt;h2 id="option-1-external-heartbeat">Option 1: External heartbeat&lt;/h2>
&lt;p>We can use an external system to emit a value for each time interval and inject
it into the stream of data that Beam consumes. This simple option moves any
complexity out of the Beam pipeline. But using an external system means we need
to monitor this system and perform other maintenance tasks in tandem with the
Beam pipeline.&lt;/p>
&lt;h2 id="option-2-use-a-generated-source-in-the-beam-pipeline">Option 2: Use a generated source in the Beam pipeline&lt;/h2>
&lt;p>We can use a generating source to emit the value using this code snippet:&lt;/p>
&lt;pre>&lt;code>pipeline.apply(GenerateSequence.
from(0).withRate(1,Duration.standardSeconds(1L)))
&lt;/code>&lt;/pre>&lt;p>We can then:&lt;/p>
&lt;ol>
&lt;li>Use a DoFn to convert the value to zero.&lt;/li>
&lt;li>Flatten this value with the real source.&lt;/li>
&lt;li>Produce a PCollection which has ticks in every time interval.&lt;/li>
&lt;/ol>
&lt;p>This is also a simple way of producing a value in each time interval.&lt;/p>
&lt;h2 id="option-1--2-the-problem-with-multiple-keys">Option 1 &amp;amp; 2 The problem with multiple keys&lt;/h2>
&lt;p>Both options 1 and 2 work well for the case where there the pipeline processes a
single key. Let’s now deal with the case where instead of 1 IoT device, there
are 1000s or 100,000s of these devices, each with a unique key. To make option 1
or option 2 work in this scenario, we need to carry out an extra step: creating
a FanOut DoFn. Each tick needs to be distributed to all the potential keys, so
we need to create a FanOut DoFn that takes the dummy value and generates a
key-value pair for every available key.&lt;/p>
&lt;p>For example, let&amp;rsquo;s assume we have 3 keys for 3 IoT devices, {key1,key2,key3}.
Using the method we outlined in Option 2 when we get the first element from
GenerateSequence, we need to create a loop in the DoFn that generates 3
key-value pairs. These pairs become the heartbeat value for each of the IoT
devices.&lt;/p>
&lt;p>And things get a lot more fun when we need to deal with lots of IoT devices,
with a list of keys that are dynamically changing. We would need to add a
transform that does a Distinct operation and feed the data produced as a
side-input into the FanOut DoFn.&lt;/p>
&lt;h2 id="option-3-implementing-a-heartbeat-using-beam-timers">Option 3: Implementing a heartbeat using Beam timers&lt;/h2>
&lt;p>So how do timers help? Well let&amp;rsquo;s have a look at a new transform:&lt;/p>
&lt;p>Edit: Looping Timer State changed from Boolean to Long to allow for min value check.&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">LoopingStatefulTimer&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">Instant&lt;/span> &lt;span class="n">stopTimerTime&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="n">LoopingStatefulTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Instant&lt;/span> &lt;span class="n">stopTime&lt;/span>&lt;span class="o">){&lt;/span>
&lt;span class="k">this&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">stopTimerTime&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">stopTime&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimerTime&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">loopingTimerTime&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">BigEndianLongCoder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;key&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">key&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">StringUtf8Coder&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">TimerSpec&lt;/span> &lt;span class="n">loopingTimer&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">TimerSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">EVENT_TIME&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;key&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimerTime&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">loopingTimerTime&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">loopingTimer&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="c1">// If the timer has been set already, or if the value is smaller than
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="c1">// the current element + window duration, do not set
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">Long&lt;/span> &lt;span class="n">currentTimerValue&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loopingTimerTime&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">Instant&lt;/span> &lt;span class="n">nextTimerTimeBasedOnCurrentElement&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamp&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">currentTimerValue&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="kc">null&lt;/span> &lt;span class="o">||&lt;/span> &lt;span class="n">currentTimerValue&lt;/span> &lt;span class="o">&amp;gt;&lt;/span>
&lt;span class="n">nextTimerTimeBasedOnCurrentElement&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getMillis&lt;/span>&lt;span class="o">())&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">loopingTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">set&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nextTimerTimeBasedOnCurrentElement&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">loopingTimerTime&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nextTimerTimeBasedOnCurrentElement&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getMillis&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="c1">// We need this value so that we can output a value for the correct key in OnTimer
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="kc">null&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">key&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getKey&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@OnTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">onTimer&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">OnTimerContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;key&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">key&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;loopingTimer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">loopingTimer&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">LOG&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">info&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;Timer @ {} fired&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamp&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="c1">// If we do not put in a “time to live” value, then the timer would loop forever
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">Instant&lt;/span> &lt;span class="n">nextTimer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamp&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">nextTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">isBefore&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">stopTimerTime&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">loopingTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">set&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">nextTimer&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">LOG&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">info&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="s">&amp;#34;Timer not being set as exceeded Stop Timer value {} &amp;#34;&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">stopTimerTime&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>There are two data values that the state API needs to keep:&lt;/p>
&lt;ol>
&lt;li>A boolean &lt;code>timeRunning&lt;/code> value used to avoid resetting the timer if it’s
already running.&lt;/li>
&lt;li>A &amp;ldquo;&lt;em>key&lt;/em>&amp;rdquo; state object value that allows us to store the key that we are
working with. This information will be needed in the &lt;code>OnTimer&lt;/code> event later.&lt;/li>
&lt;/ol>
&lt;p>We also have a Timer with the ID &lt;code>**loopingTimer**&lt;/code> that acts as our per
interval alarm clock. Note that the timer is an &lt;em>event timer&lt;/em>. It fires based on
the watermark, not on the passage of time as the pipeline runs.&lt;/p>
&lt;p>Next, let&amp;rsquo;s unpack what&amp;rsquo;s happening in the @ProcessElement block:&lt;/p>
&lt;p>The first element to come to this block will:&lt;/p>
&lt;ol>
&lt;li>Set the state of the &lt;code>timerRunner&lt;/code> to True.&lt;/li>
&lt;li>Write the value of the key from the key-value pair into the key StateValue.&lt;/li>
&lt;li>The code sets the value of the timer to fire one minute after the elements
timestamp. Note that the maximum value allowed for this timestamp is
XX:XX:59.999. This places the maximum alarm value at the upper boundary of
the next time interval.&lt;/li>
&lt;li>Finally, we output the data from the &lt;code>@ProcessElement&lt;/code> block using
&lt;code>c.output&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>In the @OnTimer block, the following occurs:&lt;/p>
&lt;ol>
&lt;li>The code emits a value with the key pulled from our key StateValue and a
value of 0. The timestamp of the event corresponds to the event time of the
timer firing.&lt;/li>
&lt;li>We set a new timer for one minute from now, unless we are past the
&lt;code>stopTimerTime&lt;/code> value. Your use case will normally have more complex stopping
conditions, but we use a simple condition here to allow us to keep the
illustrated code simple. The topic of stopping conditions is discussed in
more detail later.&lt;/li>
&lt;/ol>
&lt;p>And that&amp;rsquo;s it, let&amp;rsquo;s add our transform back into the pipeline:&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="c1">// Apply a fixed window of duration 1 min and Sum the results
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamped&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">time_1&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">time_2&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">time_3&lt;/span>&lt;span class="o">)).&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">Window&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span>&lt;span class="n">into&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">FixedWindows&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="c1">// We use a combiner to reduce the number of calls in keyed state
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="c1">// from all elements to 1 per FixedWindow
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Sum&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">integersPerKey&lt;/span>&lt;span class="o">())&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Window&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">into&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">GlobalWindows&lt;/span>&lt;span class="o">()))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">LoopingStatefulTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Instant&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">parse&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;2000-01-01T00:04:00Z&amp;#34;&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Window&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">into&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">FixedWindows&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardMinutes&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">))))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Sum&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">integersPerKey&lt;/span>&lt;span class="o">())&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ParDo&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">LOG&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">info&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;Value is {} timestamp is {}&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timestamp&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}));&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;ol>
&lt;li>In the first part of the pipeline we create FixedWindows and reduce the value
per key down to a single Sum.&lt;/li>
&lt;li>Next we re-window the output into a GlobalWindow. Since state and timers are
per window, they must be set within the window boundary. We want the looping
timer to span all the fixed windows, so we set it up in the global window.&lt;/li>
&lt;li>We then add our LoopingStatefulTimer DoFn.&lt;/li>
&lt;li>Finally, we reapply the FixedWindows and Sum our values.&lt;/li>
&lt;/ol>
&lt;p>This pipeline ensures that a value of zero exists for each interval window, even
if the Source of the pipeline emitted a value in the minimum and maximum
boundaries of the interval window. This means that we can mark the absence of
data.&lt;/p>
&lt;p>You might question why we use two reducers with multiple &lt;code>Sum.integersPerKey&lt;/code>.
Why not just use one? Functionally, using one would also produce the correct
result. However, putting two &lt;code>Sum.integersPerKey&lt;/code> gives us a nice performance
advantage. It reduces the number of elements from many to just one per time
interval. This can reduce the number of reads of the State API during the
&lt;code>@ProcessElement&lt;/code> calls.&lt;/p>
&lt;p>Here is the logging output of running our modified pipeline:&lt;/p>
&lt;pre>&lt;code>INFO LoopingTimer - Timer @ 2000-01-01T00:01:59.999Z fired
INFO LoopingTimer - Timer @ 2000-01-01T00:02:59.999Z fired
INFO LoopingTimer - Timer @ 2000-01-01T00:03:59.999Z fired
INFO LoopingTimer - Timer not being set as exceeded Stop Timer value 2000-01-01T00:04:00.000Z
INFO LoopingTimer - Value is KV{Key_A, 1} timestamp is 2000-01-01T00:00:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 0} timestamp is 2000-01-01T00:02:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 2} timestamp is 2000-01-01T00:01:59.999Z
INFO LoopingTimer - Value is KV{Key_A, 3} timestamp is 2000-01-01T00:03:59.999Z
&lt;/code>&lt;/pre>&lt;p>Yay! We now have output from the time interval [00:01:00, 00:01:59.999), even
though the source dataset has no elements in that interval.&lt;/p>
&lt;p>In this blog, we covered one of the fun areas around time series use cases and
worked through several options, including an advanced use case of the Timer API.
Happy looping everyone!&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Looping timers is an interesting new use case for the Timer API and
runners will need to add support for it with all of their more advanced
feature sets. You can experiment with this pattern today using the
DirectRunner. For other runners, please look out for their release notes on
support for dealing with this use case in production.&lt;/p>
&lt;p>(&lt;a href="/documentation/runners/capability-matrix/">Capability Matrix&lt;/a>)&lt;/p>
&lt;p>Runner specific notes:
Google Cloud Dataflow Runners Drain feature does not support looping timers (Link to matrix)&lt;/p></description></item><item><title>Blog: Apache Beam 2.13.0</title><link>/blog/beam-2.13.0/</link><pubDate>Fri, 07 Jun 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.13.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.13.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2130-2019-05-21">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.13.0, check out the
&lt;a href="https://jira.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12345166">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Support reading query results with the BigQuery storage API.&lt;/li>
&lt;li>Support KafkaIO to be configured externally for use with other SDKs.&lt;/li>
&lt;li>BigQuery IO now supports BYTES datatype on Python 3.&lt;/li>
&lt;li>Avro IO support enabled on Python 3.&lt;/li>
&lt;li>For Python 3 pipelines, the default Avro library used by Beam AvroIO and Dataflow workers was switched from avro-python3 to fastavro.&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Flink 1.8 support added.&lt;/li>
&lt;li>Support to run word count on Portable Spark runner.&lt;/li>
&lt;li>ElementCount metrics in FnApi Dataflow Runner.&lt;/li>
&lt;li>Support to create BinaryCombineFn from lambdas.&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>When writing BYTES Datatype into Bigquery with Beam Bigquery IO on Python DirectRunner, users need to base64-encode bytes values before passing them to Bigquery IO. Accordingly, when reading bytes data from BigQuery, the IO will also return base64-encoded bytes. This change only affects Bigquery IO on Python DirectRunner. New DirectRunner behavior is consistent with treatment of Bytes by Beam Java Bigquery IO, and Python Dataflow Runner.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.13.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron Li, Ahmet Altay, Aizhamal Nurmamat kyzy, Alex Amato, Alexey Romanenko,
Andrew Pilloud, Ankur Goenka, Anton Kedin, apstndb, Boyuan Zhang, Brian Hulette,
Brian Quinlan, Chamikara Jayalath, Cyrus Maden, Daniel Chen, Daniel Oliveira,
David Cavazos, David Moravek, David Yan, EdgarLGB, Etienne Chauchot, frederik2,
Gleb Kanterov, Harshit Dwivedi, Harsh Vardhan, Heejong Lee, Hennadiy Leontyev,
Henri-Mayeul de Benque, Ismaël Mejía, Jae-woo Kim, Jamie Kirkpatrick, Jan Lukavský,
Jason Kuster, Jean-Baptiste Onofré, JohnZZGithub, Jozef Vilcek, Juta, Kenneth Jung,
Kenneth Knowles, Kyle Weaver, Łukasz Gajowy, Luke Cwik, Mark Liu, Mathieu Blanchard,
Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Kaplinskiy,
Mike Pedersen, Mikhail Gryzykhin, Mikhail-Ivanov, Niklas Hansson, pabloem,
Pablo Estrada, Pranay Nanda, Reuven Lax, Richard Moorhead, Robbe Sneyders,
Robert Bradshaw, Robert Burke, Roman van der Krogt, rosetn, Rui Wang, Ryan Yuan,
Sam Whittle, sudhan499, Sylwester Kardziejonek, Ted, Thomas Weise, Tim Robertson,
ttanay, tvalentyn, Udi Meiri, Valentyn Tymofieiev, Xinyu Liu, Yifan Zou,
yoshiki.obata, Yueyang Qiu&lt;/p></description></item><item><title>Blog: Adding new Data Sources to Beam SQL CLI</title><link>/blog/adding-data-sources-to-sql/</link><pubDate>Tue, 04 Jun 2019 00:00:01 -0800</pubDate><guid>/blog/adding-data-sources-to-sql/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>A new, exciting feature that came to Apache Beam is the ability to use
SQL in your pipelines. This is done using Beam&amp;rsquo;s
&lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/extensions/sql/SqlTransform.html">&lt;code>SqlTransform&lt;/code>&lt;/a>
in Java pipelines.&lt;/p>
&lt;p>Beam also has a fancy new SQL command line that you can use to query your
data interactively, be it Batch or Streaming. If you haven&amp;rsquo;t tried it, check out
&lt;a href="https://bit.ly/ExploreBeamSQL">http://bit.ly/ExploreBeamSQL&lt;/a>.&lt;/p>
&lt;p>A nice feature of the SQL CLI is that you can use &lt;code>CREATE EXTERNAL TABLE&lt;/code>
commands to &lt;em>add&lt;/em> data sources to be accessed in the CLI. Currently, the CLI
supports creating tables from BigQuery, PubSub, Kafka, and text files. In this
post, we explore how to add new data sources, so that you will be able to
consume data from other Beam sources.&lt;/p>
&lt;p>The table provider we will be implementing in this post will be generating a
continuous unbounded stream of integers. It will be based on the
&lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/GenerateSequence.html">&lt;code>GenerateSequence&lt;/code> PTransform&lt;/a>
from the Beam SDK. In the end will be able to define and use the sequence generator
in SQL like this:&lt;/p>
&lt;pre>&lt;code>CREATE EXTERNAL TABLE -- all tables in Beam are external, they are not persisted
sequenceTable -- table alias that will be used in queries
(
sequence BIGINT, -- sequence number
event_timestamp TIMESTAMP -- timestamp of the generated event
)
TYPE sequence -- type identifies the table provider
TBLPROPERTIES '{ elementsPerSecond : 12 }' -- optional rate at which events are generated
&lt;/code>&lt;/pre>&lt;p>And we&amp;rsquo;ll be able to use it in queries like so:&lt;/p>
&lt;pre>&lt;code>SELECT sequence FROM sequenceTable;
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s dive in!&lt;/p>
&lt;h3 id="implementing-a-tableprovider">Implementing a &lt;code>TableProvider&lt;/code>&lt;/h3>
&lt;p>Beam&amp;rsquo;s &lt;code>SqlTransform&lt;/code> works by relying on &lt;code>TableProvider&lt;/code>s, which it uses when
one uses a &lt;code>CREATE EXTERNAL TABLE&lt;/code> statement. If you are looking to add a new
data source to the Beam SQL CLI, then you will want to add a &lt;code>TableProvider&lt;/code> to
do it. In this post, I will show what steps are necessary to create a new table
provider for the
&lt;a href="https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/GenerateSequence.html">&lt;code>GenerateSequence&lt;/code> transform&lt;/a> available in the Java SDK.&lt;/p>
&lt;p>The &lt;code>TableProvider&lt;/code> classes are under
&lt;a href="https://github.com/apache/beam/tree/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider">&lt;code>sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/meta/provider/&lt;/code>&lt;/a>. If you look in there, you can find providers, and their implementations, for all available data sources. So, you just need to add the one you want, along with an implementation of &lt;code>BaseBeamTable&lt;/code>.&lt;/p>
&lt;h3 id="the-generatesequencetableprovider">The GenerateSequenceTableProvider&lt;/h3>
&lt;p>Our table provider looks like this:&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@AutoService&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TableProvider&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">GenerateSequenceTableProvider&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">InMemoryMetaTableProvider&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">String&lt;/span> &lt;span class="nf">getTableType&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="s">&amp;#34;sequence&amp;#34;&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">BeamSqlTable&lt;/span> &lt;span class="nf">buildBeamSqlTable&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Table&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">GenerateSequenceTable&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>All it does is give a type to the table - and it implements the
&lt;code>buildBeamSqlTable&lt;/code> method, which simply returns a &lt;code>BeamSqlTable&lt;/code> defined by
our &lt;code>GenerateSequenceTable&lt;/code> implementation.&lt;/p>
&lt;h3 id="the-generatesequencetable">The GenerateSequenceTable&lt;/h3>
&lt;p>We want a table implementation that supports streaming properly, so we will
allow users to define the number of elements to be emitted per second. We will
define a simple table that emits sequential integers in a streaming fashion.
This looks like so:&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">class&lt;/span> &lt;span class="nc">GenerateSequenceTable&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">BaseBeamTable&lt;/span> &lt;span class="kd">implements&lt;/span> &lt;span class="n">Serializable&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">Schema&lt;/span> &lt;span class="n">TABLE_SCHEMA&lt;/span> &lt;span class="o">=&lt;/span>
&lt;span class="n">Schema&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Field&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;sequence&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">FieldType&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">INT64&lt;/span>&lt;span class="o">),&lt;/span> &lt;span class="n">Field&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;event_time&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">FieldType&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">DATETIME&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="n">Integer&lt;/span> &lt;span class="n">elementsPerSecond&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">5&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="n">GenerateSequenceTable&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Table&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">super&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TABLE_SCHEMA&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getProperties&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">containsKey&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;elementsPerSecond&amp;#34;&lt;/span>&lt;span class="o">))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">elementsPerSecond&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getProperties&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">getInteger&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;elementsPerSecond&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">PCollection&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">IsBounded&lt;/span> &lt;span class="nf">isBounded&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">IsBounded&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">UNBOUNDED&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">buildIOReader&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">PBegin&lt;/span> &lt;span class="n">begin&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">begin&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">GenerateSequence&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">from&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">0&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">withRate&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">elementsPerSecond&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardSeconds&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">)))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">apply&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">MapElements&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">into&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TypeDescriptor&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">of&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">via&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">elm&lt;/span> &lt;span class="o">-&amp;gt;&lt;/span> &lt;span class="n">Row&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">withSchema&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TABLE_SCHEMA&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">addValues&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">elm&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Instant&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">now&lt;/span>&lt;span class="o">()).&lt;/span>&lt;span class="na">build&lt;/span>&lt;span class="o">()))&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">setRowSchema&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">getSchema&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">POutput&lt;/span> &lt;span class="nf">buildIOWriter&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">PCollection&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Row&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">throw&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">UnsupportedOperationException&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buildIOWriter unsupported!&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="the-real-fun">The real fun&lt;/h2>
&lt;p>Now that we have implemented the two basic classes (a &lt;code>BaseBeamTable&lt;/code>, and a
&lt;code>TableProvider&lt;/code>), we can start playing with them. After building the
&lt;a href="https://beam.apache.org/documentation/dsls/sql/shell/">SQL CLI&lt;/a>, we
can now perform selections on the table:&lt;/p>
&lt;pre>&lt;code>0: BeamSQL&amp;gt; CREATE EXTERNAL TABLE input_seq (
. . . . . &amp;gt; sequence BIGINT COMMENT 'this is the primary key',
. . . . . &amp;gt; event_time TIMESTAMP COMMENT 'this is the element timestamp'
. . . . . &amp;gt; )
. . . . . &amp;gt; TYPE 'sequence';
No rows affected (0.005 seconds)
&lt;/code>&lt;/pre>&lt;p>And let&amp;rsquo;s select a few rows:&lt;/p>
&lt;pre>&lt;code>0: BeamSQL&amp;gt; SELECT * FROM input_seq LIMIT 5;
+---------------------+------------+
| sequence | event_time |
+---------------------+------------+
| 0 | 2019-05-21 00:36:33 |
| 1 | 2019-05-21 00:36:33 |
| 2 | 2019-05-21 00:36:33 |
| 3 | 2019-05-21 00:36:33 |
| 4 | 2019-05-21 00:36:33 |
+---------------------+------------+
5 rows selected (1.138 seconds)
&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s try something more interesting. Such as grouping. This will also let
us make sure that we&amp;rsquo;re providing the timestamp for each row properly:&lt;/p>
&lt;pre>&lt;code>0: BeamSQL&amp;gt; SELECT
. . . . . &amp;gt; COUNT(sequence) as elements,
. . . . . &amp;gt; TUMBLE_START(event_time, INTERVAL '2' SECOND) as window_start
. . . . . &amp;gt; FROM input_seq
. . . . . &amp;gt; GROUP BY TUMBLE(event_time, INTERVAL '2' SECOND) LIMIT 5;
+---------------------+--------------+
| elements | window_start |
+---------------------+--------------+
| 6 | 2019-06-05 00:39:24 |
| 10 | 2019-06-05 00:39:26 |
| 10 | 2019-06-05 00:39:28 |
| 10 | 2019-06-05 00:39:30 |
| 10 | 2019-06-05 00:39:32 |
+---------------------+--------------+
5 rows selected (10.142 seconds)
&lt;/code>&lt;/pre>&lt;p>And voilà! We can start playing with some interesting streaming queries to our
sequence generator.&lt;/p></description></item><item><title>Blog: Apache Beam Katas</title><link>/blog/beam-kata-release/</link><pubDate>Thu, 30 May 2019 00:00:01 -0800</pubDate><guid>/blog/beam-kata-release/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to announce
&lt;a href="https://github.com/apache/beam/tree/master/learning/katas">Apache Beam Katas&lt;/a>, a set of
interactive Beam coding exercises (i.e. &lt;a href="http://codekata.com/">code katas&lt;/a>) that can help you in
learning Apache Beam concepts and programming model hands-on.&lt;/p>
&lt;p>Beam Katas objective is to provide a series of structured hands-on learning experiences for learners
to understand about Apache Beam and its SDKs by solving exercises with gradually increasing
complexity. It is built based on
&lt;a href="https://www.jetbrains.com/education/">JetBrains Educational Products&lt;/a>. Beam Katas is available for
both Java and Python SDKs. Currently we have about 20 lessons that cover Apache Beam fundamentals,
such as core transforms, common transforms, and simple use case (word count), with more katas to
be added in the coming future.&lt;/p>
&lt;p>To start with the courses, you can simply download
&lt;a href="https://www.jetbrains.com/education/download/#section=idea">IntelliJ Edu&lt;/a> or
&lt;a href="https://www.jetbrains.com/education/download/#section=pycharm-edu">PyCharm Edu&lt;/a> and then browse
the integrated Stepik courses from the menu. Search for “Beam Katas” and once the course is loaded
on the IDE, you’re good to go.&lt;/p>
&lt;p>We have plans to add more katas covering more topics including some of the intermediate and
advanced ones in the coming future, such as windowing, streaming, and use case patterns. We would
also like to welcome you to &lt;a href="https://github.com/apache/beam">contribute&lt;/a> by building and adding more katas that you think would be
useful for people to learn more about Apache Beam, and eventually become Beam Masters!&lt;/p>
&lt;br/>
&lt;img src="/images/blog/beam-kata/beam-kata-intellij-edu-1.png" alt="Beam Kata - IntelliJ Edu" width="363" height="350">
&lt;img src="/images/blog/beam-kata/beam-kata-intellij-edu-2.png" alt="Beam Kata - IntelliJ Edu" width="455" height="350">
&lt;img src="/images/blog/beam-kata/beam-kata-pycharm-edu-1.png" alt="Beam Kata - PyCharm Edu" width="363" height="350">
&lt;img src="/images/blog/beam-kata/beam-kata-pycharm-edu-2.png" alt="Beam Kata - PyCharm Edu" width="459" height="350"></description></item><item><title>Blog: Beam community update!</title><link>/blog/beam-summit-europe-2019/</link><pubDate>Sat, 11 May 2019 00:00:01 -0800</pubDate><guid>/blog/beam-summit-europe-2019/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="the-apache-beam-community-in-2019">The Apache Beam community in 2019&lt;/h1>
&lt;p>2019 has already been a busy time for the Apache Beam community. The ASF blog featured &lt;a href="https://blogs.apache.org/comdev/date/20190222">our way of community building&lt;/a> and we&amp;rsquo;ve had &lt;a href="https://www.meetup.com/San-Francisco-Apache-Beam/events/257482350">more Beam meetups&lt;/a> around the world. Apache Beam also received the &lt;a href="https://www.infoworld.com/article/3336072/infoworlds-2019-technology-of-the-year-award-winners.html">Technology of the Year Award&lt;/a> from InfoWorld.&lt;/p>
&lt;p>As these events happened, we were building up to the &lt;a href="https://opensource.googleblog.com/2019/03/celebrating-20-years-of-apache.html">20th anniversary of the Apache Software Foundation&lt;/a>. The contributions of the Beam community were a part of Maximilian Michels blog post on the success of the ASF&amp;rsquo;s open source development model:&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Success at Apache: What You Need to Know by Maximilian Michels &lt;a href="https://t.co/XjtVYgPAHX">https://t.co/XjtVYgPAHX&lt;/a> &lt;a href="https://twitter.com/hashtag/Apache?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Apache&lt;/a> &lt;a href="https://twitter.com/hashtag/Open?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Open&lt;/a> &lt;a href="https://twitter.com/hashtag/Innovation?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Innovation&lt;/a> &lt;a href="https://twitter.com/hashtag/Community?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Community&lt;/a> &lt;a href="https://twitter.com/hashtag/people?src=hash&amp;amp;ref_src=twsrc%5Etfw">#people&lt;/a> &lt;a href="https://twitter.com/hashtag/processes?src=hash&amp;amp;ref_src=twsrc%5Etfw">#processes&lt;/a> &lt;a href="https://twitter.com/hashtag/JustWorks?src=hash&amp;amp;ref_src=twsrc%5Etfw">#JustWorks&lt;/a> &lt;a href="https://twitter.com/stadtlegende?ref_src=twsrc%5Etfw">@stadtlegende&lt;/a> &lt;a href="https://t.co/xSibnyWAMe">pic.twitter.com/xSibnyWAMe&lt;/a>&lt;/p>&amp;mdash; Apache - The ASF (@TheASF) &lt;a href="https://twitter.com/TheASF/status/1110364656143601664?ref_src=twsrc%5Etfw">26 maart 2019&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>In that spirit, let&amp;rsquo;s have an overview of the things that have happened, what the next few months look like, and how we can foster even more community growth.&lt;/p>
&lt;h2 id="meetups">Meetups&lt;/h2>
&lt;p>We&amp;rsquo;ve had a flurry of activity, with several meetups in the planning process and more popping up globally over time. As diversity of contributors is a core ASF value, this geographic spread is exciting for the community. Here&amp;rsquo;s a picture from the latest Apache Beam meetup organized at Lyft in San Francisco:&lt;/p>
&lt;p>&lt;img src="https://secure.meetupstatic.com/photos/event/8/0/1/2/600_481292786.jpeg" alt="Beam Meetup Bay Area" >&lt;/p>
&lt;p>We have more &lt;a href="https://www.meetup.com/San-Francisco-Apache-Beam">Bay Area meetups&lt;/a> coming soon, and the community is looking into kicking off a meetup in Toronto!&lt;/p>
&lt;p>&lt;a href="https://www.meetup.com/London-Apache-Beam-Meetup">London&lt;/a> had its first meetup of 2019 at the start of April:&lt;/p>
&lt;p>&lt;img src="https://secure.meetupstatic.com/photos/event/4/7/0/e/600_480318190.jpeg" alt="Beam Meetup London" height="360" width="640" >&lt;/p>
&lt;p>and &lt;a href="https://www.meetup.com/Apache-Beam-Stockholm/events/260634514">Stockholm&lt;/a> had its second meetup at the start of May:&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="en-gb">&lt;p lang="en" dir="ltr">Big audience for the second &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> meetup in Stockholm! Gleb, &lt;a href="https://twitter.com/kanterov?ref_src=twsrc%5Etfw">@kanterov&lt;/a> from &lt;a href="https://twitter.com/SpotifyEng?ref_src=twsrc%5Etfw">@SpotifyEng&lt;/a> kicking off the first talk with Beam SQL.&lt;a href="https://twitter.com/hashtag/ApacheBeamStockholm?src=hash&amp;amp;ref_src=twsrc%5Etfw">#ApacheBeamStockholm&lt;/a> &lt;a href="https://t.co/fDqPPFh2gY">pic.twitter.com/fDqPPFh2gY&lt;/a>&lt;/p>&amp;mdash; Matthias Baetens 🌆 (@matthiasbaetens) &lt;a href="https://twitter.com/matthiasbaetens/status/1125442916711915521?ref_src=twsrc%5Etfw">6 May 2019&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>Keep an eye out for a meetup in &lt;a href="https://www.meetup.com/Paris-Apache-Beam-Meetup">Paris&lt;/a>.&lt;/p>
&lt;p>If you are interested in starting your own meetup, feel free &lt;a href="https://beam.apache.org/community/contact-us">to reach out&lt;/a>! Good places to start include our Slack channel, the dev and user mailing lists, or the Apache Beam Twitter.&lt;/p>
&lt;p>Even if you can’t travel to these meetups, you can stay informed on the happenings of the community. The talks and sessions from previous conferences and meetups are archived on the &lt;a href="https://www.youtube.com/c/ApacheBeamYT">Apache Beam YouTube channel&lt;/a>. If you want your session added to the channel, don’t hesitate to get in touch! And in case you want to attend the next Beam event in style, you can also order your swag on the &lt;a href="https://store-beam.myshopify.com">Beam swag store&lt;/a>&lt;/p>
&lt;h2 id="summits">Summits&lt;/h2>
&lt;p>The first summit of the year will be held in Berlin:&lt;/p>
&lt;p>&lt;img src="https://img.evbuc.com/https%3A%2F%2Fcdn.evbuc.com%2Fimages%2F58635346%2F70962106775%2F1%2Foriginal.20190317-212619?w=800&amp;auto=compress&amp;rect=0%2C115%2C2666%2C1333&amp;s=2680f5036dcad9177b027cce026c0224" alt="Beam Summit Europe Banner" >&lt;/p>
&lt;p>You can find more info on the &lt;a href="https://beamsummit.org">website&lt;/a> and read about the inaugural edition of the Beam Summit Europe &lt;a href="https://beam.apache.org/blog/2018/10/31/beam-summit-aftermath.html">here&lt;/a>. At these summits, you have the opportunity to meet with other Apache Beam creators and users, get expert advice, learn from the speaker sessions, and participate in workshops.&lt;/p>
&lt;p>We strongly encourage you to get involved again this year! You can participate in the following ways for the upcoming summit in Europe:&lt;/p>
&lt;p>🎫 If you want to secure your ticket to attend the Beam Summit Europe 2019, check our &lt;a href="https://beam-summit-europe.eventbrite.com">event page&lt;/a>.&lt;/p>
&lt;p>💸 If you want to make the Summit even &lt;strong>more&lt;/strong> awesome, check out our &lt;a href="https://drive.google.com/file/d/1R3vvOHihQbpuzF2aaSV8WYg9YHRmJwxS/view">sponsor booklet&lt;/a>!&lt;/p>
&lt;p>We also launched the CfP for our Beam Summit in North America, which will be held in collaboration with &lt;a href="https://www.apachecon.com">ApacheCon&lt;/a>.&lt;/p>
&lt;p>🎤 If you want to give a talk, take a look at our &lt;a href="https://www.apachecon.com/acna19/cfp.html">CfP&lt;/a>.&lt;/p>
&lt;p>Stay tuned for more information on the summit in North America and Asia.&lt;/p>
&lt;h2 id="why-community-engagement-matters">Why community engagement matters&lt;/h2>
&lt;p>Why we need a strong Apache Beam community:&lt;/p>
&lt;ul>
&lt;li>We&amp;rsquo;re receiving lots of code contributions and need committers to review those and help onboard new contributors to the project.&lt;/li>
&lt;li>We want people to feel a sense of ownership to the project. By fostering this level of engagement, the work becomes even more exciting.&lt;/li>
&lt;li>A healthy community has a further reach and leads to more growth. More hours can be contributed to the project as we can spread the work and ownership.&lt;/li>
&lt;/ul>
&lt;p>Why are we organizing these summits:&lt;/p>
&lt;ul>
&lt;li>We&amp;rsquo;d like to give folks a place to meet, congregate, and share ideas.&lt;/li>
&lt;li>We know that offline interactions often changes the nature of the online ones in a positive manner.&lt;/li>
&lt;li>Building an active and diverse community is part of the Apache Way. These summits provide an opportunity for us to engage people from different locations, companies, and backgrounds.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Apache Beam + Kotlin = ❤️</title><link>/blog/beam-kotlin/</link><pubDate>Thu, 25 Apr 2019 00:00:01 -0800</pubDate><guid>/blog/beam-kotlin/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Apache Beam samples are now available in Kotlin!&lt;/p>
&lt;p>&lt;img src="/images/blog/kotlin.png" alt="Kotlin" height="320" width="800" >&lt;/p>
&lt;p>If you are someone who&amp;rsquo;s been working with Java in your professional career; there&amp;rsquo;s a good chance that you&amp;rsquo;ve also heard of &lt;a href="https://kotlinlang.org/">Kotlin&lt;/a>, which is an Open Sourced, statically typed language for JVM and is mostly being favoured by Android Developers due to the many myriad features which enable more concise and cleaner code than Java without sacrificing performance or safety.&lt;/p>
&lt;p>It gives us an immense pleasure to announce that we are also taking a step ahead in the same direction and releasing the samples for the Beam SDK in Kotlin alongside Java!&lt;/p>
&lt;p>(Note : At the time of writing this post, only the WordCount samples have been added in Koltin with more samples underway)&lt;/p>
&lt;h2 id="code-snippets">Code Snippets&lt;/h2>
&lt;p>Here are few brief snippets of code that show how the Kotlin Samples compare to Java&lt;/p>
&lt;h3 id="java">Java&lt;/h3>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="n">String&lt;/span> &lt;span class="n">filename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">String&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">format&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="s">&amp;#34;%s-%s-of-%s%s&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">filenamePrefixForWindow&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">intervalWindow&lt;/span>&lt;span class="o">),&lt;/span>
&lt;span class="n">shardNumber&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">numShards&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">outputFileHints&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">suggestedFilenameSuffix&lt;/span>&lt;span class="o">);&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="kotlin">Kotlin&lt;/h3>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="c1">// String templating
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="n">val&lt;/span> &lt;span class="n">filename&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;$filenamePrefixForWindow(intervalWindow)-$shardNumber-of-$numShards${outputFileHints.suggestedFilenameSuffix)&amp;#34;&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="java-1">Java&lt;/h3>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">FormatAsTextFn&lt;/span> &lt;span class="kd">extends&lt;/span> &lt;span class="n">SimpleFunction&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Override&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">String&lt;/span> &lt;span class="nf">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getKey&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s">&amp;#34;: &amp;#34;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">input&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">getValue&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="kotlin-1">Kotlin&lt;/h2>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">FormatAsTextFn&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">SimpleFunction&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;,&lt;/span> &lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">override&lt;/span> &lt;span class="n">fun&lt;/span> &lt;span class="nf">apply&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">input&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="n">KV&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">Long&lt;/span>&lt;span class="o">&amp;gt;)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;${input.key} : ${input.value}&amp;#34;&lt;/span> &lt;span class="c1">//Single line functions
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="java-2">Java&lt;/h3>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">if&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">tableRow&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="kc">null&lt;/span>&lt;span class="o">){&lt;/span>
&lt;span class="n">formatAndInsert&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">tableRow&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="kotlin-2">Kotlin&lt;/h3>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">tableRow&lt;/span>&lt;span class="o">?.&lt;/span>&lt;span class="na">let&lt;/span>&lt;span class="o">{&lt;/span>
&lt;span class="n">formatAndInsert&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">it&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="c1">// No need for null checks
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="java-3">Java&lt;/h3>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">String&lt;/span> &lt;span class="n">tableName&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;testTable&amp;#34;&lt;/span>&lt;span class="o">;&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h3 id="kotlin-3">Kotlin&lt;/h3>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="n">val&lt;/span> &lt;span class="n">tableName&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;testTable&amp;#34;&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">Type&lt;/span> &lt;span class="n">inferencing&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;h2 id="contributors-welcomed">Contributors Welcomed!&lt;/h2>
&lt;p>While we&amp;rsquo;re still adding more samples and streamlining the current ones, we would love to have your feedback on the code snippets.
You can find them over here : &lt;a href="https://github.com/apache/beam/tree/master/examples/kotlin">https://github.com/apache/beam/tree/master/examples/kotlin&lt;/a>&lt;/p>
&lt;p>If you are using Kotlin with Apache Beam already; we would very much appreciate if you went ahead and help us convert the existing samples from Java into Koltin.&lt;/p>
&lt;p>Thank you, and we are looking forward to feedback from you!&lt;/p></description></item><item><title>Blog: Apache Beam 2.12.0</title><link>/blog/beam-2.12.0/</link><pubDate>Thu, 25 Apr 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.12.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.12.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2120-2019-04-25">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.12.0, check out the
&lt;a href="https://jira.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12344944">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Add support for a BigQuery custom sink for Python SDK.&lt;/li>
&lt;li>Add support to specify a query in CassandraIO for Java SDK.&lt;/li>
&lt;li>Add experimental support for cross-language transforms,
please see &lt;a href="https://issues.apache.org/jira/browse/BEAM-6730">BEAM-6730&lt;/a>&lt;/li>
&lt;li>Add support in the Flink Runner for exactly-once Writes with KafkaIO&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Enable Bundle Finalization in Python SDK for portable runners.&lt;/li>
&lt;li>Add support to the Java SDK harness to merge windows.&lt;/li>
&lt;li>Add Kafka Sink EOS support on Flink runner.&lt;/li>
&lt;li>Added a dead letter queue to Python streaming BigQuery sink.&lt;/li>
&lt;li>Add Experimental Python 3.6 and 3.7 workloads enabled.
Beam 2.12 supports starting Dataflow pipelines under Python 3.6, 3.7, however 3.5 remains the only recommended minor version for Dataflow runner. In addition to announced 2.11 limitations, Beam typehint annotations are currently not supported on Python &amp;gt;= 3.6.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.12.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed El.Hussaini, Ahmet Altay, Alan Myrvold, Alex Amato, Alexander Savchenko,
Alexey Romanenko, Andrew Brampton, Andrew Pilloud, Ankit Jhalaria,
Ankur Goenka, Anton Kedin, Boyuan Zhang, Brian Hulette, Chamikara Jayalath,
Charles Chen, Colm O hEigeartaigh, Craig Chambers, Dan Duong, Daniel Mescheder,
Daniel Oliveira, David Moravek, David Rieber, David Yan, Eric Roshan-Eisner,
Etienne Chauchot, Gleb Kanterov, Heejong Lee, Ho Tien Vu, Ismaël Mejía,
Jan Lukavský, Jean-Baptiste Onofré, Jeff Klukas, Juta, Kasia Kucharczyk,
Kengo Seki, Kenneth Jung, Kenneth Knowles, kevin, Kyle Weaver, Kyle Winkelman,
Łukasz Gajowy, Mark Liu, Mathieu Blanchard, Max Charas, Maximilian Michels,
Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Kaplinskiy,
Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Radoslaw Stankiewicz,
Reuven Lax, Robbe Sneyders, Robert Bradshaw, Robert Burke, Rui Wang,
Ruoyun Huang, Ryan Williams, Slava Chernyak, Shahar Frank, Sunil Pedapudi,
Thomas Weise, Tim Robertson, Tanay Tummalapalli, Udi Meiri,
Valentyn Tymofieiev, Xinyu Liu, Yifan Zou, Yueyang Qiu&lt;/p></description></item><item><title>Blog: Apache Beam is applying to Season of Docs</title><link>/blog/season-of-docs/</link><pubDate>Fri, 19 Apr 2019 00:00:01 -0800</pubDate><guid>/blog/season-of-docs/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The Apache Beam community is thrilled to announce its application to the first edition of Season of Docs 2019!&lt;/p>
&lt;p>&lt;img src="/images/blog/SoD.png" alt="Season of Docs 2019 flyer" height="455" width="640" >&lt;/p>
&lt;p>&lt;a href="https://developers.google.com/season-of-docs/">Season of Docs&lt;/a> is a unique program that pairs technical writers with open source mentors to contribute to open source. This creates an opportunity to introduce the technical writer to an open source community and provide guidance while the writer works on a real world open source project. We, in the Apache Beam community, would love to take this chance and invite technical writers to collaborate with us, and help us improve our documentation in many ways.&lt;/p>
&lt;p>Apache Beam does have help from excellent technical writers, but the documentation needs of the project often exceed their bandwidth. This is why we are excited about this program.&lt;/p>
&lt;p>After discussing ideas in the community, we have been able to find mentors, and frame two ideas that we think would be a great fit for an incoming tech writer to tackle. We hope you will find this opportunity interesting - and if you do, please get in touch by emailing the Apache Beam mailing list at &lt;a href="mailto:dev@beam.apache.org">dev@beam.apache.org&lt;/a> (you will need to subscribe first by emailing to &lt;a href="mailto:dev-subscribe@beam.apache.org">dev-subscribe@beam.apache.org&lt;/a>).&lt;/p>
&lt;p>The project ideas available in Apache Beam are described below. Please take a look and ask any questions that you may have. We will be very happy to help you get onboarded with the project.&lt;/p>
&lt;h3 id="project-ideas">Project ideas&lt;/h3>
&lt;p>&lt;strong>Deployment of Flink and Spark Clusters for use with Portable Beam&lt;/strong>&lt;/p>
&lt;p>The Apache Beam vision has been to provide a framework for users to write and execute pipelines on the programming language of your choice, and the runner of your choice. As the reality of Beam has evolved towards this vision, the way in which Beam is run on top of runners such as Apache Spark and Apache Flink has changed.&lt;/p>
&lt;p>These changes are documented in the wiki and in design documents, and are accessible for Beam contributors; but they are not available in the user-facing documentation. This has been a barrier of adoption for other users of Beam.&lt;/p>
&lt;p>This project involves improving the &lt;a href="https://beam.apache.org/documentation/runners/flink/">Flink Runner page&lt;/a> to include strategies to deploy Beam on a few different environments: A Kubernetes cluster, a Google Cloud Dataproc cluster, and an AWS EMR cluster. There are other places in the documentation that should be updated in this regard, such as the &lt;a href="https://beam.apache.org/documentation/sdks/python-streaming/">Python streaming&lt;/a> section, and the &lt;a href="https://beam.apache.org/documentation/sdks/python-streaming/#unsupported-features">set of supported features&lt;/a>.&lt;/p>
&lt;p>After working on the Flink Runner, then similar updates should be made to the &lt;a href="https://beam.apache.org/documentation/runners/spark/">Spark Runner page&lt;/a>, and the &lt;a href="https://beam.apache.org/get-started/beam-overview/">getting started documentation&lt;/a>.&lt;/p>
&lt;p>&lt;strong>The runner comparison page / capability matrix update&lt;/strong>&lt;/p>
&lt;p>Beam maintains a &lt;a href="https://beam.apache.org/documentation/runners/capability-matrix/">capability matrix&lt;/a> to track which Beam features are supported by which set of language SDKs + Runners.
This project involves a number of &lt;a href="https://issues.apache.org/jira/browse/BEAM-2888">corrections and improvements to the capability matrix&lt;/a>; followed by a few larger set of changes, involving:&lt;/p>
&lt;ul>
&lt;li>Plain english summaries for each runner’s support of the Beam model.&lt;/li>
&lt;li>A paragraph-length description of the production-readiness for each runner.&lt;/li>
&lt;li>Comparisons for non-model differences between runners.&lt;/li>
&lt;li>Comparison for support of the portability framework for each runner.&lt;/li>
&lt;/ul>
&lt;p>Thank you, and we are looking forward to hearing from you!&lt;/p></description></item><item><title>Blog: Announcing Beam Summit Site</title><link>/blog/beam-summit-site/</link><pubDate>Mon, 18 Mar 2019 00:00:01 -0800</pubDate><guid>/blog/beam-summit-site/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are thrilled to announce the launch of our new website dedicated to Beam Summits!&lt;/p>
&lt;p>The &lt;a href="https://beamsummit.org">beamsummit.org&lt;/a> site provides all the information you need towards the upcoming Beam Summits in Europe, Asia and North America in 2019. You will find information about the conference theme, the speakers and sessions, the abstract submission timeline and the registration process, the conference venues and hosting cities - and much more that you will find useful until and during the Beam Summits 2019.&lt;/p>
&lt;p>We are working to make the website easy to use, so that anyone who is organizing a Beam event can rely on it. You can find the &lt;a href="https://github.com/matthiasa4/hoverboard">code for it in Github&lt;/a>.&lt;/p>
&lt;p>The pages will be updated on a regular basis, but we also love hearing thoughts from our community! Let us know if you have any questions, comments or suggestions, and help us improve! Also, if you are thinking of organizing a Beam event, please feel free to reach out for support, and to use the code in GitHub as well.&lt;/p>
&lt;p>We sincerely hope that you like the new Beam Summit website and will find it useful for accessing information. Enjoy browsing around!&lt;/p>
&lt;p>See you in Berlin!&lt;/p>
&lt;p>#beamsummit2019.&lt;/p></description></item><item><title>Blog: Apache Beam 2.11.0</title><link>/blog/beam-2.11.0/</link><pubDate>Tue, 05 Mar 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.11.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.11.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2110-2019-02-26">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.11.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12344775">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="dependency-upgradeschanges">Dependency Upgrades/Changes&lt;/h3>
&lt;ul>
&lt;li>Java: antlr: 4.7&lt;/li>
&lt;li>Java: antlr_runtime: 4.7&lt;/li>
&lt;li>Java: bigdataoss_gcsio: 1.9.16&lt;/li>
&lt;li>Java: bigdataoss_util: 1.9.16&lt;/li>
&lt;li>Java: bigtable_client_core: 1.8.0&lt;/li>
&lt;li>Java: cassandra-driver-core: 3.6.0&lt;/li>
&lt;li>Java: cassandra-driver-mapping: 3.6.0&lt;/li>
&lt;li>Java: commons-compress: 1.18&lt;/li>
&lt;li>Java: gax_grpc: 1.38.0&lt;/li>
&lt;li>Java: google_api_common: 1.7.0&lt;/li>
&lt;li>Java: google_api_services_dataflow: v1b3-rev20190126-1.27.0&lt;/li>
&lt;li>Java: google_cloud_bigquery_storage: 0.79.0-alpha&lt;/li>
&lt;li>Java: google_cloud_bigquery_storage_proto: 0.44.0&lt;/li>
&lt;li>Java: google_auth_library_credentials: 0.12.0&lt;/li>
&lt;li>Java: google_auth_library_oauth2_http: 0.12.0&lt;/li>
&lt;li>Java: google_cloud_core: 1.61.0&lt;/li>
&lt;li>Java: google_cloud_core_grpc: 1.61.0&lt;/li>
&lt;li>Java: google_cloud_spanner: 1.6.0&lt;/li>
&lt;li>Java: grpc_all: 1.17.1&lt;/li>
&lt;li>Java: grpc_auth: 1.17.1&lt;/li>
&lt;li>Java: grpc_core: 1.17.1&lt;/li>
&lt;li>Java: grpc_google_cloud_pubsub_v1: 1.17.1&lt;/li>
&lt;li>Java: grpc_protobuf: 1.17.1&lt;/li>
&lt;li>Java: grpc_protobuf_lite: 1.17.1&lt;/li>
&lt;li>Java: grpc_netty: 1.17.1&lt;/li>
&lt;li>Java: grpc_stub: 1.17.1&lt;/li>
&lt;li>Java: netty_handler: 4.1.30.Final&lt;/li>
&lt;li>Java: netty_tcnative_boringssl_static: 2.0.17.Final&lt;/li>
&lt;li>Java: netty_transport_native_epoll: 4.1.30.Final&lt;/li>
&lt;li>Java: proto_google_cloud_spanner_admin_database_v1: 1.6.0&lt;/li>
&lt;li>Java: zstd_jni: 1.3.8-3&lt;/li>
&lt;li>Python: futures&amp;gt;=3.2.0,&amp;lt;4.0.0; python_version &amp;lt; &amp;ldquo;3.0&amp;rdquo;&lt;/li>
&lt;li>Python: pyvcf&amp;gt;=0.6.8,&amp;lt;0.7.0; python_version &amp;lt; &amp;ldquo;3.0&amp;rdquo;&lt;/li>
&lt;li>Python: google-apitools&amp;gt;=0.5.26,&amp;lt;0.5.27&lt;/li>
&lt;li>Python: google-cloud-core==0.28.1&lt;/li>
&lt;li>Python: google-cloud-bigtable==0.31.1&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Portable Flink runner support for running cross-language transforms.&lt;/li>
&lt;li>Add Cloud KMS support to GCS copies.&lt;/li>
&lt;li>Add parameters for offsetConsumer in KafkaIO.read().&lt;/li>
&lt;li>Allow setting compression codec in ParquetIO write.&lt;/li>
&lt;li>Add kms_key to BigQuery transforms, pass to Dataflow.&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>Python 3 (experimental) suppport for DirectRunner and DataflowRunner.&lt;/li>
&lt;li>Add ZStandard compression support for Java SDK.&lt;/li>
&lt;li>Python: Add CombineFn.compact, similar to Java.&lt;/li>
&lt;li>SparkRunner: GroupByKey optimized for non-merging windows.&lt;/li>
&lt;li>SparkRunner: Add bundleSize parameter to control splitting of Spark sources.&lt;/li>
&lt;li>FlinkRunner: Portable runner savepoint / upgrade support.&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Deprecate MongoDb &lt;code>withKeepAlive&lt;/code> because it is deprecated in the Mongo driver.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.11.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato. Alexey Romanenko, Andrew Pilloud, Ankur Goenka, Anton Kedin,
Boyuan Zhang, Brian Hulette, Brian Martin, Chamikara Jayalath, Charles Chen, Craig Chambers,
Daniel Oliveira, David Moravek, David Rieber, Dustin Rhodes, Etienne Chauchot, Gleb Kanterov,
Hai Lu, Heejong Lee, Ismaël Mejía, J Ross Thomson, Jan Lukavsky, Jason Kuster, Jean-Baptiste Onofré,
Jeff Klukas, João Cabrita, Juan Rael, Juta Staes, Kasia Kucharczyk, Kengo Seki, Kenneth Jung,
Kenneth Knowles, Kyle Weaver, Kyle Winkelman, Lukas Drbal, Marek Simunek, Mark Liu,
Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Pedersen,
Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Pascal Gula, Reuven Lax, Robbe Sneyders,
Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Williams, Sam Rohde, Sam Whittle,
Scott Wegner, Tanay Tummalapalli, Thomas Weise, Tianyang Hu, Tyler Akidau, Udi Meiri,
Valentyn Tymofieiev, Xinyu Liu, Xu Mingmin, Łukasz Gajowy.&lt;/p></description></item><item><title>Blog: Apache Beam 2.10.0</title><link>/blog/beam-2.10.0/</link><pubDate>Fri, 15 Feb 2019 00:00:01 -0800</pubDate><guid>/blog/beam-2.10.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.10.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2100-2019-02-01">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.10.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12344540">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;h3 id="dependency-upgradeschanges">Dependency Upgrades/Changes&lt;/h3>
&lt;ul>
&lt;li>FlinkRunner: Flink 1.5.x/1.6.x/1.7.x&lt;/li>
&lt;li>Java: AutoValue 1.6.3&lt;/li>
&lt;li>Java: Jackson 2.9.8&lt;/li>
&lt;li>Java: google_cloud_bigdataoss 1.9.13&lt;/li>
&lt;li>Java: Apache Commons Codec: 1.10&lt;/li>
&lt;li>Python: avro&amp;gt;=1.8.1,&amp;lt;2.0.0; python_version &amp;lt; &amp;ldquo;3.0&amp;rdquo;&lt;/li>
&lt;li>Python: avro-python3&amp;gt;=1.8.1,&amp;lt;2.0.0; python_version &amp;gt;= &amp;ldquo;3.0&amp;rdquo;&lt;/li>
&lt;li>Python: bigdataoss_gcsio 1.9.12&lt;/li>
&lt;li>Python: dill&amp;gt;=0.2.9,&amp;lt;0.2.10&lt;/li>
&lt;li>Python: gcsio 1.9.13&lt;/li>
&lt;li>Python: google-cloud-pubsub 0.39.0&lt;/li>
&lt;li>Python: pytz&amp;gt;=2018.3&lt;/li>
&lt;li>Python: pyyaml&amp;gt;=3.12,&amp;lt;4.0.0&lt;/li>
&lt;li>MongoDbIO: mongo client 3.9.1&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>ParquetIO for Python SDK&lt;/li>
&lt;li>HadoopOutputFormatIO: Add batching support&lt;/li>
&lt;li>HadoopOutputFormatIO: Add streaming support&lt;/li>
&lt;li>MongoDbIO: Add projections&lt;/li>
&lt;li>MongoDbIO: Add support for server with self signed SSL&lt;/li>
&lt;li>MongoDbIO add ordered option (inserts documents even if errors)&lt;/li>
&lt;li>KafkaIO: Add support to write to multiple topics&lt;/li>
&lt;li>KafkaIO: add writing support with ProducerRecord&lt;/li>
&lt;li>CassandraIO: Add ability to delete data&lt;/li>
&lt;li>JdbcIO: Add ValueProvider support for Statement in JdbcIO.write(), so it can be templatized&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>FlinkRunner: support Flink config directory&lt;/li>
&lt;li>FlinkRunner: master url now supports IPv6 addresses&lt;/li>
&lt;li>FlinkRunner: portable runner savepoint / upgrade support&lt;/li>
&lt;li>FlinkRunner: can be built against different Flink versions&lt;/li>
&lt;li>FlinkRunner: Send metrics to Flink in portable runner&lt;/li>
&lt;li>Java: Migrate to vendored gRPC (no conflicts with user gRPC, smaller jars)&lt;/li>
&lt;li>Java: Migrate to vendored Guava (no conflicts with user Guava, smaller jars)&lt;/li>
&lt;li>SQL: support joining unbounded to bounded sources via side input (and is no longer sensitive to left vs right join)&lt;/li>
&lt;li>SQL: support table macro&lt;/li>
&lt;li>Schemas: support for Avro, with automatic schema registration&lt;/li>
&lt;li>Schemas: Automatic schema registration for AutoValue classes&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>Watch PTransform fixed (affects FileIO)&lt;/li>
&lt;li>FlinkRunner: no longer fails if GroupByKey contains null values (streaming mode only)&lt;/li>
&lt;li>FlinkRunner: no longer prepares to-be-staged file too late&lt;/li>
&lt;li>FlinkRunner: sets number of shards for writes with runner determined sharding&lt;/li>
&lt;li>FlinkRunner: prevents CheckpointMarks from not getting acknowledged&lt;/li>
&lt;li>Schemas: Generated row object for POJOs, Avros, and JavaBeans should work if the wrapped class is package private&lt;/li>
&lt;li>Schemas: Nested collection types in schemas no longer cause NullPointerException when converting to a POJO&lt;/li>
&lt;li>BigQueryIO: now handles quotaExceeded errors properly&lt;/li>
&lt;li>BigQueryIO: now handles triggering correctly in certain very large load jobs&lt;/li>
&lt;li>FileIO and other file-based IOs: Beam LocalFilesystem now matches glob patterns in windows&lt;/li>
&lt;li>SQL: joins no longer moves timestamps to the end of the window&lt;/li>
&lt;li>SQL: was missing some transitive dependencies&lt;/li>
&lt;li>SQL: JDBC driver no longer breaks interactions with other JDBC sources&lt;/li>
&lt;li>pyarrow supported on Windows Python 2&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations">Deprecations&lt;/h3>
&lt;ul>
&lt;li>Deprecate HadoopInputFormatIO&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.10.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko, Anton Kedin, Rui Wang,
Andrew Brampton Andrew Pilloud, Ankur Goenka, Antonio D&amp;rsquo;souza, Bingfeng Shu,
Boyuan Zhang, brucearctor, Cade Markegard, Chaim Turkel, Chamikara Jayalath,
Charles Chen, Colm O hEigeartaigh, Cory, Craig Chambers, Cristian, Daniel
Mills, Daniel Oliveira, David Cavazos, David Hrbacek, David Moravek, Dawid
Wysakowicz, djhworld, Dustin Rhodes, Etienne Chauchot, Fabien Rousseau, Garrett
Jones, Gleb Kanterov, Heejong Lee, Ismaël Mejía, Jason Kuster, Jean-Baptiste
Onofré, Jeff Klukas, Joar Wandborg, Jozef Vilcek, Kadir Cetinkaya, Kasia
Kucharczyk, Kengo Seki, Kenneth Knowles, lcaggio, Lukasz Cwik, Łukasz Gajowy,
Manu Zhang, marek.simunek, Mark Daoust, Mark Liu, Maximilian Michels, Melissa
Pashniak, Michael Luckey, Mikhail Gryzykhin, mlotstein, morokosi, Niel
Markwick, Pablo Estrada, Prem Kumar Karunakaran, Reuven Lax, robbe, Robbe
Sneyders, Robert Bradshaw, Robert Burke, Ruoyun Huang, Ryan Williams, Sam
Whittle, Scott Wegner, Slava Chernyak, Theodore Siu, Thomas Weise, Udi Meiri,
&lt;a href="mailto:vaclav.plajt@gmail.com">vaclav.plajt@gmail.com&lt;/a>, Valentyn Tymofieiev, Won Wook SONG, Wout Scheepers,
Xinyu Liu, Yueyang Qiu, Zhuo Peng&lt;/p></description></item><item><title>Blog: Apache Beam 2.9.0</title><link>/blog/beam-2.9.0/</link><pubDate>Thu, 13 Dec 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.9.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.9.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#290-2018-12-13">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.9.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12344258">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;h3 id="dependency-upgrades">Dependency Upgrades&lt;/h3>
&lt;ul>
&lt;li>Update google-api-client libraries to 1.27.0.&lt;/li>
&lt;li>Update byte-buddy to 1.9.3&lt;/li>
&lt;li>Update Flink Runner to 1.5.5&lt;/li>
&lt;li>Upgrade google-apitools to 0.5.24&lt;/li>
&lt;/ul>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;ul>
&lt;li>Added support for user state and timers to Flink runner.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>I/O connector for RabbitMQ.&lt;/li>
&lt;li>Update SpannerIO to support unbounded writes.&lt;/li>
&lt;li>Add PFADD method to RedisIO.&lt;/li>
&lt;/ul>
&lt;h3 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h3>
&lt;ul>
&lt;li>Dataflow runner was updated to &lt;strong>not&lt;/strong> use &lt;a href="https://github.com/google/conscrypt">Conscrypt&lt;/a> as the default security provider.&lt;/li>
&lt;li>Support set/delete of timers by ID in Flink runner.&lt;/li>
&lt;li>Improvements to stabilize integration tests.&lt;/li>
&lt;li>Updates Spark runner to show Beam metrics in web UI&lt;/li>
&lt;li>Vendor gRPC and Protobuf separately from beam-model-* Java packages&lt;/li>
&lt;li>Avoid reshuffle for zero and one element creates&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.9.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Adam Horky, Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko, Andrea Foegler, Andrew Fulton, Andrew Pilloud, Ankur Goenka, Anton Kedin, Babu, Ben Song, Bingfeng Shu, Boyuan Zhang, Brian Martin, Brian Quinlan, Chamikara Jayalath, Charles Chen, Christian Schneider, Colm O hEigeartaigh, Cory Brzycki, CraigChambersG, Daniel Oliveira, David Moravek, Dusan Rychnovsky, Etienne Chauchot, Eugene Kirpichov, Fabien Rousseau, Gleb Kanterov, Heejong Lee, Henning Rohde, Ismaël Mejía, Jan Lukavský, Jaromir Vanek, Jason Kuster, Jean-Baptiste Onofré, Jeff Klukas, Jeroen Steggink, Julien Tournay, Jára Vaněk, Katarzyna Kucharczyk, Keisuke Kondo, Kenneth Knowles, Liam Miller-Cushon, Luke Cwik, Manu Zhang, Mark Liu, Maximilian Michels, Melissa Pashniak, Micah Wylde, Michael Luckey, Mike Pedersen, Mikhail Gryzykhin, Novotnik, Petr, Ondrej Kvasnicka, Pablo Estrada, Pavel Slechta, Raghu Angadi, Reuven Lax, Robbe Sneyders, Robert Bradshaw, Robert Burke, Ruoyu Liu, Ruoyun Huang, Sam Rohde, Sam sam, Scott Wegner, Simon Plovyt, Thomas Weise, Tim Robertson, Tomas Novak, Udi Meiri, Vaclav Plajt, Valentyn Tymofieiev, Varun Dhussa, Vojtech Janota, Wout Scheepers, Xinyu Liu, XuMingmin, Yifan Zou, Yueyang Qiu, akedin, amaliujia, connelloG, flyisland, huygaa11, jasonkuster, jglezt, kkpoon, mareksimunek, matthiasa4, melissa, mingmxu, nielm, reuvenlax, robbe, ruoyu90, splovyt, svXaverius, &lt;a href="mailto:vaclav.plajt@gmail.com">vaclav.plajt@gmail.com&lt;/a>, xinyuiscool, xitep, Łukasz Gajowy&lt;/p></description></item><item><title>Blog: Inaugural edition of the Beam Summit Europe 2018 - aftermath</title><link>/blog/beam-summit-aftermath/</link><pubDate>Wed, 31 Oct 2018 00:00:01 -0800</pubDate><guid>/blog/beam-summit-aftermath/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Almost 1 month ago, we had the pleasure to welcome the Beam community at Level39 in London for the inaugural edition of the Beam Summit London Summit.&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="en">&lt;p lang="en" dir="ltr">Day 1 of the first Beam Summit London going full speed ahead! Sessions by &lt;a href="https://twitter.com/SkyUK?ref_src=twsrc%5Etfw">@SkyUK&lt;/a> &lt;a href="https://twitter.com/GCPcloud?ref_src=twsrc%5Etfw">@GCPcloud&lt;/a> &lt;a href="https://twitter.com/Talend?ref_src=twsrc%5Etfw">@Talend&lt;/a> &lt;a href="https://twitter.com/PlantixApp?ref_src=twsrc%5Etfw">@PlantixApp&lt;/a> and more! &lt;a href="https://twitter.com/hashtag/ApacheBeam?src=hash&amp;amp;ref_src=twsrc%5Etfw">#ApacheBeam&lt;/a> &lt;a href="https://twitter.com/hashtag/apachebeamlondon?src=hash&amp;amp;ref_src=twsrc%5Etfw">#apachebeamlondon&lt;/a> &lt;a href="https://twitter.com/hashtag/l39?src=hash&amp;amp;ref_src=twsrc%5Etfw">#l39&lt;/a> &lt;a href="https://twitter.com/hashtag/level39?src=hash&amp;amp;ref_src=twsrc%5Etfw">#level39&lt;/a> &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> &lt;a href="https://twitter.com/hashtag/summit?src=hash&amp;amp;ref_src=twsrc%5Etfw">#summit&lt;/a> &lt;a href="https://t.co/aEESFnbgxT">pic.twitter.com/aEESFnbgxT&lt;/a>&lt;/p>&amp;mdash; Matthias Baetens 🌆 (@matthiasbaetens) &lt;a href="https://twitter.com/matthiasbaetens/status/1046756260996149248?ref_src=twsrc%5Etfw">October 1, 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;h2 id="first-edition">First edition!&lt;/h2>
&lt;p>This first edition of the summit was a free event, with over 125 RSVPs. We had two days of content; day one was focused on the roadmap of the project, the ASF and use cases from companies that use Beam. The second day was divided into tracks (a beginner and an advanced track). Those presentations &amp;amp; workshops were organised for the more than &lt;strong>80 attendees&lt;/strong> - and next to that there were several other activities like discussions, a brainstorm session, a UX booth and a signing session. We were able to offer the attendees &lt;strong>17 sessions&lt;/strong> from a great line-up of companies:
Google, Spotify, Talend, Sky, Amazon, Data Artisans, Datatonic, Vente Exclusive, ML6, Flumaion, Plantix, Polidea, Seznam and more!&lt;/p>
&lt;br/>
#### Topics included using Python to run Beam on Flink:
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Don&amp;#39;t miss &lt;a href="https://twitter.com/snntrable?ref_src=twsrc%5Etfw">@snntrable&lt;/a>&amp;#39;s session at Beam Sumit London, Oct. 2, 2018, about &lt;a href="https://twitter.com/hashtag/Python?src=hash&amp;amp;ref_src=twsrc%5Etfw">#Python&lt;/a> Streaming Pipelines with &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> and &lt;a href="https://twitter.com/ApacheFlink?ref_src=twsrc%5Etfw">@ApacheFlink&lt;/a>! Register here: &lt;a href="https://t.co/wblzUeiTIg">https://t.co/wblzUeiTIg&lt;/a> &lt;a href="https://twitter.com/hashtag/streamprocessing?src=hash&amp;amp;ref_src=twsrc%5Etfw">#streamprocessing&lt;/a> &lt;a href="https://twitter.com/hashtag/BigData?src=hash&amp;amp;ref_src=twsrc%5Etfw">#BigData&lt;/a> &lt;a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;amp;ref_src=twsrc%5Etfw">#MachineLearning&lt;/a> &lt;a href="https://t.co/7Ph51WqspW">pic.twitter.com/7Ph51WqspW&lt;/a>&lt;/p>&amp;mdash; data Artisans (@dataArtisans) &lt;a href="https://twitter.com/dataArtisans/status/1044967266817847296?ref_src=twsrc%5Etfw">26 september 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;br/>
#### ML with Beam with the TensorFlow transform integration:
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Such a great pleasure to listen to the talk by &lt;a href="https://twitter.com/FsMatt?ref_src=twsrc%5Etfw">@FsMatt&lt;/a> on TensorFlow transform at the &lt;a href="https://twitter.com/hashtag/BeamSummit?src=hash&amp;amp;ref_src=twsrc%5Etfw">#BeamSummit&lt;/a>! &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> &lt;a href="https://twitter.com/TensorFlow?ref_src=twsrc%5Etfw">@TensorFlow&lt;/a> &lt;a href="https://twitter.com/Level39CW?ref_src=twsrc%5Etfw">@Level39CW&lt;/a> &lt;br>&lt;br>Check the code for his demo at &lt;a href="https://t.co/8IS41A2aF2">https://t.co/8IS41A2aF2&lt;/a> &lt;a href="https://t.co/UWytiudmRO">https://t.co/UWytiudmRO&lt;/a>&lt;/p>&amp;mdash; datatonic (@teamdatatonic) &lt;a href="https://twitter.com/teamdatatonic/status/1047126173493469184?ref_src=twsrc%5Etfw">2 oktober 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;br/>
#### The portability layer was a big topic:
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Excellent talk by &lt;a href="https://twitter.com/stadtlegende?ref_src=twsrc%5Etfw">@stadtlegende&lt;/a> on adding portability to &lt;a href="https://twitter.com/hashtag/ApacheBeam?src=hash&amp;amp;ref_src=twsrc%5Etfw">#ApacheBeam&lt;/a>, awesome milestone and next step to make the Apache Beam vision become a reality! &lt;a href="https://t.co/M9jERlTeAE">pic.twitter.com/M9jERlTeAE&lt;/a>&lt;/p>&amp;mdash; Matthias Feys (@FsMatt) &lt;a href="https://twitter.com/FsMatt/status/1047105336841244673?ref_src=twsrc%5Etfw">2 oktober 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;br/>
#### As well as a session on how to build your own SDK:
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Robert Bredshaw explains how to build a new &lt;a href="https://twitter.com/ApacheBeam?ref_src=twsrc%5Etfw">@ApacheBeam&lt;/a> SDK.&lt;a href="https://twitter.com/hashtag/BeamSummit?src=hash&amp;amp;ref_src=twsrc%5Etfw">#BeamSummit&lt;/a> &lt;a href="https://t.co/Bj84GJimdo">pic.twitter.com/Bj84GJimdo&lt;/a>&lt;/p>&amp;mdash; Maximilian Michels 🧗 (@stadtlegende) &lt;a href="https://twitter.com/stadtlegende/status/1047139320195366912?ref_src=twsrc%5Etfw">2 oktober 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;h2 id="presentations">Presentations&lt;/h2>
&lt;p>In the aftermath of the Summit, you can check the presentations of all the sessions.&lt;/p>
&lt;h3 id="day-1-use-cases">Day 1: Use cases&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/open?id=1hyHw7RVpFrFpli3vLt6JGBHrEm4BcgF-5nRdH1ZE8qo">Day 1 - Session 1 - Large scale stream analytics with Apache Beam at Sky&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1MxYrFDVoVFsrzbTtmr18zcbPFUU4nSdi">Day 1 - Session 2 - Running Quantitative Analytics with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=0B4bFLXEWuluSdVBJSnZrbTZjSGFHbnd4cExYOGZQU2hmY3lF">Day 1 - Session 3 - Talend Data Streams: Building Big Data pipelines with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1-GIUVn9QBtg6t-O8uINDkMO4PyZSU_HAEjMWuUHiYY4">Day 1 - Session 4 - Lesson Learned from Migrating to Apache Beam for Geo-Data Visualisation&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="day-2-beginners-track">Day 2: Beginners track&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/open?id=1ntQEDhb8gkxof4uFftxWTOfN39laUDZU7IDHTPKWrcQ">Day 2 - Beginner - Session 1 - Development Environment with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=0B4bFLXEWuluSWWJBWXV3ZTdseWpJN1o5UFdpSzV4Qi1sSGU0">Day 2 - Beginner - Session 2 - Towards Portability and Beyond&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=0B4bFLXEWuluSLTd6TFlYdFZZYjBTOFZQV3MxZzlPLWROWjZv">Day 2 - Beginner - Session 3 - Python Streaming Pipelines with Beam on Flink&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=0B4bFLXEWuluSMEV1a1cwM3ozeWQ4TkxlS0tFcnNtRGNGcjJ3">Day 2 - Beginner - Session 4 - How runners execute a Beam pipeline&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1QyqO8zJ3fIWD5DTnr1JNCEbm2dS15c1c02fI8zD-zqY">Day 2 - Beginner - Session 5 - IO Integration Testing framework in Apache Beam&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="day-2-advanced-track">Day 2: Advanced track&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/open?id=1Kr1skutObtDil2CExSQUb5rCVwZQm1m2lpmuAXFCE5I">Day 2 - Advanced - Session 1 - Pre-processing for TensorFlow pipelines with Apache Beam &amp;amp; tf.Transform&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=11x7gtuAxg76nOQKaB0YOwcvzS4TUeWONTU1ZQK0LsX8">Day 2 - Advanced - Session 2 - Streaming data into BigQuery: schema generation with Protobuf&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1cgQGBIXaACSwbYu_w3AkvvTdsCfeXAS1tBvQ77eVn74">Day 2 - Advanced - Session 3 - Implementing a SplittableParDo&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/presentation/d/1F02Lwnqm9H3cGqDQhIZ3gbftyLQSnVMRxX69H_d04OE/edit?usp=sharing">Day 2 - Advanced - Session 4 - Big Data on Google Cloud with Scala and Scio&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1D1ajcKoOR5OzehPwONdHLSzpO4PZOsLk">Day 2 - Advanced - Session 5 - Landuse Classification of Satellite Imagery&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1aFH6lhnVIq4Alu-_HItQ0QOddEPJQRqI5jV_t0o3CYI">Day 2 - Advanced - Session 6 - Java 8 DSL for Beam SDK&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://drive.google.com/open?id=1AkU-QXSflau-RSeolB4TSLy0_mg0xwb398Czw7aqVGw">Day 2 - Advanced - Session 7 - So, You Want to Write a Beam SDK?&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="recordings">Recordings&lt;/h2>
&lt;p>In case you prefer rewatching the recorded talks together with those slides, we are also happy to share the recordings of the majority of the sessions:&lt;/p>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/videoseries?list=PL4dEBWmGSIU_9JTGnkGVg6-BwaV0FMxyJ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe>
&lt;h3 id="day-1-use-cases-1">Day 1: Use cases&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/En0FrjvNr3M">Day 1 - Session 1 - Large scale stream analytics with Apache Beam at Sky&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/6yDEOUophuw">Day 1 - Session 2 - Running Quantitative Analytics with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/1AlEGUtiQek">Day 1 - Session 3 - Talend Data Streams: Building Big Data pipelines with Apache Beam&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/GBKqw03doHE">Day 1 - Session 4 - Lesson Learned from Migrating to Apache Beam for Geo-Data Visualisation&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="day-2-advanced-track-1">Day 2: Advanced track&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/L-k6-3ApXR4">Day 2 - Advanced - Session 1 - Pre-processing for TensorFlow pipelines with Apache Beam &amp;amp; tf.Transform&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/ctN5U_Ke8uk">Day 2 - Advanced - Session 2 - Streaming data into BigQuery: schema generation with Protobuf&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/jU6EmPyKefg">Day 2 - Advanced - Session 3 - Implementing a SplittableParDo&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/F0n9sqj1_NQ">Day 2 - Advanced - Session 4 - Big Data on Google Cloud with Scala and Scio&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/s-IR2eFe4B4">Day 2 - Advanced - Session 5 - Landuse Classification of Satellite Imagery&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/ott1e_CnZ04">Day 2 - Advanced - Session 6 - Java 8 DSL for Beam SDK&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/VsGQ2LFeTHY">Day 2 - Advanced - Session 7 - So, You Want to Write a Beam SDK?&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="wrapping-up">Wrapping up&lt;/h2>
&lt;p>We are also gathering feedback and thoughts on the Summit - please add your thoughts and discussions to the &lt;a href="https://lists.apache.org/thread.html/aa1306da25029dff12a49ba3ce63f2caf6a5f8ba73eda879c8403f3f@%3Cdev.beam.apache.org%3E">topic on the mailing list&lt;/a>.&lt;/p>
&lt;p>Overall, we hope our attendees enjoyed this first edition of our summit and want to thank &lt;strong>our sponsors Google, Datatonic, Vente-Exclusive&lt;/strong> to make this possible.&lt;/p>
&lt;blockquote class="twitter-tweet" data-lang="nl">&lt;p lang="en" dir="ltr">Wrapping up the first day of the &lt;a href="https://twitter.com/hashtag/BeamSummit?src=hash&amp;amp;ref_src=twsrc%5Etfw">#BeamSummit&lt;/a>. Excellent view from the &lt;a href="https://twitter.com/hashtag/level39?src=hash&amp;amp;ref_src=twsrc%5Etfw">#level39&lt;/a> venue. Very happy with the line up. &lt;a href="https://t.co/7FhokKbQY5">pic.twitter.com/7FhokKbQY5&lt;/a>&lt;/p>&amp;mdash; Alex Van Boxel (@alexvb) &lt;a href="https://twitter.com/alexvb/status/1046803829650608129?ref_src=twsrc%5Etfw">1 oktober 2018&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item><item><title>Blog: Apache Beam 2.8.0</title><link>/blog/beam-2.8.0/</link><pubDate>Mon, 29 Oct 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.8.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.8.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#280-2018-10-26">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.8.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12343985">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-4783">BEAM-4783&lt;/a> Performance degradations in certain situations when Spark runner is used.&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-upgrades">Dependency Upgrades&lt;/h3>
&lt;ul>
&lt;li>Elastic Search dependency upgraded to 6.3.2&lt;/li>
&lt;li>google-cloud-pubsub dependency upgraded to 0.35.4&lt;/li>
&lt;li>google-api-client dependency upgraded to 1.24.1&lt;/li>
&lt;li>Updated Flink Runner to 1.5.3&lt;/li>
&lt;li>Updated Spark runner to Spark version 2.3.2&lt;/li>
&lt;/ul>
&lt;h3 id="sdks">SDKs&lt;/h3>
&lt;ul>
&lt;li>Python SDK added support for user state and timers.&lt;/li>
&lt;li>Go SDK added support for side inputs.&lt;/li>
&lt;/ul>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/roadmap/portability/#python-on-flink">Python on Flink MVP&lt;/a> completed.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>Fixes to RedisIO non-prefix read operations.&lt;/li>
&lt;/ul>
&lt;h2 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h2>
&lt;ul>
&lt;li>Several bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed
to the 2.8.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Adam Horky, Ahmet Altay, Alan Myrvold, Aleksandr Kokhaniukov,
Alex Amato, Alexey Romanenko, Aljoscha Krettek, Andrew Fulton,
Andrew Pilloud, Ankur Goenka, Anton Kedin, Babu, Batkhuyag Batsaikhan, Ben Song,
Bingfeng Shu, Boyuan Zhang, Chamikara Jayalath, Charles Chen,
Christian Schneider, Cody Schroeder, Colm O hEigeartaigh, Daniel Mills,
Daniel Oliveira, Dat Tran, David Moravek, Dusan Rychnovsky, Etienne Chauchot,
Eugene Kirpichov, Gleb Kanterov, Heejong Lee, Henning Rohde, Ismaël Mejía,
Jan Lukavský, Jaromir Vanek, Jean-Baptiste Onofré, Jeff Klukas, Joar Wandborg,
Jozef Vilcek, Julien Phalip, Julien Tournay, Juta, Jára Vaněk,
Katarzyna Kucharczyk, Kengo Seki, Kenneth Knowles, Kevin Si, Kirill Kozlov,
Kyle Winkelman, Logan HAUSPIE, Lukasz Cwik, Manu Zhang, Mark Liu,
Matthias Baetens, Matthias Feys, Maximilian Michels, Melissa Pashniak,
Micah Wylde, Michael Luckey, Mike Pedersen, Mikhail Gryzykhin, Novotnik,
Petr, Ondrej Kvasnicka, Pablo Estrada, PaulVelthuis93, Pavel Slechta,
Rafael Fernández, Raghu Angadi, Renat, Reuven Lax, Robbe Sneyders,
Robert Bradshaw, Robert Burke, Rodrigo Benenson, Rong Ou, Ruoyun Huang,
Ryan Williams, Sam Rohde, Scott Wegner, Shinsuke Sugaya, Shnitz, Simon P,
Sindy Li, Stephen Lumenta, Stijn Decubber, Thomas Weise, Tomas Novak,
Tomas Roos, Udi Meiri, Vaclav Plajt, Valentyn Tymofieiev, Vitalii Tverdokhlib,
Xinyu Liu, XuMingmin, Yifan Zou, Yuan, Yueyang Qiu, aalbatross, amaliujia,
cclauss, connelloG, daidokoro, deepyaman, djhworld, flyisland, huygaa11,
jasonkuster, jglezt, kkpoon, mareksimunek, nielm, svXaverius, timrobertson100,
&lt;a href="mailto:vaclav.plajt@gmail.com">vaclav.plajt@gmail.com&lt;/a>, vitaliytv, vvarma, xiliu, xinyuiscool, xitep,
Łukasz Gajowy.&lt;/p></description></item><item><title>Blog: Apache Beam 2.7.0</title><link>/blog/beam-2.7.0/</link><pubDate>Wed, 03 Oct 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.7.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.7.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#270-lts-2018-10-02">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.7.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12343654">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;h3 id="new-ios">New I/Os&lt;/h3>
&lt;ul>
&lt;li>KuduIO&lt;/li>
&lt;li>Amazon SNS sink&lt;/li>
&lt;li>Amazon SqsIO&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-upgrades">Dependency Upgrades&lt;/h3>
&lt;ul>
&lt;li>Apache Calcite dependency upgraded to 1.17.0&lt;/li>
&lt;li>Apache Derby dependency upgraded to 10.14.2.0&lt;/li>
&lt;li>Apache HTTP components upgraded (see release notes).&lt;/li>
&lt;/ul>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;ul>
&lt;li>Experimental support for Python on local Flink runner for simple
examples, see latest information &lt;a href="/contribute/portability/#status">here&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h2>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>KinesisIO, fixed dependency issue&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following 72 people contributed
to the 2.7.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alexey Romanenko, Aljoscha Krettek,
Andrew Pilloud, Ankit Jhalaria, Ankur Goenka, Anton Kedin, Boyuan
Zhang, Carl McGraw, Carlos Alonso, cclauss, Chamikara Jayalath,
Charles Chen, Cory Brzycki, Daniel Oliveira, Dariusz Aniszewski,
devinduan, Eric Beach, Etienne Chauchot, Eugene Kirpichov, Garrett
Jones, Gene Peters, Gleb Kanterov, Henning Rohde, Henry Suryawirawan,
Holden Karau, Huygaa Batsaikhan, Ismaël Mejía, Jason Kuster, Jean-
Baptiste Onofré, Joachim van der Herten, Jozef Vilcek, jxlewis, Kai
Jiang, Katarzyna Kucharczyk, Kenn Knowles, Krzysztof Trubalski, Kyle
Winkelman, Leen Toelen, Luis Enrique Ortíz Ramirez, Lukasz Cwik,
Łukasz Gajowy, Luke Cwik, Mark Liu, Matthias Feys, Maximilian Michels,
Melissa Pashniak, Mikhail Gryzykhin, Mikhail Sokolov, mingmxu, Norbert
Chen, Pablo Estrada, Prateek Chanda, Raghu Angadi, Ravi Pathak, Reuven
Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ryan Williams, Sindy Li,
Thomas Weise, Tim Robertson, Tormod Haavi, Udi Meiri, Vaclav Plajt,
Valentyn Tymofieiev, xiliu, XuMingmin, Yifan Zou, Yueyang Qiu.&lt;/p></description></item><item><title>Blog: Beam Summit Europe 2018</title><link>/blog/beam-summit-europe/</link><pubDate>Tue, 21 Aug 2018 00:00:01 -0800</pubDate><guid>/blog/beam-summit-europe/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>With a growing community of contributors and users, the Apache Beam project is organising the first European Beam Summit.&lt;/p>
&lt;p>We are happy to invite you to this event, which will take place in &lt;strong>London&lt;/strong> on &lt;strong>October 1st and 2nd of 2018&lt;/strong>.&lt;/p>
&lt;p>&lt;img src="/images/blog/Facebook-AD.png" alt="Beam Summit Europe 2018 flyer" height="360" width="640" >&lt;/p>
&lt;h3 id="what-is-the-beam-summit-2018">What is the Beam Summit 2018?&lt;/h3>
&lt;p>The summit is a 2 day, multi-track event.&lt;/p>
&lt;p>During the first day we’ll host sessions to share use cases from companies using Apache Beam, community driven talks, and a session to discuss the project&amp;rsquo;s roadmap (from the main partners in the project as well as all users planning to contribute to the project and wanting to share their plans). We&amp;rsquo;ll also have break-out sessions that will allow cross team collaboration in multiple sub-topics.&lt;/p>
&lt;p>The second day will be a &amp;ldquo;hands-on&amp;rdquo; day. We will offer an introductory session to Apache Beam. Additionally, we&amp;rsquo;ll host an advanced track for more advanced users with open-table discussions about more complex and newer Apache Beam features.&lt;/p>
&lt;p>The agenda will grow and be communicated in the coming month, keep an eye on the page.&lt;/p>
&lt;h3 id="event-details">Event Details&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Venue&lt;/strong>: &lt;a href="https://goo.gl/maps/LAC4haDzSzR2">Level39, One Canada Square, Canary Wharf, London E14 5AB&lt;/a>&lt;/li>
&lt;li>&lt;strong>Dates&lt;/strong>: 1-2 October 2018&lt;/li>
&lt;/ul>
&lt;h3 id="how-do-i-register">How do I register?&lt;/h3>
&lt;p>You can register for free on the &lt;a href="https://www.eventbrite.com/e/beam-summit-london-2018-tickets-49100625292#tickets">Eventbrite registration page&lt;/a>.&lt;/p>
&lt;h3 id="i-am-interested-in-speaking-how-do-i-propose-my-session">I am interested in speaking, how do I propose my session?&lt;/h3>
&lt;p>With this we are also launching a Call for Papers in case you want to secure a slot for one of the sessions. Please fill out the &lt;a href="https://goo.gl/forms/nrZOCC1JwEfLtKfA2">CfP form&lt;/a>.&lt;/p>
&lt;h3 id="id-love-to-get-involved-as-a-volunteer-or-sponsor">I&amp;rsquo;d love to get involved as a volunteer or sponsor&lt;/h3>
&lt;p>Furthermore, in order to keep this event free, we are looking for people to help out at and/or sponsor some parts of the conference. If you (or your company) are interested to help out, please reach out to: &lt;a href="mailto:baetensmatthias@gmail.com">baetensmatthias@gmail.com&lt;/a> or &lt;a href="mailto:alex@vanboxel.be">alex@vanboxel.be&lt;/a>. You can find more info in the &lt;a href="https://drive.google.com/file/d/1RnZ52rGaB6BR-EKneBcabdMcg9Pl7z9M">sponsor booklet&lt;/a>&lt;/p>
&lt;p>Thanks, and we hope to see you at the event!
The Events &amp;amp; Meetups Group&lt;/p></description></item><item><title>Blog: A review of input streaming connectors</title><link>/blog/review-input-streaming-connectors/</link><pubDate>Mon, 20 Aug 2018 00:00:01 -0800</pubDate><guid>/blog/review-input-streaming-connectors/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this post, you&amp;rsquo;ll learn about the current state of support for input streaming connectors in &lt;a href="/">Apache Beam&lt;/a>. For more context, you&amp;rsquo;ll also learn about the corresponding state of support in &lt;a href="https://spark.apache.org/">Apache Spark&lt;/a>.&lt;/p>
&lt;p>With batch processing, you might load data from any source, including a database system. Even if there are no specific SDKs available for those database systems, you can often resort to using a &lt;a href="https://en.wikipedia.org/wiki/Java_Database_Connectivity">JDBC&lt;/a> driver. With streaming, implementing a proper data pipeline is arguably more challenging as generally fewer source types are available. For that reason, this article particularly focuses on the streaming use case.&lt;/p>
&lt;h2 id="connectors-for-java">Connectors for Java&lt;/h2>
&lt;p>Beam has an official &lt;a href="/documentation/sdks/java/">Java SDK&lt;/a> and has several execution engines, called &lt;a href="/documentation/runners/capability-matrix/">runners&lt;/a>. In most cases it is fairly easy to transfer existing Beam pipelines written in Java or Scala to a Spark environment by using the &lt;a href="/documentation/runners/spark/">Spark Runner&lt;/a>.&lt;/p>
&lt;p>Spark is written in Scala and has a &lt;a href="https://spark.apache.org/docs/latest/api/java/">Java API&lt;/a>. Spark&amp;rsquo;s source code compiles to &lt;a href="https://en.wikipedia.org/wiki/Java_(programming_language)#Java_JVM_and_Bytecode">Java bytecode&lt;/a> and the binaries are run by a &lt;a href="https://en.wikipedia.org/wiki/Java_virtual_machine">Java Virtual Machine&lt;/a>. Scala code is interoperable with Java and therefore has native compatibility with Java libraries (and vice versa).&lt;/p>
&lt;p>Spark offers two approaches to streaming: &lt;a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">Discretized Streaming&lt;/a> (or DStreams) and &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">Structured Streaming&lt;/a>. DStreams are a basic abstraction that represents a continuous series of &lt;a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">Resilient Distributed Datasets&lt;/a> (or RDDs). Structured Streaming was introduced more recently (the alpha release came with Spark 2.1.0) and is based on a &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#programming-model">model&lt;/a> where live data is continuously appended to a table structure.&lt;/p>
&lt;p>Spark Structured Streaming supports &lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html">file sources&lt;/a> (local filesystems and HDFS-compatible systems like Cloud Storage or S3) and &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html">Kafka&lt;/a> as streaming &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources">inputs&lt;/a>. Spark maintains built-in connectors for DStreams aimed at third-party services, such as Kafka or Flume, while other connectors are available through linking external dependencies, as shown in the table below.&lt;/p>
&lt;p>Below are the main streaming input connectors for available for Beam and Spark DStreams in Java:&lt;/p>
&lt;table class="table table-bordered">
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;td>&lt;strong>Apache Beam&lt;/strong>
&lt;/td>
&lt;td>&lt;strong>Apache Spark DStreams&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2" >File Systems
&lt;/td>
&lt;td>Local&lt;br>(Using the &lt;code>file://&lt;/code> URI)
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/TextIO.html">TextIO&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/StreamingContext.html#textFileStream-java.lang.String-">textFileStream&lt;/a>&lt;br>(Spark treats most Unix systems as HDFS-compatible, but the location should be accessible from all nodes)
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HDFS&lt;br>(Using the &lt;code>hdfs://&lt;/code> URI)
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/FileIO.html">FileIO&lt;/a> + &lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/hdfs/HadoopFileSystemOptions.html">HadoopFileSystemOptions&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/util/HdfsUtils.html">HdfsUtils&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2" >Object Stores
&lt;/td>
&lt;td>Cloud Storage&lt;br>(Using the &lt;code>gs://&lt;/code> URI)
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/FileIO.html">FileIO&lt;/a> + &lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/extensions/gcp/options/GcsOptions.html">GcsOptions&lt;/a>
&lt;/td>
&lt;td rowspan="2" >&lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#hadoopConfiguration--">hadoopConfiguration&lt;/a>
and &lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/StreamingContext.html#textFileStream-java.lang.String-">textFileStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>S3&lt;br>(Using the &lt;code>s3://&lt;/code> URI)
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/FileIO.html">FileIO&lt;/a> + &lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/aws/options/S3Options.html">S3Options&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="3" >Messaging Queues
&lt;/td>
&lt;td>Kafka
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/kafka/KafkaIO.html">KafkaIO&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html">spark-streaming-kafka&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kinesis
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/kinesis/KinesisIO.html">KinesisIO&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/streaming-kinesis-integration.html">spark-streaming-kinesis&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cloud Pub/Sub
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/javadoc/2.30.0/org/apache/beam/sdk/io/gcp/pubsub/PubsubIO.html">PubsubIO&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://github.com/apache/bahir/tree/master/streaming-pubsub">spark-streaming-pubsub&lt;/a> from &lt;a href="https://bahir.apache.org">Apache Bahir&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Other
&lt;/td>
&lt;td>Custom receivers
&lt;/td>
&lt;td>&lt;a href="/documentation/io/developing-io-overview/">Read Transforms&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/streaming-custom-receivers.html">receiverStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;h2 id="connectors-for-python">Connectors for Python&lt;/h2>
&lt;p>Beam has an official &lt;a href="/documentation/sdks/python/">Python SDK&lt;/a> that currently supports a subset of the streaming features available in the Java SDK. Active development is underway to bridge the gap between the featuresets in the two SDKs. Currently for Python, the &lt;a href="/documentation/runners/direct/">Direct Runner&lt;/a> and &lt;a href="/documentation/runners/dataflow/">Dataflow Runner&lt;/a> are supported, and &lt;a href="/documentation/sdks/python-streaming/">several streaming options&lt;/a> were introduced in beta in &lt;a href="/blog/2018/06/26/beam-2.5.0.html">version 2.5.0&lt;/a>.&lt;/p>
&lt;p>Spark also has a Python SDK called &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.html">PySpark&lt;/a>. As mentioned earlier, Scala code compiles to a bytecode that is executed by the JVM. PySpark uses &lt;a href="https://www.py4j.org/">Py4J&lt;/a>, a library that enables Python programs to interact with the JVM and therefore access Java libraries, interact with Java objects, and register callbacks from Java. This allows PySpark to access native Spark objects like RDDs. Spark Structured Streaming supports &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader">file sources&lt;/a> (local filesystems and HDFS-compatible systems like Cloud Storage or S3) and &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html">Kafka&lt;/a> as streaming inputs.&lt;/p>
&lt;p>Below are the main streaming input connectors for available for Beam and Spark DStreams in Python:&lt;/p>
&lt;table class="table table-bordered">
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;td>&lt;strong>Apache Beam&lt;/strong>
&lt;/td>
&lt;td>&lt;strong>Apache Spark DStreams&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2" >File Systems
&lt;/td>
&lt;td>Local
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/pydoc/2.30.0/apache_beam.io.textio.html">io.textio&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.textFileStream">textFileStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HDFS
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/pydoc/2.30.0/apache_beam.io.hadoopfilesystem.html">io.hadoopfilesystem&lt;/a>
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#hadoopConfiguration--">hadoopConfiguration&lt;/a> (Access through &lt;code>sc._jsc&lt;/code> with Py4J)
and &lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.textFileStream">textFileStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="2" >Object stores
&lt;/td>
&lt;td>Google Cloud Storage
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/pydoc/2.30.0/apache_beam.io.gcp.gcsio.html">io.gcp.gcsio&lt;/a>
&lt;/td>
&lt;td rowspan="2" >&lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.StreamingContext.textFileStream">textFileStream&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>S3
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td rowspan="3" >Messaging Queues
&lt;/td>
&lt;td>Kafka
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#pyspark.streaming.kafka.KafkaUtils">KafkaUtils&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kinesis
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;td>&lt;a href="https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html#module-pyspark.streaming.kinesis">KinesisUtils&lt;/a>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Cloud Pub/Sub
&lt;/td>
&lt;td>&lt;a href="https://beam.apache.org/releases/pydoc/2.30.0/apache_beam.io.gcp.pubsub.html">io.gcp.pubsub&lt;/a>
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Other
&lt;/td>
&lt;td>Custom receivers
&lt;/td>
&lt;td>&lt;a href="/documentation/sdks/python-custom-io/">BoundedSource and RangeTracker&lt;/a>
&lt;/td>
&lt;td>N/A
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;h2 id="connectors-for-other-languages">Connectors for other languages&lt;/h2>
&lt;h3 id="scala">Scala&lt;/h3>
&lt;p>Since Scala code is interoperable with Java and therefore has native compatibility with Java libraries (and vice versa), you can use the same Java connectors described above in your Scala programs. Apache Beam also has a &lt;a href="https://github.com/spotify/scio">Scala API&lt;/a> open-sourced &lt;a href="https://labs.spotify.com/2017/10/16/big-data-processing-at-spotify-the-road-to-scio-part-1/">by Spotify&lt;/a>.&lt;/p>
&lt;h3 id="go">Go&lt;/h3>
&lt;p>A &lt;a href="/documentation/sdks/go/">Go SDK&lt;/a> for Apache Beam is under active development. It is currently experimental and is not recommended for production. Spark does not have an official Go SDK.&lt;/p>
&lt;h3 id="r">R&lt;/h3>
&lt;p>Apache Beam does not have an official R SDK. Spark Structured Streaming is supported by an &lt;a href="https://spark.apache.org/docs/latest/sparkr.html#structured-streaming">R SDK&lt;/a>, but only for &lt;a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#input-sources">file sources&lt;/a> as a streaming input.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>We hope this article inspired you to try new and interesting ways of connecting streaming sources to your Beam pipelines!&lt;/p>
&lt;p>Check out the following links for further information:&lt;/p>
&lt;ul>
&lt;li>See a full list of all built-in and in-progress &lt;a href="/documentation/io/built-in/">I/O Transforms&lt;/a> for Apache Beam.&lt;/li>
&lt;li>Learn about some Apache Beam mobile gaming pipeline &lt;a href="/get-started/mobile-gaming-example/">examples&lt;/a>.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Apache Beam 2.6.0</title><link>/blog/beam-2.6.0/</link><pubDate>Fri, 10 Aug 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.6.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are glad to present the new 2.6.0 release of Beam.
This release includes multiple fixes and new functionality, such as new features in SQL and portability.&lt;/p>
&lt;p>We also spent a significant amount of time automating the release and fixing continuous integration. For more information, check the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12343392">release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;h3 id="grpcprotobuf-shading">gRPC/Protobuf shading&lt;/h3>
&lt;ul>
&lt;li>&lt;code>gRPC/protobuf&lt;/code> is now shaded in the majority of Apache Beam
Java modules. A few modules which expose &lt;code>gRPC/protobuf&lt;/code> on the
API surface still maintain a direct dependency.&lt;/li>
&lt;/ul>
&lt;h3 id="beam-sql">Beam SQL&lt;/h3>
&lt;ul>
&lt;li>Added support for the &lt;code>EXISTS&lt;/code> and &lt;code>LIKE&lt;/code> operators.&lt;/li>
&lt;li>Implemented &lt;code>SUM()&lt;/code> aggregations.&lt;/li>
&lt;li>Fixed issues with the &lt;code>CASE&lt;/code> expression.&lt;/li>
&lt;li>Added support for date comparisons.&lt;/li>
&lt;li>Added unbounded data support to &lt;code>LIMIT&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h3 id="portability">Portability&lt;/h3>
&lt;ul>
&lt;li>Shared libraries for supporting timers and user state
are now available for runner integration.&lt;/li>
&lt;li>Added a Universal Local Runner, which works on a single machine using portability and containerized SDK harnesses.&lt;/li>
&lt;li>The Flink Runner now accepts jobs using the Job API.&lt;/li>
&lt;/ul>
&lt;h3 id="ios">IOs&lt;/h3>
&lt;ul>
&lt;li>Bounded &lt;code>SplittableDoFn&lt;/code> (SDF) support is now available in all
runners (SDF is the new I/O connector API).&lt;/li>
&lt;li>&lt;code>HBaseIO&lt;/code> is the first I/O supporting Bounded SDF (using
&lt;code>readAll&lt;/code>).&lt;/li>
&lt;/ul>
&lt;h3 id="sdks">SDKs&lt;/h3>
&lt;ul>
&lt;li>Improved Python &lt;code>AvroIO&lt;/code> performance.&lt;/li>
&lt;li>Python &lt;code>AvroIO&lt;/code> has a &lt;code>use_fastavro&lt;/code> option that uses
&lt;code>fastavro&lt;/code> instead of &lt;code>apache/avro&lt;/code>, for a
&lt;a href="https://gist.github.com/ryan-williams/ede5ae61605e7ba6aa655071858ef52b">3-6x speedup&lt;/a>!&lt;/li>
&lt;/ul>
&lt;h3 id="other">Other&lt;/h3>
&lt;ul>
&lt;li>Updated various dependency versions.&lt;/li>
&lt;li>Improvements to stability, performance, and documentation.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following 39 people contributed
to the 2.6.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alexey Romanenko, Andrew Pilloud,
Ankur Goenka, Boyuan Zhang, Charles Chen, cclauss,
Daniel Oliveira, Elliott Brossard, Eric Beach,
Etienne Chauchot, Eugene Kirpichov, Henning Rohde,
Ismaël Mejía, Kai Jiang, Kasia, Kenneth Knowles, Luis Osa,
Lukasz Cwik, Maria Garcia Herrero, Mark Liu, Matthias Feys,
Pablo Estrada, Rafael Fernandez, Reuven Lax, Robert Bradshaw,
Robert Burke, Robin Qiu, Ryan Williams, Scott Wegner, Rui Weng,
Sergei Lebedev, Sindy Li, Thomas Weise, Udi Meiri,
Valentyn Tymofieiev, XuMingmin, and Yifan Zou.&lt;/p></description></item><item><title>Blog: Apache Beam 2.5.0</title><link>/blog/beam-2.5.0/</link><pubDate>Tue, 26 Jun 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.5.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are glad to present the new 2.5.0 release of Beam. This release includes
multiple fixes and new functionalities.&lt;/p>
&lt;p>For more information
please check the detailed release notes.&lt;/p>
&lt;h1 id="new-features--improvements">New Features / Improvements&lt;/h1>
&lt;h2 id="go-sdk-support">Go SDK support&lt;/h2>
&lt;p>The Go SDK has been officially accepted into the project, after an incubation period and community effort. Go pipelines run on Dataflow runner. More details are &lt;a href="/documentation/sdks/go/">here&lt;/a>.&lt;/p>
&lt;h2 id="parquet-support">Parquet support&lt;/h2>
&lt;p>Support for Apache Parquet format was added. It uses Parquet 1.10 release which, thanks to AvroParquerWriter&amp;rsquo;s API changes, allows FileIO.Sink implementation.&lt;/p>
&lt;h2 id="performanceintegration-tests">Performance/Integration Tests&lt;/h2>
&lt;ul>
&lt;li>Added new integration tests - HCatalogIOIT (Hive), HBaseIOIT, ParquetIOIT (with the IO itself, local filesystem, HDFS)&lt;/li>
&lt;li>Multinode (3 data node) HDFS cluster is used for running tests on HDFS.&lt;/li>
&lt;li>Several improvements on performance tests running and results analysis.&lt;/li>
&lt;li>Scaled up Kubernetes cluster from 1 to 3 nodes.&lt;/li>
&lt;li>Added metrics in Spark streaming.&lt;/li>
&lt;/ul>
&lt;h2 id="internal-build-system-migrated-to-gradle">Internal Build System: Migrated to Gradle&lt;/h2>
&lt;p>After a months-long community effort, the internal Beam build has been migrated from Maven to Gradle. The new build system was chosen because of dependency-driven build support, incremental build/test, and support for non-Java languages.&lt;/p>
&lt;h2 id="nexmark-improvements">Nexmark Improvements&lt;/h2>
&lt;ul>
&lt;li>Kafka support as a source/sink for events and results.&lt;/li>
&lt;li>Translation of some queries to Beam SQL.&lt;/li>
&lt;/ul>
&lt;h2 id="beam-sql">Beam SQL&lt;/h2>
&lt;ul>
&lt;li>Support for MAP, ROW, ARRAY data types&lt;/li>
&lt;li>Support UNNEST on array fields&lt;/li>
&lt;li>Improved optimizations&lt;/li>
&lt;li>Upgrade Calcite to 1.16&lt;/li>
&lt;li>Support SQL on POJOs via automatic conversion&lt;/li>
&lt;li>Schema moved into core Beam&lt;/li>
&lt;li>UDAFs can be indirect suclasses of CombineFn&lt;/li>
&lt;li>Many other small bugfixes&lt;/li>
&lt;/ul>
&lt;h2 id="portability">Portability&lt;/h2>
&lt;ul>
&lt;li>Common shared code related to supporting portable execution for runners.&lt;/li>
&lt;li>Python SDK supporting side inputs over the portability APIs.&lt;/li>
&lt;/ul>
&lt;h2 id="extract-metrics-in-a-runner-agnostic-way">Extract metrics in a runner agnostic way&lt;/h2>
&lt;p>Metrics are pushed by the runners to configurable sinks (Http REST sink available). It is already enabled in Flink and Spark runner, work is in progress for Dataflow.&lt;/p>
&lt;h1 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h1>
&lt;h2 id="sdks">SDKs&lt;/h2>
&lt;ul>
&lt;li>Implemented HDFS FileSystem for Python SDK.&lt;/li>
&lt;li>Python SDK adds support for side inputs for streaming execution.&lt;/li>
&lt;/ul>
&lt;h2 id="runners">Runners&lt;/h2>
&lt;ul>
&lt;li>Updated Spark runner to Spark version 2.3.1&lt;/li>
&lt;li>Fixed issue with late elements windowed into expired fixed windows get dropped in Directrunner.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">IOs&lt;/h2>
&lt;ul>
&lt;li>CassandraIO gained a better split algorithm based on overlapping regions.&lt;/li>
&lt;li>ElasticsearchIO supports partial updates.&lt;/li>
&lt;li>ElasticsearchIO allows to pass id, type and index per document.&lt;/li>
&lt;li>SolrIO supports a more robust retry on write strategy.&lt;/li>
&lt;li>S3 FileSystem supports encryption (SSE-S3, SSE-C and SSE-KMS).&lt;/li>
&lt;li>Improved connection management in JdbcIO.&lt;/li>
&lt;li>Added support the element timestamps while publishing to Kafka.&lt;/li>
&lt;/ul>
&lt;h2 id="other">Other&lt;/h2>
&lt;ul>
&lt;li>Use Java ErrorProne for static analysis.&lt;/li>
&lt;/ul>
&lt;h1 id="list-of-contributors">List of Contributors&lt;/h1>
&lt;p>According to git shortlog, the following 84 people contributed to the 2.5.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alex Van Boxel, Alexander Dejanovski, Alexey Romanenko, Aljoscha Krettek, ananvay, Andreas Ehrencrona, Andrew Pilloud, Ankur Goenka, Anton Kedin, arkash, Austin Bennett, Axel Magnuson, Ben Chambers, Ben Sidhom, Bill Neubauer, Boyuan Zhang, Braden Bassingthwaite, Cade Markegard, cclauss, Chamikara Jayalath, Charles Chen, Chuan Yu Foo, Cody Schroeder, Colm O hEigeartaigh, Daniel Oliveira, Dariusz Aniszewski, David Cavazos, Dawid Wysakowicz, Eric Roshan-Eisner, Etienne Chauchot, Eugene Kirpichov, Flavio Fiszman, Geet Kumar, GlennAmmons, Grzegorz Kołakowski, Henning Rohde, Innocent Djiofack, Ismaël Mejía, Jack Hsueh, Jason Kuster, Javier Antonio Gonzalez Trejo, Jean-Baptiste Onofré, Kai Jiang, Kamil Szewczyk, Katarzyna Kucharczyk, Kenneth Jung, Kenneth Knowles, Kevin Peterson, Lukasz Cwik, Łukasz Gajowy, Mairbek Khadikov, Manu Zhang, Maria Garcia Herrero, Marian Dvorsky, Mark Liu, Matthias Feys, Matthias Wessendorf, mingmxu, Nathan Howell, Pablo Estrada, Paul Gerver, Raghu Angadi, rarokni, Reuven Lax, Rezan Achmad, Robbe Sneyders, Robert Bradshaw, Robert Burke, Romain Manni-Bucau, Sam Waggoner, Sam Whittle, Scott Wegner, Stephan Hoyer, Thomas Groh, Thomas Weise, Tim Robertson, Udi Meiri, Valentyn Tymofieiev, XuMingmin, Yifan Zou, Yunqing Zhou&lt;/p></description></item><item><title>Blog: Apache Beam 2.3.0</title><link>/blog/beam-2.3.0/</link><pubDate>Mon, 19 Feb 2018 00:00:01 -0800</pubDate><guid>/blog/beam-2.3.0/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are glad to present the new 2.3.0 release of Beam. This release includes
multiple fixes and new functionalities.&lt;/p>
&lt;p>For more information
please check the detailed release notes.&lt;/p>
&lt;h1 id="new-features--improvements">New Features / Improvements&lt;/h1>
&lt;h2 id="beam-moves-to-java-8">Beam moves to Java 8&lt;/h2>
&lt;p>The supported version of Java for Beam is now Java 8. The code and examples have
been refactored to use multiple of the advantages of the language, e.g. lambdas,
streams, improved type inference, etc.&lt;/p>
&lt;h2 id="spark-runner-is-now-based-on-spark-2x">Spark runner is now based on Spark 2.x&lt;/h2>
&lt;p>Spark runner moves forward into the Spark 2.x development line, this would allow
to benefit of improved performance, as well as open the runner for future
compatibility with the Structured Streaming APIs. Notice that support for Spark
1.x is finished with this release.&lt;/p>
&lt;h2 id="amazon-web-services-s3-filesystem-support">Amazon Web Services S3 Filesystem support&lt;/h2>
&lt;p>Beam already supported AWS S3 via HadoopFileSystem, but this version brings a
native implementation with the corresponding performance advantages of the S3
filesystem.&lt;/p>
&lt;h2 id="general-purpose-writing-to-files">General-purpose writing to files&lt;/h2>
&lt;p>This release contains a new transform, FileIO.write() / writeDynamic() that
implements a general-purpose fluent and Java8-friendly API for writing to files
using a FileIO.Sink. This API has similar capabilities to DynamicDestinations
APIs from Beam 2.2 but is much easier to use and extend. The DynamicDestinations
APIs for writing to files are deprecated by it, as is FileBasedSink.&lt;/p>
&lt;h2 id="splittable-dofn-support-on-the-python-sdk">Splittable DoFn support on the Python SDK&lt;/h2>
&lt;p>This release adds the Splittable DoFn API for Python SDK and adds Splittable
DoFn support for Python streaming DirectRunner.&lt;/p>
&lt;h2 id="portability">Portability&lt;/h2>
&lt;p>Progress continues to being able to execute Python on runners other then Google
Cloud Dataflow and the Go SDK on any runner.&lt;/p>
&lt;h1 id="miscellaneous-fixes">Miscellaneous Fixes&lt;/h1>
&lt;h2 id="sdks">SDKs&lt;/h2>
&lt;ul>
&lt;li>MapElements and FlatMapElements support using side inputs using the new
interface Contextful.Fn. For library authors, this interface is the
recommended choice for user-code callbacks that may use side inputs.&lt;/li>
&lt;li>Introduces the family of Reify transforms for converting between explicit and
implicit representations of various Beam entities.&lt;/li>
&lt;li>Introduces two transforms for approximate sketching of data: Count-Min Sketch
(approximate element frequency estimation) and HyperLogLog (approximate
cardinality estimation).&lt;/li>
&lt;/ul>
&lt;h2 id="runners">Runners&lt;/h2>
&lt;ul>
&lt;li>Staging files on Dataflow shows progress&lt;/li>
&lt;li>Flink runner is based now on Flink version 1.4.0&lt;/li>
&lt;/ul>
&lt;h2 id="ios">IOs&lt;/h2>
&lt;ul>
&lt;li>BigtableIO now supports ValueProvider configuration&lt;/li>
&lt;li>BigQueryIO supports writing bounded collections to tables with partition
decorators&lt;/li>
&lt;li>KafkaIO moves to version 1.0 (it is still backwards compatible with versions &amp;gt;= 0.9.x.x)&lt;/li>
&lt;li>Added IO source for VCF files (Python)&lt;/li>
&lt;li>Added support for backoff on deadlocks in JdbcIO.write() and connection
improvement&lt;/li>
&lt;li>Improved performance of KinesisIO.read()&lt;/li>
&lt;li>Many improvements to TikaIO&lt;/li>
&lt;/ul>
&lt;h1 id="list-of-contributors">List of Contributors&lt;/h1>
&lt;p>According to git shortlog, the following 78 people contributed to the 2.3.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alan Myrvold, Alex Amato, Alexey Romanenko, Ankur Goenka, Anton Kedin, Arnaud Fournier, Asha Rostamianfar, Ben Chambers, Ben Sidhom, Bill Neubauer, Brian Foo, cclauss, Chamikara Jayalath, Charles Chen, Colm O hEigeartaigh, Daniel Oliveira, Dariusz Aniszewski, David Cavazos, David Sabater, David Sabater Dinter, Dawid Wysakowicz, Dmytro Ivanov, Etienne Chauchot, Eugene Kirpichov, Exprosed, Grzegorz Kołakowski, Henning Rohde, Holden Karau, Huygaa Batsaikhan, Ilya Figotin, Innocent Djiofack, Ismaël Mejía, Itamar Ostricher, Jacky, Jacob Marble, James Xu, Jean-Baptiste Onofré, Jeremie Lenfant-Engelmann, Kamil Szewczyk, Kenneth Knowles, Lukasz Cwik, Łukasz Gajowy, Luke Zhu, Mairbek Khadikov, María García Herrero, Marian Dvorsky, Mark Liu, melissa, Miles Saul, mingmxu, Motty Gruda, nerdynick, Neville Li, Nigel Kilmer, Pablo, Pawel Kaczmarczyk, Petr Shevtsov, Rafal Wojdyla, Raghu Angadi, Robert Bradshaw, Robert Burke, Romain Manni-Bucau, Ryan Niemocienski, Ryan Skraba, Sam Whittle, Scott Wegner, Shashank Prabhakara, Solomon Duskis, Thomas Groh, Thomas Weise, Udi Meiri, Valentyn Tymofieiev, wtanaka.com, XuMingmin, zhouhai02, Zohar Yahav, 琨瑜.&lt;/p></description></item><item><title>Blog: Apache Beam: A Look Back at 2017</title><link>/blog/beam-a-look-back/</link><pubDate>Tue, 09 Jan 2018 00:00:01 -0800</pubDate><guid>/blog/beam-a-look-back/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>On January 10, 2017, Apache Beam got &lt;a href="/blog/2017/01/10/beam-graduates.html">promoted&lt;/a>
as a Top-Level Apache Software Foundation project. It was an important milestone
that validated the value of the project, legitimacy of its community, and
heralded its growing adoption. In the past year, Apache Beam has been on a
phenomenal growth trajectory, with significant growth in its community and
feature set. Let us walk you through some of the notable achievements.&lt;/p>
&lt;h2 id="use-cases">Use cases&lt;/h2>
&lt;p>First, lets take a glimpse at how Beam was used in 2017. Apache Beam being a
unified framework for batch and stream processing, enables a very wide spectrum
of diverse use cases. Here are some use cases that exemplify the versatility of
Beam.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/2017-look-back/timeline.png"
alt="Use Cases"
width="600">&lt;/p>
&lt;h2 id="community-growth">Community growth&lt;/h2>
&lt;p>In 2017, Apache Beam had 174 contributors worldwide, from many different
organizations. As an Apache project, we are proud to count 18 PMC members and
31 committers. The community had 7 releases in 2017, each bringing a rich set of
new features and fixes.&lt;/p>
&lt;p>The most obvious and encouraging sign of the growth of Apache Beam’s community,
and validation of its core value proposition of portability, is the addition of
significant new &lt;a href="/documentation/runners/capability-matrix/">runners&lt;/a>
(i.e. execution engines). We entered 2017 with Apache Flink, Apache Spark 1.x,
Google Cloud Dataflow, Apache Apex, and Apache Gearpump. In 2017, the following
new and updated runners were developed:&lt;/p>
&lt;ul>
&lt;li>Apache Spark 2.x update&lt;/li>
&lt;li>&lt;a href="https://www.ibm.com/blogs/bluemix/2017/10/streaming-analytics-updates-ibm-streams-runner-apache-beam-2-0/">IBM Streams runner&lt;/a>&lt;/li>
&lt;li>MapReduce runner&lt;/li>
&lt;li>&lt;a href="http://jstorm.io/">JStorm runner&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>In addition to runners, Beam added new IO connectors, some notable ones being
the Cassandra, MQTT, AMQP, HBase/HCatalog, JDBC, Solr, Tika, Redis, and
Elasticsearch connectors. Beam’s IO connectors make it possible to read from or
write to data sources/sinks even when they are not natively supported by the
underlying execution engine. Beam also provides fully pluggable filesystem
support, allowing us to support and extend our coverage to HDFS, S3, Azure
Storage, and Google Storage. We continue to add new IO connectors and
filesystems to extend the Beam use cases.&lt;/p>
&lt;p>A particularly telling sign of the maturity of an open source community is when
it is able to collaborate with multiple other open source communities, and
mutually improve the state of the art. Over the past few months, the Beam,
Calcite, and Flink communities have come together to define a robust &lt;a href="https://docs.google.com/document/d/1wrla8mF_mmq-NW9sdJHYVgMyZsgCmHumJJ5f5WUzTiM/edit">spec&lt;/a>
for Streaming SQL, with engineers from over four organizations contributing to
it. If, like us, you are excited by the prospect of improving the state of
streaming SQL, please join us!&lt;/p>
&lt;p>In addition to SQL, new XML and JSON based declarative DSLs are also in PoC.&lt;/p>
&lt;h2 id="continued-innovation">Continued innovation&lt;/h2>
&lt;p>Innovation is important to the success on any open source project, and Beam has
a rich history of bringing innovative new ideas to the open source community.
Apache Beam was the first to introduce some seminal concepts in the world of
big-data processing:&lt;/p>
&lt;ul>
&lt;li>Unified batch and streaming SDK that enables users to author big-data jobs
without having to learn multiple disparate SDKs/APIs.&lt;/li>
&lt;li>Cross-Engine Portability: Giving enterprises the confidence that workloads
authored today will not have to be re-written when open source engines become
outdated and are supplanted by newer ones.&lt;/li>
&lt;li>&lt;a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">Semantics&lt;/a>
essential for reasoning about unbounded unordered data, and achieving
consistent and correct output from a streaming job.&lt;/li>
&lt;/ul>
&lt;p>In 2017, the pace of innovation continued. The following capabilities were
introduced:&lt;/p>
&lt;ul>
&lt;li>Cross-Language Portability framework, and a &lt;a href="https://golang.org/">Go&lt;/a> SDK
developed with it.&lt;/li>
&lt;li>Dynamically Shardable IO (SplittableDoFn)&lt;/li>
&lt;li>Support for schemas in PCollection, allowing us to extend the runner
capabilities.&lt;/li>
&lt;li>Extensions addressing new use cases such as machine learning, and new data
formats.&lt;/li>
&lt;/ul>
&lt;h2 id="areas-of-improvement">Areas of improvement&lt;/h2>
&lt;p>Any retrospective view of a project is incomplete without an honest assessment
of areas of improvement. Two aspects stand out:&lt;/p>
&lt;ul>
&lt;li>Helping runners showcase their individual strengths. After all, portability
does not imply homogeneity. Different runners have different areas in which
they excel, and we need to do a better job of helping them highlight their
strengths.&lt;/li>
&lt;li>Based on the previous point, helping customers make a more informed decision
when they select a runner or migrate from one to another.&lt;/li>
&lt;/ul>
&lt;p>In 2018, we aim to take proactive steps to improve the above aspects.&lt;/p>
&lt;h2 id="ethos-of-the-project-and-its-community">Ethos of the project and its community&lt;/h2>
&lt;p>The world of batch and stream big-data processing today is reminiscent of the
&lt;a href="https://en.wikipedia.org/wiki/Tower_of_Babel">Tower of Babel&lt;/a> parable: a
slowdown of progress because different communities spoke different languages.
Similarly, today there are multiple disparate big-data SDKs/APIs, each with
their own distinct terminology to describe similar concepts. The side effect is
user confusion and slower adoption.&lt;/p>
&lt;p>The Apache Beam project aims to provide an industry standard portable SDK that
will:&lt;/p>
&lt;ul>
&lt;li>Benefit users by providing &lt;em>&lt;strong>innovation with stability&lt;/strong>&lt;/em>: The separation of
SDK and engine enables healthy competition between runners, without requiring
users to constantly learn new SDKs/APIs and rewrite their workloads to
benefit from new innovation.&lt;/li>
&lt;li>Benefit big-data engines by &lt;em>&lt;strong>growing the pie for everyone&lt;/strong>&lt;/em>: Making it
easier for users to author, maintain, upgrade and migrate their big-data
workloads will lead to significant growth in the number of production
big-data deployments.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Timely (and Stateful) Processing with Apache Beam</title><link>/blog/timely-processing/</link><pubDate>Mon, 28 Aug 2017 00:00:01 -0800</pubDate><guid>/blog/timely-processing/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In a &lt;a href="/blog/2017/02/13/stateful-processing.html">prior blog
post&lt;/a>, I
introduced the basics of stateful processing in Apache Beam, focusing on the
addition of state to per-element processing. So-called &lt;em>timely&lt;/em> processing
complements stateful processing in Beam by letting you set timers to request a
(stateful) callback at some point in the future.&lt;/p>
&lt;p>What can you do with timers in Beam? Here are some examples:&lt;/p>
&lt;ul>
&lt;li>You can output data buffered in state after some amount of processing time.&lt;/li>
&lt;li>You can take special action when the watermark estimates that you have
received all data up to a specified point in event time.&lt;/li>
&lt;li>You can author workflows with timeouts that alter state and emit output in
response to the absence of additional input for some period of time.&lt;/li>
&lt;/ul>
&lt;p>These are just a few possibilities. State and timers together form a powerful
programming paradigm for fine-grained control to express a huge variety of
workflows. Stateful and timely processing in Beam is portable across data
processing engines and integrated with Beam&amp;rsquo;s unified model of event time
windowing in both streaming and batch processing.&lt;/p>
&lt;h2 id="what-is-stateful-and-timely-processing">What is stateful and timely processing?&lt;/h2>
&lt;p>In my prior post, I developed an understanding of stateful processing largely
by contrast with associative, commutative combiners. In this post, I&amp;rsquo;ll
emphasize a perspective that I had mentioned only briefly: that elementwise
processing with access to per-key-and-window state and timers represents a
fundamental pattern for &amp;ldquo;embarrassingly parallel&amp;rdquo; computation, distinct from
the others in Beam.&lt;/p>
&lt;p>In fact, stateful and timely computation is the low-level computational pattern
that underlies the others. Precisely because it is lower level, it allows you
to really micromanage your computations to unlock new use cases and new
efficiencies. This incurs the complexity of manually managing your state and
timers - it isn&amp;rsquo;t magic! Let&amp;rsquo;s first look again at the two primary
computational patterns in Beam.&lt;/p>
&lt;h3 id="element-wise-processing-pardo-map-etc">Element-wise processing (ParDo, Map, etc)&lt;/h3>
&lt;p>The most elementary embarrassingly parallel pattern is just using a bunch of
computers to apply the same function to every input element of a massive
collection. In Beam, per-element processing like this is expressed as a basic
&lt;code>ParDo&lt;/code> - analogous to &amp;ldquo;Map&amp;rdquo; from MapReduce - which is like an enhanced &amp;ldquo;map&amp;rdquo;,
&amp;ldquo;flatMap&amp;rdquo;, etc, from functional programming.&lt;/p>
&lt;p>The following diagram illustrates per-element processing. Input elements are
squares, output elements are triangles. The colors of the elements represent
their key, which will matter later. Each input element maps to the
corresponding output element(s) completely independently. Processing may be
distributed across computers in any way, yielding essentially limitless
parallelism.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/ParDo.png"
alt="ParDo offers limitless parallelism"
width="600">&lt;/p>
&lt;p>This pattern is obvious, exists in all data-parallel paradigms, and has
a simple stateless implementation. Every input element can be processed
independently or in arbitrary bundles. Balancing the work between computers is
actually the hard part, and can be addressed by splitting, progress estimation,
work-stealing, etc.&lt;/p>
&lt;h3 id="per-key-and-window-aggregation-combine-reduce-groupbykey-etc">Per-key (and window) aggregation (Combine, Reduce, GroupByKey, etc.)&lt;/h3>
&lt;p>The other embarassingly parallel design pattern at the heart of Beam is per-key
(and window) aggregation. Elements sharing a key are colocated and then
combined using some associative and commutative operator. In Beam this is
expressed as a &lt;code>GroupByKey&lt;/code> or &lt;code>Combine.perKey&lt;/code>, and corresponds to the shuffle
and &amp;ldquo;Reduce&amp;rdquo; from MapReduce. It is sometimes helpful to think of per-key
&lt;code>Combine&lt;/code> as the fundamental operation, and raw &lt;code>GroupByKey&lt;/code> as a combiner that
just concatenates input elements. The communication pattern for the input
elements is the same, modulo some optimizations possible for &lt;code>Combine&lt;/code>.&lt;/p>
&lt;p>In the illustration here, recall that the color of each element represents the
key. So all of the red squares are routed to the same location where they are
aggregated and the red triangle is the output. Likewise for the yellow and
green squares, etc. In a real application, you may have millions of keys, so
the parallelism is still massive.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/CombinePerKey.png"
alt="Gathering elements per key then combining them"
width="600">&lt;/p>
&lt;p>The underlying data processing engine will, at some level of abstraction, use
state to perform this aggregation across all the elements arriving for a key.
In particular, in a streaming execution, the aggregation process may need to
wait for more data to arrive or for the watermark to estimate that all input
for an event time window is complete. This requires some way to store the
intermediate aggregation between input elements as well a way to a receive a
callback when it is time to emit the result. As a result, the &lt;em>execution&lt;/em> of
per key aggregation by a stream processing engine fundamentally involves state
and timers.&lt;/p>
&lt;p>However, &lt;em>your&lt;/em> code is just a declarative expression of the aggregation
operator. The runner can choose a variety of ways to execute your operator.
I went over this in detail in &lt;a href="/blog/2017/02/13/stateful-processing.html">my prior post focused on state alone&lt;/a>. Since you do not
observe elements in any defined order, nor manipulate mutable state or timers
directly, I call this neither stateful nor timely processing.&lt;/p>
&lt;h3 id="per-key-and-window-stateful-timely-processing">Per-key-and-window stateful, timely processing&lt;/h3>
&lt;p>Both &lt;code>ParDo&lt;/code> and &lt;code>Combine.perKey&lt;/code> are standard patterns for parallelism that go
back decades. When implementing these in a massive-scale distributed data
processing engine, we can highlight a few characteristics that are particularly
important.&lt;/p>
&lt;p>Let us consider these characteristics of &lt;code>ParDo&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>You write single-threaded code to process one element.&lt;/li>
&lt;li>Elements are processed in arbitrary order with no dependencies
or interaction between processing of elements.&lt;/li>
&lt;/ul>
&lt;p>And these characteristics for &lt;code>Combine.perKey&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Elements for a common key and window are gathered together.&lt;/li>
&lt;li>A user-defined operator is applied to those elements.&lt;/li>
&lt;/ul>
&lt;p>Combining some of the characteristics of unrestricted parallel mapping and
per-key-and-window combination, we can discern a megaprimitive from which we
build stateful and timely processing:&lt;/p>
&lt;ul>
&lt;li>Elements for a common key and window are gathered together.&lt;/li>
&lt;li>Elements are processed in arbitrary order.&lt;/li>
&lt;li>You write single-threaded code to process one element or timer, possibly
accessing state or setting timers.&lt;/li>
&lt;/ul>
&lt;p>In the illustration below, the red squares are gathered and fed one by one to
the stateful, timely, &lt;code>DoFn&lt;/code>. As each element is processed, the &lt;code>DoFn&lt;/code> has
access to state (the color-partitioned cylinder on the right) and can set
timers to receive callbacks (the colorful clocks on the left).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/StateAndTimers.png"
alt="Gathering elements per key then timely, stateful processing"
width="600">&lt;/p>
&lt;p>So that is the abstract notion of per-key-and-window stateful, timely
processing in Apache Beam. Now let&amp;rsquo;s see what it looks like to write code that
accesses state, sets timers, and receives callbacks.&lt;/p>
&lt;h2 id="example-batched-rpc">Example: Batched RPC&lt;/h2>
&lt;p>To demonstrate stateful and timely processing, let&amp;rsquo;s work through a concrete
example, with code.&lt;/p>
&lt;p>Suppose you are writing a system to analyze events. You have a ton of data
coming in and you need to enrich each event by RPC to an external system. You
can&amp;rsquo;t just issue an RPC per event. Not only would this be terrible for
performance, but it would also likely blow your quota with the external system.
So you&amp;rsquo;d like to gather a number of events, make one RPC for them all, and then
output all the enriched events.&lt;/p>
&lt;h3 id="state">State&lt;/h3>
&lt;p>Let&amp;rsquo;s set up the state we need to track batches of elements. As each element
comes in, we will write the element to a buffer while tracking the number of
elements we have buffered. Here are the state cells in code:&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">EnrichedEvent&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bufferedEvents&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">bag&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">TBD&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class='language-py snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">StatefulBufferingFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">BUFFER_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">BagStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;buffer&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">EventCoder&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">COUNT_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">CombiningValueStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;count&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">VarIntCoder&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;span class="n">combiners&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SumCombineFn&lt;/span>&lt;span class="p">())&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Walking through the code, we have:&lt;/p>
&lt;ul>
&lt;li>The state cell &lt;code>&amp;quot;buffer&amp;quot;&lt;/code> is an unordered bag of buffered events.&lt;/li>
&lt;li>The state cell &lt;code>&amp;quot;count&amp;quot;&lt;/code> tracks how many events have been buffered.&lt;/li>
&lt;/ul>
&lt;p>Next, as a recap of reading and writing state, let&amp;rsquo;s write our &lt;code>@ProcessElement&lt;/code>
method. We will choose a limit on the size of the buffer, &lt;code>MAX_BUFFER_SIZE&lt;/code>. If
our buffer reaches this size, we will perform a single RPC to enrich all the
events, and output.&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">EnrichedEvent&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">MAX_BUFFER_SIZE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">500&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">bufferedEvents&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">bag&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">StateSpec&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">StateSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">value&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="kt">int&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">firstNonNull&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">1&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">write&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">count&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">add&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">element&lt;/span>&lt;span class="o">());&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">count&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">MAX_BUFFER_SIZE&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">EnrichedEvent&lt;/span> &lt;span class="n">enrichedEvent&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">enrichEvents&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">()))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">enrichedEvent&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">TBD&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class='language-py snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">StatefulBufferingFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">MAX_BUFFER_SIZE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">500&lt;/span>&lt;span class="p">;&lt;/span>
&lt;span class="n">BUFFER_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">BagStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;buffer&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">EventCoder&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">COUNT_STATE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">CombiningValueStateSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;count&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">VarIntCoder&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;span class="n">combiners&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SumCombineFn&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">element&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">count&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">count&lt;/span> &lt;span class="o">&amp;gt;=&lt;/span> &lt;span class="n">MAX_BUFFER_SIZE&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">event&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">():&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">event&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Here is an illustration to accompany the code:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/BatchedRpcState.png"
alt="Batching elements in state, then performing RPCs"
width="600">&lt;/p>
&lt;ul>
&lt;li>The blue box is the &lt;code>DoFn&lt;/code>.&lt;/li>
&lt;li>The yellow box within it is the &lt;code>@ProcessElement&lt;/code> method.&lt;/li>
&lt;li>Each input event is a red square - this diagram just shows the activity for
a single key, represented by the color red. Your &lt;code>DoFn&lt;/code> will run the same
workflow in parallel for all keys which are perhaps user IDs.&lt;/li>
&lt;li>Each input event is written to the buffer as a red triangle, representing
the fact that you might actually buffer more than just the raw input, even
though this code doesn&amp;rsquo;t.&lt;/li>
&lt;li>The external service is drawn as a cloud. When there are enough buffered
events, the &lt;code>@ProcessElement&lt;/code> method reads the events from state and issues
a single RPC.&lt;/li>
&lt;li>Each output enriched event is drawn as a red circle. To consumers of this
output, it looks just like an element-wise operation.&lt;/li>
&lt;/ul>
&lt;p>So far, we have only used state, but not timers. You may have noticed that
there is a problem - there will usually be data left in the buffer. If no more
input arrives, that data will never be processed. In Beam, every window has
some point in event time when any further input for the window is considered
too late and is discarded. At this point, we say that the window has &amp;ldquo;expired&amp;rdquo;.
Since no further input can arrive to access the state for that window, the
state is also discarded. For our example, we need to ensure that all leftover
events are output when the window expires.&lt;/p>
&lt;h3 id="event-time-timers">Event Time Timers&lt;/h3>
&lt;p>An event time timer requests a call back when the watermark for an input
&lt;code>PCollection&lt;/code> reaches some threshold. In other words, you can use an event time
timer to take action at a specific moment in event time - a particular point of
completeness for a &lt;code>PCollection&lt;/code> - such as when a window expires.&lt;/p>
&lt;p>For our example, let us add an event time timer so that when the window expires,
any events remaining in the buffer are processed.&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">EnrichedEvent&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="err">…&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;expiry&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">TimerSpec&lt;/span> &lt;span class="n">expirySpec&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TimerSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">EVENT_TIME&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">BoundedWindow&lt;/span> &lt;span class="n">window&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;expiry&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">expiryTimer&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">expiryTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">set&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">window&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">maxTimestamp&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">plus&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">allowedLateness&lt;/span>&lt;span class="o">));&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">logic&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@OnTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;expiry&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">onExpiry&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">OnTimerContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(!&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">isEmpty&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">())&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">EnrichedEvent&lt;/span> &lt;span class="n">enrichedEvent&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">enrichEvents&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">()))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">enrichedEvent&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class='language-py snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">StatefulBufferingFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="err">…&lt;/span>
&lt;span class="n">EXPIRY_TIMER&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TimerSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;expiry&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WATERMARK&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">w&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowParam&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">expiry_timer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TimerParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">EXPIRY_TIMER&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">expiry_timer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">end&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">ALLOWED_LATENESS&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">logic&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="nd">@on_timer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">EXPIRY_TIMER&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">expiry&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">event&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">events&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">event&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Let&amp;rsquo;s unpack the pieces of this snippet:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>We declare an event time timer with &lt;code>@TimerId(&amp;quot;expiry&amp;quot;)&lt;/code>. We will use the
identifier &lt;code>&amp;quot;expiry&amp;quot;&lt;/code> to identify the timer for setting the callback time as
well as receiving the callback.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The variable &lt;code>expiryTimer&lt;/code>, annotated with &lt;code>@TimerId&lt;/code>, is set to the value
&lt;code>TimerSpecs.timer(TimeDomain.EVENT_TIME)&lt;/code>, indicating that we want a
callback according to the event time watermark of the input elements.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the &lt;code>@ProcessElement&lt;/code> element we annotate a parameter &lt;code>@TimerId(&amp;quot;expiry&amp;quot;) Timer&lt;/code>. The Beam runner automatically provides this &lt;code>Timer&lt;/code> parameter by which
we can set (and reset) the timer. It is inexpensive to reset a timer
repeatedly, so we simply set it on every element.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We define the &lt;code>onExpiry&lt;/code> method, annotated with &lt;code>@OnTimer(&amp;quot;expiry&amp;quot;)&lt;/code>, that
performs a final event enrichment RPC and outputs the result. The Beam runner
delivers the callback to this method by matching its identifier.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Illustrating this logic, we have the diagram below:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/BatchedRpcExpiry.png"
alt="Batched RPCs with window expiration"
width="600">&lt;/p>
&lt;p>Both the &lt;code>@ProcessElement&lt;/code> and &lt;code>@OnTimer(&amp;quot;expiry&amp;quot;)&lt;/code> methods perform the same
access to buffered state, perform the same batched RPC, and output enriched
elements.&lt;/p>
&lt;p>Now, if we are executing this in a streaming real-time manner, we might still
have unbounded latency for particular buffered data. If the watermark is advancing
very slowly, or event time windows are chosen to be quite large, then a lot of
time might pass before output is emitted based either on enough elements or
window expiration. We can also use timers to limit the amount of wall-clock
time, aka processing time, before we process buffered elements. We can choose
some reasonable amount of time so that even though we are issuing RPCs that are
not as large as they might be, it is still few enough RPCs to avoid blowing our
quota with the external service.&lt;/p>
&lt;h3 id="processing-time-timers">Processing Time Timers&lt;/h3>
&lt;p>A timer in processing time (time as it passes while your pipeline is executing)
is intuitively simple: you want to wait a certain amount of time and then
receive a call back.&lt;/p>
&lt;p>To put the finishing touches on our example, we will set a processing time
timer as soon as any data is buffered. Note that we set the timer only when
the current buffer is empty, so that we don&amp;rsquo;t continually reset the timer.
When the first element arrives, we set the timer for the current moment plus
&lt;code>MAX_BUFFER_DURATION&lt;/code>. After the allotted processing time has passed, a
callback will fire and enrich and emit any buffered elements.&lt;/p>
&lt;div class='language-java snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="k">new&lt;/span> &lt;span class="n">DoFn&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">EnrichedEvent&lt;/span>&lt;span class="o">&amp;gt;()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="err">…&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">Duration&lt;/span> &lt;span class="n">MAX_BUFFER_DURATION&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Duration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">standardSeconds&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">1&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;stale&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">private&lt;/span> &lt;span class="kd">final&lt;/span> &lt;span class="n">TimerSpec&lt;/span> &lt;span class="n">staleSpec&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TimerSpecs&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">timer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">PROCESSING_TIME&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="nd">@ProcessElement&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">ProcessContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="n">BoundedWindow&lt;/span> &lt;span class="n">window&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;stale&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">staleTimer&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@TimerId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;expiry&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">Timer&lt;/span> &lt;span class="n">expiryTimer&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">firstNonNull&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">(),&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">0&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">staleTimer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">offset&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">MAX_BUFFER_DURATION&lt;/span>&lt;span class="o">).&lt;/span>&lt;span class="na">setRelative&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">processing&lt;/span> &lt;span class="n">logic&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@OnTimer&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;stale&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">onStale&lt;/span>&lt;span class="o">(&lt;/span>
&lt;span class="n">OnTimerContext&lt;/span> &lt;span class="n">context&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;buffer&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">BagState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Event&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">bufferState&lt;/span>&lt;span class="o">,&lt;/span>
&lt;span class="nd">@StateId&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="n">ValueState&lt;/span>&lt;span class="o">&amp;lt;&lt;/span>&lt;span class="n">Integer&lt;/span>&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="n">countState&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="o">(!&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">isEmpty&lt;/span>&lt;span class="o">().&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">())&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">EnrichedEvent&lt;/span> &lt;span class="n">enrichedEvent&lt;/span> &lt;span class="o">:&lt;/span> &lt;span class="n">enrichEvents&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">read&lt;/span>&lt;span class="o">()))&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">context&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">output&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">enrichedEvent&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="n">bufferState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">countState&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">clear&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">expiry&lt;/span> &lt;span class="n">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="o">}&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class='language-py snippet'>
&lt;div class="notebook-skip code-snippet">
&lt;a class="copy" type="button" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Copy to clipboard">
&lt;img src="/images/copy-icon.svg"/>
&lt;/a>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="k">class&lt;/span> &lt;span class="nc">StatefulBufferingFn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="err">…&lt;/span>
&lt;span class="n">STALE_TIMER&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">TimerSpec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;stale&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">TimeDomain&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">REAL_TIME&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">MAX_BUFFER_DURATION&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">process&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">element&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">w&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WindowParam&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">expiry_timer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TimerParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">EXPIRY_TIMER&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">stale_timer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TimerParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">STALE_TIMER&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># We set an absolute timestamp here (not an offset like in the Java SDK)&lt;/span>
&lt;span class="n">stale_timer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">time&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">StatefulBufferingFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MAX_BUFFER_DURATION&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="err">…&lt;/span> &lt;span class="n">same&lt;/span> &lt;span class="n">logic&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">above&lt;/span> &lt;span class="err">…&lt;/span>
&lt;span class="nd">@on_timer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">STALE_TIMER&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">stale&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">BUFFER_STATE&lt;/span>&lt;span class="p">),&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DoFn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StateParam&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">COUNT_STATE&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;span class="n">events&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">read&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">event&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">events&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">event&lt;/span>
&lt;span class="n">buffer_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="n">count_state&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">clear&lt;/span>&lt;span class="p">()&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div>
&lt;/div>
&lt;p>Here is an illustration of the final code:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/BatchedRpcStale.png"
alt="Batching elements in state, then performing RPCs"
width="600">&lt;/p>
&lt;p>Recapping the entirety of the logic:&lt;/p>
&lt;ul>
&lt;li>As events arrive at &lt;code>@ProcessElement&lt;/code> they are buffered in state.&lt;/li>
&lt;li>If the size of the buffer exceeds a maximum, the events are enriched and output.&lt;/li>
&lt;li>If the buffer fills too slowly and the events get stale before the maximum is reached,
a timer causes a callback which enriches the buffered events and outputs.&lt;/li>
&lt;li>Finally, as any window is expiring, any events buffered in that window are
processed and output prior to the state for that window being discarded.&lt;/li>
&lt;/ul>
&lt;p>In the end, we have a full example that uses state and timers to explicitly
manage the low-level details of a performance-sensitive transform in Beam. As
we added more and more features, our &lt;code>DoFn&lt;/code> actually became pretty large. That
is a normal characteristic of stateful, timely processing. You are really
digging in and managing a lot of details that are handled automatically when
you express your logic using Beam&amp;rsquo;s higher-level APIs. What you gain from this
extra effort is an ability to tackle use cases and achieve efficiencies that
may not have been possible otherwise.&lt;/p>
&lt;h2 id="state-and-timers-in-beams-unified-model">State and Timers in Beam&amp;rsquo;s Unified Model&lt;/h2>
&lt;p>Beam&amp;rsquo;s unified model for event time across streaming and batch processing has
novel implications for state and timers. Usually, you don&amp;rsquo;t need to do anything
for your stateful and timely &lt;code>DoFn&lt;/code> to work well in the Beam model. But it will
help to be aware of the considerations below, especially if you have used
similar features before outside of Beam.&lt;/p>
&lt;h3 id="event-time-windowing-just-works">Event Time Windowing &amp;ldquo;Just Works&amp;rdquo;&lt;/h3>
&lt;p>One of the raisons d'être for Beam is correct processing of out-of-order event
data, which is almost all event data. Beam&amp;rsquo;s solution to out-of-order data is
event time windowing, where windows in event time yield correct results no
matter what windowing a user chooses or what order the events come in.&lt;/p>
&lt;p>If you write a stateful, timely transform, it should work no matter how the
surrounding pipeline chooses to window event time. If the pipeline chooses
fixed windows of one hour (sometimes called tumbling windows) or windows of 30
minutes sliding by 10 minutes, the stateful, timely transform should
transparently work correctly.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/WindowingChoices.png"
alt="Two windowing strategies for the same stateful and timely transform"
width="600">&lt;/p>
&lt;p>This works in Beam automatically, because state and timers are partitioned per
key and window. Within each key and window, the stateful, timely processing is
essentially independent. As an added benefit, the passing of event time (aka
advancement of the watermark) allows automatic release of unreachable state
when a window expires, so you often don&amp;rsquo;t have to worry about evicting old
state.&lt;/p>
&lt;h3 id="unified-real-time-and-historical-processing">Unified real-time and historical processing&lt;/h3>
&lt;p>A second tenet of Beam&amp;rsquo;s semantic model is that processing must be unified
between batch and streaming. One important use case for this unification
is the ability to apply the same logic to a stream of events in real time and
to archived storage of the same events.&lt;/p>
&lt;p>A common characteristic of archived data is that it may arrive radically out of
order. The sharding of archived files often results in a totally different
ordering for processing than events coming in near-real-time. The data will
also all be all available and hence delivered instantaneously from the point of
view of your pipeline. Whether running experiments on past data or reprocessing
past results to fix a data processing bug, it is critically important that your
processing logic be applicable to archived events just as easily as incoming
near-real-time data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/timely-processing/UnifiedModel.png"
alt="Unified stateful processing over streams and file archives"
width="600">&lt;/p>
&lt;p>It is (deliberately) possible to write a stateful and timely DoFn that delivers
results that depend on ordering or delivery timing, so in this sense there is
additional burden on you, the &lt;code>DoFn&lt;/code> author, to ensure that this nondeterminism
falls within documented allowances.&lt;/p>
&lt;h2 id="go-use-it">Go use it!&lt;/h2>
&lt;p>I&amp;rsquo;ll end this post in the same way I ended the last. I hope you will go try out
Beam with stateful, timely processing. If it opens up new possibilities for
you, then great! If not, we want to hear about it. Since this is a new feature,
please check the &lt;a href="/documentation/runners/capability-matrix/">capability matrix&lt;/a> to see the level of support for
your preferred Beam backend(s).&lt;/p>
&lt;p>And please do join the Beam community at
&lt;a href="/get-started/support">user@beam.apache.org&lt;/a> and follow
&lt;a href="https://twitter.com/ApacheBeam">@ApacheBeam&lt;/a> on Twitter.&lt;/p></description></item></channel></rss>