<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Beam – Case Studies</title><link>/case-studies/</link><description>Recent content in Case Studies on Apache Beam</description><generator>Hugo -- gohugo.io</generator><atom:link href="/case-studies/index.xml" rel="self" type="application/rss+xml"/><item><title>Case-Studies: Apache Beam Amplified Ricardo’s Real-time and ML Data Processing for eCommerce Platform</title><link>/case-studies/ricardo/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/ricardo/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div class="case-study-opinion">
&lt;div class="case-study-opinion-img">
&lt;img src="/images/logos/powered-by/ricardo.png"/>
&lt;/div>
&lt;blockquote class="case-study-quote-block">
&lt;p class="case-study-quote-text">
“Without Beam, without all this data and real time information, we could not deliver the services we are providing and handle the volumes of data we are processing.”
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;/div>
&lt;div class="case-study-post">
&lt;h1 id="apache-beam-amplified-ricardos-real-time-and-ml-data-processing-for-ecommerce-platform">Apache Beam Amplified Ricardo’s Real-time and ML Data Processing for eCommerce Platform.&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://www.ricardo.ch/">Ricardo&lt;/a> is a leading second hand marketplace in Switzerland. The site supports over 4 million
registered buyers and sellers, processing more than 6.5 million article transactions via the platform annually. Ricardo
needs to process high volumes of streaming events and manage over 5 TB of articles, assets, and analytical data.&lt;/p>
&lt;p>With the scale that came from 20 years in the market, Ricardo made the decision to migrate from their on-premises data
center to cloud to easily grow and evolve further and reduce operational costs through managed cloud services. Data
intelligence and engineering teams took the lead on this transformation and development of new AI/ML-enabled customer
experiences. Apache Beam has been a technology amplifier that expedited Ricardo’s transformation.&lt;/p>
&lt;h2 id="challenge">Challenge&lt;/h2>
&lt;p>Migrating from an on-premises data center to the cloud presented Ricardo with an opportunity to modernize their
marketplace from heavy legacy reliance on transactional SQL, switch to BigQuery for analytics, and take advantage of the
event-based streaming architecture.&lt;/p>
&lt;p>Ricardo’s data intelligence team identified two key success factors: a carefully designed data model and a framework
that provides unified stream and batch data pipelines execution, both on-premises and in the cloud.&lt;/p>
&lt;p>Ricardo needed a data processing framework that can scale easily, enrich event streams with historic data from multiple
sources, provide granular control on data freshness, and provide an abstract pipeline operational infrastructure, thus
helping their team focus on creating new value for customers and business&lt;/p>
&lt;h2 id="journey-to-beam">Journey to Beam&lt;/h2>
&lt;p>Ricardo’s data intelligence team began modernizing their stack in 2018. They selected frameworks that provide reliable
and scalable data processing both on-premises and in the cloud. Apache Beam enables users to create pipelines in their
favorite programming language offering SDKs in Java, Python, Go, SQL, Scala (SCIO).
A &lt;a href="https://beam.apache.org/documentation/#available-runners">Beam Runner&lt;/a> runs a Beam pipeline on a specific (often
distributed) data processing system. Ricardo selected the Apache Beam Flink runner for executing pipelines on-premises
and the Dataflow runner as a managed cloud service for the same pipelines developed using Apache Beam Java SDK. Apache
Flink is well known for its reliability and cost-efficiency and an on-premises cluster was spun up at Ricardo’s
datacenter as the initial environment.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
We wanted to implement a solution that would multiply our possibilities, and that’s exactly where Beam comes in. One of the major drivers in this decision was the ability to evolve without adding too much operational load.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>Beam pipelines for core business workloads to ingest events data from Apache Kafka into BigQuery were running stable in
just one month. As Ricardo’s cloud migration progressed, the data intelligence
team &lt;a href="https://www.youtube.com/watch?v=EcvnFH5LDE4">migrated Flink cluster from Kubernetes&lt;/a> in their on-premises
datacenter to GKE.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
I knew Beam, I knew it works. When you need to move from Kafka to BigQuery and you know that Beam is exactly the right tool, you just need to choose the right executor for it.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>The flexibility to refresh data every hour, minute, or stream data real-time, depending on the specific use case and
need, helped the team improve data freshness which was a significant advancement for Ricardo’s eCommerce platform
analytics and reporting.&lt;/p>
&lt;p>Ricardo’s team found benefits in Apache Beam Flink runner on self-managed Flink cluster in GKE for streaming pipelines.
Full control over Flink provisioning enabled to set up required connectivity from Flink cluster to an external peered
Kafka managed service. The data intelligence team optimized operating costs through cluster resource utilization
significantly. For batch pipelines, the team chose Dataflow managed service for its on-demand autoscaling and cost
reduction features like FlexRS, especially efficient for training ML models over TBs of historic data. This hybrid
approach has been serving Ricardo’s needs well and proved to be a reliable production solution.&lt;/p>
&lt;h2 id="evolution-of-use-cases">Evolution of Use Cases&lt;/h2>
&lt;p>Thinking of a stream as data in motion, and a table as data at rest provided a fortuitous chance to take a look at some
data model decisions that were made as far back as 20 years before. Articles that are on the marketplace have assets
that describe them, and for performance and cost optimizations purposes, data entities that belong together were split
into separate database instances. Apache Beam enabled Ricardo’s data intelligence team
to &lt;a href="https://youtu.be/PiwLC-YK_Zw">join assets and articles streams&lt;/a> and optimize BigQuery scans to reduce costs. When
designing the pipeline, the team created streams for assets and articles. Since the assets stream is the primary one,
they shifted the stream 5 minutes back and created a lookup schema with it in BigTable. This elegant solution ensures
that the assets stream is always processed first while BigTable allows for matching the latest asset to an article and
Apache Beam joins them both together.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/post_scheme.png">
&lt;/div>
&lt;p>The successful case of joining different data streams facilitated further Apache Beam adoption by Ricardo in areas like
data science and ML.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
Once you start laying out the simple use cases, you will always figure out the edge case scenarios. This pipeline has been running for a year now, and Beam handles it all, from super simple use cases to something crazy.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>As an eCommerce retailer, Ricardo faces the increasing scale and sophistication of fraud transactions and takes a
strategic approach by employing Beam pipelines for fraud detection and prevention. Beam pipelines act on an external
intelligent API to identify the signs of fraudulent behaviour, like device characteristics or user activity. Apache Beam
&lt;a href="https://beam.apache.org/documentation/programming-guide/#state-and-timers">stateful processing&lt;/a> feature enables Ricardo
to apply an associating operation to the streams of data (trigger banishing a user for example). Thus, Apache Beam saves
Ricardo’s customer care team&amp;rsquo;s time and effort on investigating duplicate cases. It also runs batch pipelines
to &lt;a href="https://www.youtube.com/watch?v=LXnh9jNNfYY">find linked accounts&lt;/a>, associate products to categories by
encapsulating a ML model, or calculates the likelihood something is going to sell, at a scale or precision that was
previously not possible.&lt;/p>
&lt;p>Originally implemented by Ricardo’s data intelligence team, Apache Beam has proven to be a powerful framework that
supports advanced scenarios and acts as a glue between Kafka, BigQuery, and platform and external APIs, which encouraged
other teams at Ricardo to adopt it.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
[Apache Beam] is a framework that is so good that other teams are picking up the idea and starting to work with it after we tested it.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Apache Beam has provided Ricardo with a scalable and reliable data processing framework that supported Ricardo’s
fundamental business scenarios and enabled new use cases to respond to events in real-time.&lt;/p>
&lt;p>Throughout Ricardo’s transformation, Apache Beam has been a unified framework that can run batch and stream pipelines,
offers on-premises and cloud managed services execution, and programming language options like Java and Python,
empowered data science and research teams to advance customer experience with new real-time scenarios fast-tracking time
to value.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
After this first pipeline, we are working on other use cases and planning to move them to Beam. I was always trying to spread the idea that this a framework that is reliable, it actually helps you to get the stuff done in a consistent way.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>Apache Beam has been a technology that multiplied possibilities, allowing Ricardo to maximize technology benefits at all
stages of their modernization and cloud journey.&lt;/p>
&lt;div class="case-study-feedback" id="case-study-feedback">
&lt;p class="case-study-feedback-title">Was this information useful?&lt;/p>
&lt;div>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(true, 'Ricardo')">Yes&lt;/button>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(false, 'Ricardo')">No&lt;/button>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="clear-nav">&lt;/div></description></item><item><title>Case-Studies: Beam and Geocoding</title><link>/case-studies/goga/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/goga/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">Beam and Geocoding&lt;/h2>
&lt;/header>
&lt;p>GOGA Data Analysis and Consulting is a company based in Japan that specializes in analytics of geospatial and mapping data. They use Apache Beam and Cloud Dataflow for a smooth data transformation process for analytical purposes. This use case focuses on handling multiple extractions, geocoding, and insertion process by wrangling and requesting API call of each data based on the location provided.&lt;/p>
&lt;/div></description></item><item><title>Case-Studies: Beam visual pipeline development with Hop</title><link>/case-studies/hop/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/hop/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div class="case-study-opinion">
&lt;div class="case-study-opinion-img">
&lt;img src="/images/logos/powered-by/hop.png"/>
&lt;/div>
&lt;blockquote class="case-study-quote-block">
&lt;p class="case-study-quote-text">
“Apache Beam and its abstraction of the execution engines is a big thing for us. The amount of work that that saves...it would be hard to build that support for Dataflow or Spark all by yourself. It is amazing that this technology exists in the first place, really amazing! Not having to worry about all those underlying platforms - that is tremendous!”
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/matt_casters_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Matt Casters
&lt;/div>
&lt;div class="case-study-quote-author-position">
Chief Solutions Architect, Neo4j, Apache Hop co-founder
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;/div>
&lt;div class="case-study-post">
&lt;h1 id="visual-apache-beam-pipeline-design-and-orchestration-with-apache-hop">Visual Apache Beam Pipeline Design and Orchestration with Apache Hop&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://hop.apache.org/">Apache Hop&lt;/a> is an open source data orchestration and data engineering
platform that aims to facilitate all aspects of data processing with visual pipeline development
environment. This easy-to-use, fast, and flexible platform enables developers to create and manage
Apache Beam batch and streaming pipelines in Hop GUI. Apache Hop uses metadata and kernel to
describe how the data should be processed, and Apache Beam to “design once, run anywhere”.&lt;/p>
&lt;p>&lt;a href="https://neo4j.com/">Neo4j’s&lt;/a> Chief Solutions
Architect, &lt;a href="https://be.linkedin.com/in/mattcasters">Matt Casters&lt;/a>, has been an early adopter of
Apache Beam and its abstraction of execution engines. Matt has been an active member of the Apache
open-source community for years and has leveraged Apache Beam as an execution engine to build Apache
Hop.&lt;/p>
&lt;h2 id="apache-hop-project">Apache Hop Project&lt;/h2>
&lt;p>Thriving popularity and the growing number of Apache Beam users across the globe inspired Matt
Casters to expand the idea of abstraction to visual pipeline lifecycle management and development.
Matt co-founded and incubated the
&lt;a href="https://hop.apache.org/">Apache Hop&lt;/a> project that became a top level project at
the &lt;a href="https://www.apache.org/">Apache Software Foundation&lt;/a>
in December 2021. The platform enables users of all skill levels to build, test, launch, and deploy
powerful data workflows without writing code. Apache Hop’s intuitive drag and drop interface
provides a visual representation of Apache Beam pipelines, simplifying pipeline design, execution,
preview, monitoring, and debugging.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
I was a big fan of Beam from the get go. Apache Beam is now a very important part of the Apache Hop project.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/matt_casters_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Matt Casters
&lt;/div>
&lt;div class="case-study-quote-author-position">
Chief Solutions Architect, Neo4j,
&lt;br>Apache Hop co-founder
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>The Apache Hop GUI allows data professionals to work visually and focus on “what” they need to do
rather than “how”, using metadata to describe how the Apache Beam pipelines should be processed.
Apache
Hop’s &lt;a href="https://hop.apache.org/manual/latest/pipeline/create-pipeline.html#_concepts">transform-agnostic&lt;/a>
action
plugins (&lt;a href="https://hop.apache.org/manual/latest/pipeline/create-pipeline.html#_concepts">“hops”&lt;/a>)
link transforms together, creating a pipeline. Various Apache Beam runners, such as
&lt;a href="https://hop.apache.org/manual/latest/pipeline/pipeline-run-configurations/beam-spark-pipeline-engine.html">Spark&lt;/a>
,
&lt;a href="https://hop.apache.org/manual/latest/pipeline/pipeline-run-configurations/beam-flink-pipeline-engine.html">Flink&lt;/a>
,
&lt;a href="https://hop.apache.org/manual/latest/pipeline/pipeline-run-configurations/beam-dataflow-pipeline-engine.html">Dataflow&lt;/a>
, and
the &lt;a href="https://hop.apache.org/manual/latest/pipeline/pipeline-run-configurations/beam-direct-pipeline-engine.html">Direct&lt;/a>
runner, read the metadata with help of Apache
Hop&amp;rsquo;s &lt;a href="https://hop.apache.org/dev-manual/latest/sdk/hop-sdk.html#_hop_metadata_providers">Metadata Provider&lt;/a>
and &lt;a href="https://hop.apache.org/dev-manual/latest/sdk/hop-sdk.html#_workflow_execution">workflow engines(plugins)&lt;/a>
, and execute the pipeline.&lt;/p>
&lt;p>Apache Hop’s custom plugins and metadata objects
for &lt;a href="https://hop.apache.org/manual/latest/technology/technology.html">some of the most popular technologies&lt;/a>
, such as &lt;a href="https://neo4j.com/">Neo4j&lt;/a>, empower users to execute database- and technology-specific
transforms inside the Apache Beam pipelines, which allows for native optimized connectivity and
flexible Apache Beam pipeline configurations. For instance, the Apache
Hop’s &lt;a href="https://hop.apache.org/manual/latest/technology/neo4j/index.html#_description">Neo4j plugin&lt;/a>
stores logging and execution lineage of Apache Beam pipelines in the Neo4j graph database and
enables users to query this information for more details, such as quickly jump to the place where an
error occurred. The combination of Apache Hop
transforms, &lt;a href="https://beam.apache.org/documentation/io/built-in/">Apache Beam built-in I/Os&lt;/a>, and
Apache Beam-powered data processing opens up new horizons for more sinks and sources and custom use
cases.&lt;/p>
&lt;p>Apache Hop aims to bring a no-code approach to Apache Beam data pipelines. Sometimes the choice of a
particular programming language, framework, or engine is driven by developers&amp;rsquo; preferences, which
results in businesses becoming tied to a specific technology skill set and stack. Apache Hop
eliminates this dependency by abstracting out the I/Os with a fully pluggable runtime support and
providing a graphic user interface on top of Apache Beam pipelines. All settings for pipeline
elements are performed in the Hop’s visual editor just once, and pipeline is automatically described
as metadata in JSON and CSV formats. Programming data pipelines’ source code becomes an option, not
a necessity. Apache Hop does not require knowledge of a particular programming language to create
pipelines, helping with the adoption of Apache Beam unified streaming and batch processing
technology.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
In general, a visual pipeline design interface is really valuable for a non-developer audience…
We categorically choose the side of the organization when it comes to lowering setup costs,
maintenance costs, increasing ROI, and safeguarding an investment over time.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/matt_casters_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Matt Casters
&lt;/div>
&lt;div class="case-study-quote-author-position">
Chief Solutions Architect, Neo4j,
&lt;br>Apache Hop co-founder
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Apache Beam continuously expands the number of use cases and scenarios it supports and makes it
possible to bring advanced technology solutions into a reality. Being an early adopter of Apache
Beam and its powerful abstraction, Matt Casters leveraged this knowledge and experience to create
Apache Hop. The platform creates a value-add for Apache Beam users by enabling visual pipeline
development and lifecycle management.&lt;/p>
&lt;p>Matt sees Apache Beam as a foundation and a driving force behind Apache Hop. Communication between
Apache Beam and Apache Hop projects keeps fostering co-creation and enriches both products with new
features.&lt;/p>
&lt;p>Apache Hop project is the example of the continuous improvement driven by the Apache open source
community and amplified by collaborative organizations.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
Knowledge sharing and collaboration is something that comes naturally in the community. If we
see some room for improvement, we exchange ideas and this way, we keep driving Apache Beam and
Apache Hop projects forward. Together, we can work with the most complex problems and just solve them.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/matt_casters_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Matt Casters
&lt;/div>
&lt;div class="case-study-quote-author-position">
Chief Solutions Architect, Neo4j,
&lt;br>Apache Hop co-founder
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;div class="case-study-feedback" id="case-study-feedback">
&lt;p class="case-study-feedback-title">Was this information useful?&lt;/p>
&lt;div>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(true, 'Hop')">Yes&lt;/button>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(false, 'Hop')">No&lt;/button>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="clear-nav">&lt;/div></description></item><item><title>Case-Studies: Cloud Dataflow</title><link>/case-studies/dataflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/dataflow/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">Cloud Dataflow&lt;/h2>
&lt;/header>
&lt;p>&lt;strong>&lt;a href="https://cloud.google.com/dataflow">Cloud Dataflow&lt;/a>:&lt;/strong> Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud Platform ecosystem.&lt;/p>
&lt;/div></description></item><item><title>Case-Studies: Feature Powered by Apache Beam - Beyond Lambda</title><link>/case-studies/ebay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/ebay/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">Feature Powered by Apache Beam - Beyond Lambda&lt;/h2>
&lt;/header>
&lt;p>eBay is an American e-commerce company that provides business-to-consumer and consumer-to-consumer sales through the online website. They build feature pipelines with Apache Beam: unify feature extraction and selection in online and offline, speed up E2E iteration for model training, evaluation and serving, support different types (streaming, runtime, batch) of features, etc. eBay leverages Apache Beam for the streaming feature SDK as a foundation to integrate with Kafka, Hadoop, Flink, Airflow and others in eBay.&lt;/p>
&lt;/div></description></item><item><title>Case-Studies: From Apache Beam to Leukemia early detection</title><link>/case-studies/oriel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/oriel/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">From Apache Beam to Leukemia early detection&lt;/h2>
&lt;/header>
&lt;p>Oriel Research Therapeutics (ORT) is a startup company in the greater Boston area that provides early detection services for
multiple medical conditions, utilizing cutting edge Artificial Intelligence technologies and Next Generation Sequencing (NGS). ORT utilizes Apache Beam pipelines to process over 1 million samples of genomics and clinical information. The processed data is used by ORT in detecting Leukemia, Sepsis and other medical conditions.&lt;/p>
&lt;/div></description></item><item><title>Case-Studies: Kio</title><link>/case-studies/kio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/kio/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Kio is a set of Kotlin extensions for Apache Beam to implement fluent-like API for Java SDK.&lt;/p>
&lt;h2 id="word-count-example">Word Count example&lt;/h2>
&lt;pre>&lt;code>// Create Kio context
val kio = Kio.fromArguments(args)
// Configure a pipeline
kio.read().text(&amp;quot;~/input.txt&amp;quot;)
.map { it.toLowerCase() }
.flatMap { it.split(&amp;quot;\\W+&amp;quot;.toRegex()) }
.filter { it.isNotEmpty() }
.countByValue()
.forEach { println(it) }
// And execute it
kio.execute().waitUntilDone()
&lt;/code>&lt;/pre>&lt;h2 id="documentation">Documentation&lt;/h2>
&lt;p>For more information about Kio, please see the documentation here: &lt;a href="https://code.chermenin.ru/kio">https://code.chermenin.ru/kio&lt;/a>.&lt;/p></description></item><item><title>Case-Studies: Klio</title><link>/case-studies/klio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/klio/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Developed at Spotify and built on top of Apache Beam for Python, Klio is an open source framework that lets researchers and engineers build smarter data pipelines for processing audio and other media files, easily and at scale.&lt;/p></description></item><item><title>Case-Studies: Scalability and Cost Optimization for Search Engine's Workloads</title><link>/case-studies/seznam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/seznam/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div class="case-study-opinion">
&lt;div class="case-study-opinion-img">
&lt;img src="/images/logos/powered-by/seznam.png"/>
&lt;/div>
&lt;blockquote class="case-study-quote-block">
&lt;p class="case-study-quote-text">
“Apache Beam is a well-defined data processing model that lets you concentrate on business logic rather than low-level details of distributed processing.”
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/marek_simunek_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Marek Simunek
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Software Engineer @ seznam.cz
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;/div>
&lt;div class="case-study-post">
&lt;h1 id="scalability-and-cost-optimization-for-search-engines-workloads">Scalability and Cost Optimization for Search Engine&amp;rsquo;s Workloads&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://www.seznam.cz/">Seznam.cz&lt;/a> is a Czech search engine that serves over 25% of local organic search traffic.
Seznam employs over 1,500 people and runs a portfolio of more than 30 web services and associated brands,
processing around &lt;a href="https://www.searchenginejournal.com/seznam-interview/302851/#close">15 million queries a day&lt;/a>.&lt;/p>
&lt;p>Seznam continuously optimizes their big data infrastructure, web crawlers, algorithms,
and ML models on a mission to achieve excellence in accuracy, quality, and usefulness of search results for their users.
Seznam has been an early contributor and adopter of Apache Beam, and they migrated several petabyte-scale workloads
to Apache Beam pipelines running in Apache Spark and Apache Flink clusters in Seznam’s on-premises data center.&lt;/p>
&lt;h2 id="journey-to-apache-beam">Journey to Apache Beam&lt;/h2>
&lt;p>Seznam started using MapReduce in a Hadoop Yarn cluster back in 2010 to facilitate concurrent batch jobs processing
for the web crawler components of their search engine.
Within several years, their data infrastructure evolved to &lt;a href="https://www.youtube.com/watch?v=rJIpva0tD0g">over 40 billion rows with 400 terabytes&lt;/a>
in HBase, 2 on-premises data centers with over 1,100 bare metal servers, 13 PB storage, and 50 TB memory, which made their business logic more complex.
MapReduce no longer provided enough flexibility, &lt;a href="https://youtu.be/rJIpva0tD0g?t=130">cost efficiency, and performance&lt;/a>
to support this growth, and Seznam rewrote the jobs to native Spark.
Spark &lt;a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations">shuffle operations&lt;/a>
enabled Seznam to split large data keys into partitions, load them in-memory one by one, and process them iteratively.
However, exponential data skews and inability to fit all values for a single key into an in-memory buffer resulted in
&lt;a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#performance-impact">increased disk space utilization and memory overhead&lt;/a>.
Some tasks took unexpectedly long time to complete, and it was challenging
to debug Spark pipelines due to generic exceptions. Thus, Seznam needed a data processing framework that can scale more efficiently.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
To manage this kind of scale, you need the abstraction.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/marek_simunek_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Marek Simunek
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Software Engineer @ seznam.cz
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>In 2014, Seznam started work on Euphoria API - a proprietary programming model that can express business logic
in batch and streaming pipelines and allow for runner independent implementation.&lt;/p>
&lt;p>Apache Beam was released in 2016 and became a readily available and well-defined unified programming model.
This engine-independent model has been evolving very fast, supports multiple shuffle operators and fits perfectly
into Seznam’s existing on-premises data infrastructure. For a while, Seznam continued to develop Euphoria,
but soon the high cost and the amount of effort needed to maintain the solution and create their own
runners in-house surpassed the benefits of having a proprietary framework.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/seznam_scheme_1.png">
&lt;/div>
&lt;p>Seznam started migrating their key workloads to Apache Beam.
They decided to merge the &lt;a href="https://beam.apache.org/documentation/sdks/java/euphoria/">Euphoria API&lt;/a>
as a high-level DSL for Apache Beam Java SDK.
This significant contribution to Apache Beam was a starting point for Seznam’s active participation in the community,
later presenting their unique experience and findings at &lt;a href="https://www.youtube.com/watch?v=ZIFtmx8nBow">Beam Summit Europe 2019&lt;/a>
and developer conferences.&lt;/p>
&lt;h2 id="adopting-apache-beam">Adopting Apache Beam&lt;/h2>
&lt;p>Apache Beam enabled Seznam to execute batch and stream jobs much faster without increasing memory and disk space,
thus maximizing scalability, performance, and efficiency.&lt;/p>
&lt;p>Apache Beam offers a variety of ways to distribute skewed data evenly.
&lt;a href="https://beam.apache.org/documentation/programming-guide/#windowing">Windowing&lt;/a>
for processing unbounded and &lt;a href="https://beam.apache.org/documentation/transforms/java/elementwise/partition/">Partition&lt;/a>
for bounded data sets transform input into finite
collections of elements that can be reshuffled. Apache Beam provides a byte-based shuffle that can be
executed by Spark runner or Flink runner, without requiring Apache Spark or Apache Flink to deserialize the full key.
Apache Beam SDKs provide effective coders to serialize and deserialize elements and pass to distributed workers.
Using Apache Beam serialization and byte-based shuffle resulted in substantial performance gains for many of the
Seznam’s use cases and reduced memory required for the shuffling by Apache Spark execution environment.
Seznam’s infrastructure costs associated with &lt;a href="https://youtu.be/rJIpva0tD0g?t=522">disk I/O and memory splits&lt;/a>
decreased significantly.&lt;/p>
&lt;p>One of the most valuable use cases is Seznam’s LinkRevert job, which analyzes the web graph to improve search relevance.
This data pipeline figuratively “turns the Internet upside down”, processing over 150 TB daily,
extending redirect chains to identify every successor of a specific URL, and discovering backlinks that point to a specific web page.
The Apache Beam pipeline executes multiple large-scale skewed joins, and scores the URLs for search results based on the redirect and backlinking factors.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/seznam_scheme_2.png">
&lt;/div>
&lt;p>Apache Beam allows for a unified engine-independent execution, so Seznam was able to select between
Spark or Flink runner depending on the use case. For example, the Apache Beam batch pipeline executed by
Spark runner on a Hadoop Yarn cluster parses new web documents, enriches data with additional features,
and scores the web pages based on their relevance, ensuring timely database updates and accurate search results.
Apache Beam stream processing runs in the Apache Flink execution environment on a Kubernetes cluster for thumbnail
requests that are displayed in users’ search results. Another example of stream event processing is the Apache Beam Flink
runner pipeline that maps, joins, and processes search logs to calculate SLO metrics and other features.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/seznam_scheme_3.png">
&lt;/div>
&lt;div class="post-scheme">
&lt;img src="/images/seznam_scheme_4.png">
&lt;/div>
&lt;p>Over the years, Seznam’s approach has evolved. They have realized the tremendous benefits of Apache Beam
for balancing petabyte-size workloads and optimizing memory and compute resources in on-premises data centers.
Apache Beam is Seznam’s go-to platform for batch and stream pipelines that require multiple shuffle operations,
processing skewed data, and implementing complex business logic. Apache Beam unified model with sources
and sinks exposed as transforms, increased business logic maintainability and traceability with unit tests.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
One of the biggest benefits is Apache Beam sinks and sources. By exposing your source or sink as a transform, your implementation is hidden and later on, you can add additional functionality without breaking the existing implementation for users.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/marek_simunek_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Marek Simunek
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Software Engineer @ seznam.cz
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;h2 id="monitoring-and-debugging">Monitoring and Debugging&lt;/h2>
&lt;p>Apache Beam pipelines monitoring and debugging was critical for cases with complex business logic and
multiple data transformations. Seznam engineers identified optimal tools depending on the execution engine.
Seznam leveraged &lt;a href="https://github.com/criteo/babar">Babar from Criteo&lt;/a> to profile Apache Beam pipelines
on Spark runner and identify the root causes
of downtimes in their performance. Babar allows for easier monitoring, debugging, and performance optimization
by analyzing cluster resource utilization, memory allocated, CPU used, etc. For Apache Beam pipelines executed by Flink runner
on Kubernetes cluster, Seznam employs Elasticsearch to store, search, and analyze metrics.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Apache Beam offered a unified model for Seznam’s stream and batch processing that provided performance at scale.
Apache Beam supported multiple runners, language SDKs, and built-in and custom pluggable I/O transforms,
thus eliminating the need to invest into the development and support of proprietary runners and solutions.
After evaluation, Seznam transitioned their workloads to Apache Beam and integrated
&lt;a href="https://beam.apache.org/documentation/sdks/java/euphoria/">Euphoria API&lt;/a>
(a fast prototyping framework developed by Seznam), contributing to the Apache Beam open source community.&lt;/p>
&lt;p>The Apache Beam abstraction and execution model allowed Seznam to robustly scale their data processing.
It also provided the flexibility to write the business logic just once and keep freedom of choice between runners.
The model was especially valuable for pipeline maintainability in complex use cases.
Apache Beam helped overcome memory and compute resource constraints by reshuffling unevenly distributed data into manageable partitions.
&lt;div class="case-study-feedback" id="case-study-feedback">
&lt;p class="case-study-feedback-title">Was this information useful?&lt;/p>
&lt;div>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(true, 'Seznam')">Yes&lt;/button>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(false, 'Seznam')">No&lt;/button>
&lt;/div>
&lt;/div>
&lt;/p>
&lt;/div>
&lt;div class="clear-nav">&lt;/div></description></item><item><title>Case-Studies: Scio</title><link>/case-studies/scio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/scio/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Scio is a Scala API for Apache Beam and Google Cloud Dataflow inspired by Apache Spark and Scalding.&lt;/p></description></item><item><title>Case-Studies: TensorFlow Extended (TFX)</title><link>/case-studies/tfx/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/tfx/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>&lt;strong>&lt;a href="https://www.tensorflow.org/tfx">TensorFlow Extended (TFX)&lt;/a>:&lt;/strong> TensorFlow Extended (TFX) is an end-to-end platform
for deploying production ML pipelines based on Apache Beam.&lt;/p></description></item><item><title>Case-Studies: The Nitty-Gritty of Moving Data with Beam</title><link>/case-studies/mozilla/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/mozilla/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">The Nitty-Gritty of Moving Data with Beam&lt;/h2>
&lt;/header>
&lt;p>Mozilla is the non-profit Firefox browser. This use case focuses on complexity that comes from moving data from one system to another safely, modeling data as it passes from one transform to another, handling errors, testing the system, and organizing the code to make the pipeline configurable for different source and destination systems in their open source codebase for ingesting telemetry data from Firefox clients.&lt;/p>
&lt;/div></description></item></channel></rss>