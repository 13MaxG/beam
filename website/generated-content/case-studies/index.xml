<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Beam – Case Studies</title><link>/case-studies/</link><description>Recent content in Case Studies on Apache Beam</description><generator>Hugo -- gohugo.io</generator><atom:link href="/case-studies/index.xml" rel="self" type="application/rss+xml"/><item><title>Case-Studies: Real-time ML with Beam at Lyft</title><link>/case-studies/lyft/</link><pubDate>Fri, 17 Jun 2022 00:12:00 +0000</pubDate><guid>/case-studies/lyft/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div class="case-study-opinion">
&lt;div class="case-study-opinion-img">
&lt;img class="case-study-opinion-img-cropped" src="/images/logos/powered-by/lyft.png"/>
&lt;/div>
&lt;blockquote class="case-study-quote-block">
&lt;p class="case-study-quote-text">
“Lyft Marketplace team aims to improve our business efficiency by being nimble to real-world dynamics. Apache Beam has enabled us to meet the goal of having a robust and scalable ML infrastructure for improving model accuracy with features in real-time. These real-time features support critical functions like Forecasting, Primetime, Dispatch.”
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/case-study/lyft/ravi_kiran_magham.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Ravi Kiran Magham
&lt;/div>
&lt;div class="case-study-quote-author-position">
Software Engineer @ Lyft
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;/div>
&lt;div class="case-study-post">
&lt;h1 id="real-time-ml-with-beam-at-lyft">Real-time ML with Beam at Lyft&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://www.lyft.com/">Lyft, Inc.&lt;/a> is an American mobility-as-a-service provider that offers ride-hailing, car and motorized scooter rentals, bicycle-sharing, food delivery, and business transportation solutions. Lyft is based in San Francisco, California, and &lt;a href="https://www.lyft.com/rider/cities">operates in&lt;/a> 644 cities in the United States and 12 cities in Canada.&lt;/p>
&lt;p>As you might expect from a company as large as Lyft, connecting drivers and riders in space and time at such a scale requires a powerful real-time streaming infrastructure. Ravi Kiran Magham, Software Engineer at Lyft, shared the story of how Apache Beam has become a mission-critical and integral real-time data processing technology for Lyft by enabling large-scale streaming data processing and machine learning pipelines.&lt;/p>
&lt;h2 id="democratizing-stream-processing">Democratizing Stream Processing&lt;/h2>
&lt;p>Lyft originally built streaming ETL pipelines to transform, enrich, and sink events generated by application services to their data lake in &lt;a href="https://aws.amazon.com/s3/">AWS S3 &lt;/a> using &lt;a href="https://aws.amazon.com/kinesis/">Amazon Kinesis&lt;/a> and &lt;a href="https://flink.apache.org/">Apache Flink&lt;/a>. Apache Flink is the foundation of Lyft’s streaming architecture and was chosen over Apache Spark due to its robust, fault-tolerant, and intuitive API for distributed stateful stream processing, exactly-once processing, and variety of I/O connectors.&lt;/p>
&lt;p>Lyft’s popularity and growth were bringing new demands to data streaming infrastructure: more teams with diverse programming language preferences wanted to explore event-driven streaming applications, and build streaming features for real-time machine learning models to make business more efficient, enhance customer experiences, and provide time-sensitive compliance operations. The Data Platform team looked into improving the prime time (surge pricing) computation for the Marketplace team, which had a service orchestrating an ensemble of ML models, exchanging data over &lt;a href="https://redis.com/">Redis&lt;/a>. The teams aimed at reducing code complexity and improving latency (from 5 to &amp;lt; 1 min end to end). With Python being a prerequisite by the Marketplace team and Java being heavily used by the Data Platform team, Lyft started exploring the &lt;a href="/">Apache Beam&lt;/a> &lt;a href="/roadmap/portability/">portability framework&lt;/a> in 2019 to democratize streaming for all teams.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
The Apache Beam portability and multi-language capabilities were the key pique and the primary reason for us to start exploring Beam in a bigger way.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/case-study/lyft/ravi_kiran_magham.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Ravi Kiran Magham
&lt;/div>
&lt;div class="case-study-quote-author-position">
Software Engineer @ Lyft
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>Apache Beam provides a solution to the programming language and data processing engine dilemma, as it offers a variety of &lt;a href="/documentation/basics/#runner">runners&lt;/a> (including the &lt;a href="/documentation/runners/flink/">Beam Flink runner&lt;/a> for Apache Flink) and a &lt;a href="/documentation/sdks/java/">variety of programming language SDKs&lt;/a>. Apache Beam offers an ultimate level of portability with its concept of “write once, run anywhere” and its ability to create &lt;a href="/documentation/programming-guide/#multi-language-pipelines">multi-language pipelines - data pipelines&lt;/a> that use transforms from more than one programming language.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
Leveraging Apache Beam has been a “win-win” decision for us because our data infra teams use Java but we are able to offer Python SDK for our product teams, as it has been the de-facto language that they prefer. We write streaming pipelines with ease and comfort and run them on the Beam Flink runner.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/case-study/lyft/ravi_kiran_magham.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Ravi Kiran Magham
&lt;/div>
&lt;div class="case-study-quote-author-position">
Software Engineer @ Lyft
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>The Data Platform team built a control plane of in-house services and &lt;a href="https://github.com/lyft/flinkk8soperator">FlinkK8sOperator&lt;/a> to manage Flink applications on a Kubernetes cluster and deploy streaming Apache Beam and Apache Flink jobs. Lyft uses a blue/green deployment strategy on critical pipelines to minimize any downtime and uses custom macros for improved observability and seamless integration of the CI/CD deployments. To improve developer productivity, the Data Platform team offers a lightweight, YAML-based DSL to abstract the source and sink configurations, and provides reusable Apache Beam PTransforms for filtering and enrichment of incoming events.&lt;/p>
&lt;h2 id="powering-real-time-machine-learning-pipelines">Powering Real-time Machine Learning Pipelines&lt;/h2>
&lt;p>Lyft Marketplace plays a pivotal role in optimizing fleet demand and supply prediction, dynamic pricing, ETA calculation, and more. The Apache Beam Python SDK and Flink Runner enable the team to be nimble to change and support the demands for real-time ML – streaming feature generation and model execution. The Data Platform team has extended the streaming infrastructure to support Continual Learning use cases. Apache Beam powers continuous training of ML models with real-time data over larger windows of 2 hours to identify and fine-tune biases in cost and ETA.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/case-study/lyft/apache_beam_ml_features_generation.svg" alt="Apache Beam Feature Generation and ML Model Execution">
&lt;span>Apache Beam Feature Generation and ML Model Execution &lt;/span>
&lt;/div>
&lt;p>Lyft separated Feature Generation and ML Model Execution into multiple streaming pipelines. The streaming Apache Beam pipeline generates features in real-time and writes them to a Kafka topic to be consumed by the model execution pipeline. Based on user configuration, the features are replicated and keyed out by model ID to &lt;a href="/blog/stateful-processing/">stateful&lt;/a> ParDo transforms, which leverage &lt;a href="/documentation/programming-guide/#timers">timers&lt;/a> and/or data (feature) availability to invoke ML models. Features are stored in a global window and the &lt;a href="/documentation/programming-guide/#state-and-timers">state&lt;/a> is explicitly cleaned up. The ML models run as part of the Model Serving infrastructure and their output can be an input feature to another ML model. To support this DAG workflow, Apache Beam pipelines write the output to Kafka and feed it to the model execution streaming pipeline for processing, in addition to writing it to Redis.&lt;/p>
&lt;p>The complex real-time Feature Generation involves processing ~4 million events of 1KB per minute with sub-second latency, generating ~100 features on multiple event attributes across space and time granularities (1 and 5 minutes). Apache Beam allowed the Lyft Marketplace team to reduce latency by &lt;a href="https://conferences.oreilly.com/strata/strata-ca-2019/cdn.oreillystatic.com/en/assets/1/event/290/The%20magic%20behind%20your%20Lyft%20ride%20prices_%20A%20case%20study%20on%20machine%20learning%20and%20streaming%20Presentation.pdf">60%&lt;/a>, significantly simplify the code, and onboard many teams and use cases onto streaming.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
The Marketplace team are &lt;a href="https://eng.lyft.com/gotchas-of-stream-processing-data-skewness-cfba58eb45d4">heavy users of Apache Beam&lt;/a> for real-time feature computation and model executions. Processing events in real-time with a sub-second latency allows our ML models to understand marketplace dynamics early and make informed decisions.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/case-study/lyft/ravi_kiran_magham.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Ravi Kiran Magham
&lt;/div>
&lt;div class="case-study-quote-author-position">
Software Engineer @ Lyft
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;h2 id="amplifying-use-cases">Amplifying Use Cases&lt;/h2>
&lt;p>Lyft has leveraged Apache Beam for more than 60 use cases and enabled them to complete critical business commitments and improve real-time user experiences.&lt;/p>
&lt;p>For example, Lyft&amp;rsquo;s Map Data Delivery team moved from a batch process to a streaming pipeline for identifying road closures in real-time. Their Routing Engine uses this information to determine the best routes, improve ETA and provide a better driver and customer experience. The job processes ~400k events per second, conflates streams of data coming from 3rd party road closures and real-time traffic data to determine actual closures and publish them as events to Kafka. A custom S3 PTransform allows for the job to regularly publish a snapshot of closures for downstream batch processing.&lt;/p>
&lt;p>Apache Beam enabled Lyft to optimize a very specific use case that relates to reporting pick-ups and drop-offs at airports. Airports require mobility applications to report every pick-up and drop-off and match them with the time of fleet entry and exit. Failing to do so results in a lower compliance score and even risk of being penalized. Originally, Lyft had a complicated implementation using the &lt;a href="https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-implementation-app-py.html">KCL library&lt;/a> to consume events and store them in Redis. Python worker processes ran at regular intervals to consume data from Redis, join and enrich the data with service API calls, and send the output to airport applications. With that implementation, late-arriving updates and out-of-order events significantly impacted the completeness score. Lyft migrated the use case to a streaming Apache Beam pipeline with state and timers to keep events in a global window and manage sessions. Apache Beam helped Lyft achieve a top compliance score by improving the latency of event reporting from 5 to 2 seconds and reducing missing entry/exit data to 1.3%.&lt;/p>
&lt;p>Like many companies shaking up standard business models, Lyft relies on open-source software and likes to give back to the community. Many of the big data frameworks, tools, and implementations developed by Lyft are open-sourced on their &lt;a href="https://github.com/orgs/lyft/repositories">GitHub&lt;/a>. Lyft has been an ample Apache Beam contributor since 2018, and Lyft engineers have presented their Apache Beam integrations at various events, such as &lt;a href="https://www.youtube.com/watch?v=D_NA-LY1xP0">Beam Summit North America&lt;/a>, &lt;a href="https://2019.berlinbuzzwords.de/sites/2019.berlinbuzzwords.de/files/media/documents/streaming_at_lyft_-_berlin_buzzwords_2019.pdf">Berlin Buzzwords&lt;/a>, &lt;a href="https://conferences.oreilly.com/strata/strata-ca-2019/cdn.oreillystatic.com/en/assets/1/event/290/The%20magic%20behind%20your%20Lyft%20ride%20prices_%20A%20case%20study%20on%20machine%20learning%20and%20streaming%20Presentation.pdf">O’Reilly Strata Data &amp;amp; AI&lt;/a>, and more.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>The portability of the Apache Beam model is the key to distributed execution. It enabled Lyft to run mission-critical data pipelines written in a non-JVM language on a JVM-based runner. Thus, they avoided code rewrites and sidestepped the potential cost of many API styles and runtime environments, reducing pipeline development time from multiple days to just hours. Full isolation of user code and native CPython execution without library restrictions resulted in easy onboarding and adoption. Apache Beam’s multi-language and cross-language capabilities solved Lyft’s programming language dilemma. With the unified programming model, Lyft is no longer tied to a specific technology stack.&lt;/p>
&lt;p>Apache Beam enabled Lyft to switch from batch ML model training to real-time ML training with granular control of data freshness using windowing. Their data engineering and product teams can use both Python and Java, based on the appropriateness for a particular task or their preference. Apache Beam has helped Lyft successfully build and scale 60+ streaming pipelines processing events at very low latencies in near-real-time. New use cases keep coming, and Lyft is planning on leveraging &lt;a href="/documentation/dsls/sql/overview/">Beam SQL&lt;/a> and the &lt;a href="/documentation/sdks/go/">Go SDK&lt;/a> to provide a full range of Apache Beam multi-language capabilities for their teams.&lt;/p>
&lt;div class="case-study-feedback" id="case-study-feedback">
&lt;p class="case-study-feedback-title">Was this information useful?&lt;/p>
&lt;div>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(true, 'Lyft')">Yes&lt;/button>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(false, 'Lyft')">No&lt;/button>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="clear-nav">&lt;/div></description></item><item><title>Case-Studies: Real-time Event Stream Processing at Scale for Palo Alto Networks</title><link>/case-studies/paloalto/</link><pubDate>Tue, 22 Feb 2022 20:19:00 +0000</pubDate><guid>/case-studies/paloalto/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div class="case-study-opinion">
&lt;div class="case-study-opinion-img">
&lt;img src="/images/logos/powered-by/paloalto.png"/>
&lt;/div>
&lt;blockquote class="case-study-quote-block">
&lt;p class="case-study-quote-text">
“I know one thing: Beam is very powerful and the abstraction is its most significant feature. With the right abstraction we have the flexibility to run workloads where needed. Thanks to Beam, we are not locked to any vendor, and we don’t need to change anything else if we make the switch.”
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/case-study/paloalto/talat_uyarer.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Talat Uyarer
&lt;/div>
&lt;div class="case-study-quote-author-position">
Sr Principal Software Engineer
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;/div>
&lt;div class="case-study-post">
&lt;h1 id="real-time-event-stream-processing-at-scale-for-palo-alto-networks">Real-time Event Stream Processing at Scale for Palo Alto Networks&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://www.paloaltonetworks.com/">Palo Alto Networks, Inc.&lt;/a> is a global cybersecurity leader with a comprehensive
portfolio of enterprise products. Palo Alto Networks protects and provides visibility, trusted intelligence, automation,
and flexibility to &lt;a href="https://www.paloaltonetworks.com/about-us">over 85K customers&lt;/a> across clouds, networks, and devices.&lt;/p>
&lt;p>Palo Alto Networks’ integrated security operations platform - &lt;a href="https://www.paloaltonetworks.com/cortex">Cortex™&lt;/a> -
applies AI and machine learning to enable security automation, advanced threat intelligence, and effective rapid
security responses for Palo Alto Networks’
customers. &lt;a href="https://www.paloaltonetworks.com/cortex/cortex-data-lake">Cortex™ Data Lake&lt;/a> infrastructure collects,
integrates, and normalizes enterprises’ security data combined with trillions of multi-source artifacts.&lt;/p>
&lt;p>Cortex™ data infrastructure processes ~10 millions of security log events per second currently, at ~3 PB per day, which
are on the high end of real-time streaming processing scale in the industry. Palo Alto Networks’ Sr Principal Software
Engineer, Talat Uyarer, shared insights on how Apache Beam provides a high-performing, reliable, and resilient data
processing framework to support this scale.&lt;/p>
&lt;h2 id="large-scale-streaming-infrastructure">Large-scale Streaming Infrastructure&lt;/h2>
&lt;p>When building the data infrastructure from the ground up, Palo Alto Networks’ Cortex Data Lake team faced a challenging
task. We needed to ensure that the Cortex platform could stream and process petabyte-sized data coming from customers’
firewalls, networks, and all kinds of devices to customers and internal apps with low latency and perfect quality.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/case-study/paloalto/data_lake_scheme.png" alt="Cortex™ Data Lake">
&lt;/div>
&lt;p>To meet the SLAs, the Cortex Data Lake team had to design a large-scale data infrastructure for real-time processing and
reduce time-to-value. One of their initial architectural decisions was to leverage Apache Beam, the industry standard
for unified distributed processing, due to its portability and abstraction.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
Beam is very flexible, its abstraction from implementation details of distributed data processing is wonderful for delivering proofs of concept really fast.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/case-study/paloalto/talat_uyarer.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Talat Uyarer
&lt;/div>
&lt;div class="case-study-quote-author-position">
Sr Principal Software Engineer
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>Apache Beam provides a variety of runners, offering freedom of choice between different data processing engines. Palo
Alto Networks’ data infrastructure is hosted entirely on &lt;a href="https://cloud.google.com/gcp/">Google Cloud Platform&lt;/a>,
and &lt;a href="https://beam.apache.org/documentation/runners/capability-matrix/">with Apache Beam Dataflow runner&lt;/a>, we could
easily benefit from &lt;a href="https://cloud.google.com/dataflow">Google Cloud Dataflow&lt;/a>’s managed service and
&lt;a href="https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#horizontal-autoscaling">autotuning&lt;/a> capabilities.
Apache Kafka was selected as the message broker for the backend, and all events were stored as binary data with a common
schema on multiple Kafka clusters.&lt;/p>
&lt;p>The Cortex Data Lake team considered the option of having separate data processing infrastructures for each customer,
with multiple upstream applications creating their own streaming jobs, consuming and processing events from Kafka
directly. Therefore we are building a multi-tenants system. However, the team anticipated possible issues related to
Kafka migrations and partition creation, as well as a lack of visibility into the tenant use cases, which might arise
when having multiple infrastructures.&lt;/p>
&lt;p>Hence, the Cortex Data Lake team took a common streaming infrastructure approach. At the core of the common data
infrastructure, Apache Beam served as a unified programming model to implement business logic just once for all internal
and customer tenant applications.&lt;/p>
&lt;p>The first data workflows that the Cortex Data Lake team implemented were simple: reading from Kafka, creating a batch
job, and writing the results to sink. The release of
the &lt;a href="https://beam.apache.org/get-started/downloads/#releases">Apache Beam version with SQL support&lt;/a> opened up new
possibilities. &lt;a href="https://beam.apache.org/documentation/dsls/sql/calcite/overview/">Beam Calcite SQL&lt;/a> provides full
support for &lt;a href="https://beam.apache.org/documentation/dsls/sql/calcite/data-types/">complex Apache Calcite data types&lt;/a>,
including nested rows, in SQL statements, so developers can use SQL queries in an Apache Beam pipeline for composite
transforms. The Cortex Data Lake team decided to take advantage of the
&lt;a href="https://beam.apache.org/documentation/dsls/sql/overview/">Beam SQL&lt;/a> to write Beam pipelines with standard SQL
statements.&lt;/p>
&lt;p>The main challenge of the common infrastructure was to support a variety of business logic customizations and
user-defined functions and transform them to a variety of sink formats. Tenant applications needed to consume data from
dynamically-changing Kafka clusters, and streaming pipeline &lt;a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAGs&lt;/a>
had to be regenerated if the jobs’ source had been updated.&lt;/p>
&lt;p>The Cortex Data Lake team developed their own “subscription” model that allows tenant applications to “subscribe” to the
streaming job when sending job deployment requests to the REST API service. The Subscription service abstracts tenant
applications from the changes in DAG by storing infrastructure-specific information in metadata service. This way, the
streaming jobs stay in sync with the dynamic Kafka infrastructure.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/case-study/paloalto/subscription_service_scheme.png" alt="Cortex™ Data Lake Subscription Service">
&lt;/div>
&lt;p>Apache Beam is flexible, it allows creating streaming jobs dynamically, on the fly. The Apache Beam constructs allow for
generic pipeline coding, enabling pipelines that process data even if schemas are not fully defined in advance. Cortex’s
Subscription Service generates Apache Beam pipeline DAG based on the tenant application’s REST payload and submits the
job to the runner. When the job is
running, &lt;a href="https://beam.apache.org/releases/javadoc/2.4.0/org/apache/beam/sdk/io/kafka/KafkaIO.html">Apache Beam SDK’s Kafka I/O&lt;/a>
returns an unbounded collection of Kafka records as
a &lt;a href="https://beam.apache.org/releases/javadoc/2.1.0/org/apache/beam/sdk/values/PCollection.html">PCollection&lt;/a>
. &lt;a href="https://avro.apache.org/">Apache Avro&lt;/a> turns the binary Kafka representation into generic records, which are further
converted to the &lt;a href="https://beam.apache.org/releases/javadoc/2.4.0/org/apache/beam/sdk/values/Row.html">Apache Beam Row&lt;/a>
format. The Row structure supports primitives, byte arrays, and containers, and allows organizing values in the same
order as the schema definition.&lt;/p>
&lt;p>Apache Beam’s cross-language transforms allow the Cortex Data Lake team to execute SQL with Java. The output of
an &lt;a href="https://beam.apache.org/releases/javadoc/2.7.0/org/apache/beam/sdk/extensions/sql/SqlTransform.html">SQL Transform&lt;/a>
performed inside the Apache Beam pipeline is sequentially converted from Beam Row format to a generic record, then to
the output format required by a subscriber application, such as Avro, JSON, CSV, etc.&lt;/p>
&lt;p>Once the base use cases had been implemented, the Cortex Data Lake team turned to more complex transformations, such as
filtering a subset of events directly inside Apache Beam pipelines, and kept looking into customization and
optimization.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
We have more than 10 use cases running across customers and apps. More are coming, like the machine learning use cases .... for these use cases, Beam provides a really good programming model.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/case-study/paloalto/talat_uyarer.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Talat Uyarer
&lt;/div>
&lt;div class="case-study-quote-author-position">
Sr Principal Software Engineer
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>Apache Beam provides a pluggable data processing model that seamlessly integrates with various tools and technologies,
which allowed the Cortex Data Lake team to customize their data processing to performance requirements and specific use
cases.&lt;/p>
&lt;h2 id="customizing-serialization-for-use-cases">Customizing Serialization for Use Cases&lt;/h2>
&lt;p>Palo Alto Networks’ streaming data infrastructure deals with hundreds of billions of real-time security events every
day, and even a sub-second difference in processing times is crucial.&lt;/p>
&lt;p>To enhance performance, the Cortex Data Lake team developed their own library for direct serialization and
deserialization. The library reads Avro binary records from Kafka and turns them into the Beam Row format, then converts
the Beam Row format pipeline output to the required sink format.&lt;/p>
&lt;p>This custom library replaced serializing data into generic records with steps optimized for Palo Alto Networks’ specific
use cases. Direct serialization eliminated shuffling and creating additional memory copies from processing steps.&lt;/p>
&lt;p>This customization increased serialization performance 10x times, allowing to process up to 3K events per second per
vCPU with reduced latency and infrastructure costs.&lt;/p>
&lt;div class="post-scheme vertical-scheme">
&lt;img src="/images/case-study/paloalto/direct_serialization.png" alt="Direct Serialization from Avro to Beam Row">
&lt;/div>
&lt;h2 id="in-flight-streaming-job-updates">In-flight Streaming Job Updates&lt;/h2>
&lt;p>At a scale of thousands of jobs running concurrently, the Cortex Data Lake team faced cases when needed to improve the
pipeline code or fix bugs for an ongoing job. Google Cloud Dataflow provides a way
to &lt;a href="https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline">replace an “in-flight” streaming job&lt;/a> with a new
job that runs an updated Apache Beam pipeline code. However, Palo Alto Networks needed to expand the supported
scenarios.&lt;/p>
&lt;p>To address updating jobs in the dynamically-changing Kafka infrastructure, the Cortex Data Lake team created an
additional workflow in their deployment service
which &lt;a href="https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain">drains the jobs&lt;/a> if the change
is &lt;a href="https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#UpdateSchemas">not permitted&lt;/a> by the Dataflow
update and starts a new job with the exact same naming. This internal job replacement workflow allows the Cortex Data
Lake to update the jobs and payloads automatically for all use cases.&lt;/p>
&lt;h2 id="handling-schema-changes-in-beam-sql">Handling Schema Changes In Beam SQL&lt;/h2>
&lt;p>Another use case that Palo Alto Networks tackled is handling changes in data schemas for ongoing jobs. Apache Beam
allows PCollections to have &lt;a href="https://beam.apache.org/documentation/programming-guide/#schemas">schemas&lt;/a> with named
fields, that are validated at pipeline construction step. When a job is submitted, an execution plan in the form of a
Beam pipeline fragment is generated based on the latest schema. Beam SQL does not yet have built-in support for relaxed
schema compatibility for running jobs. For optimized performance, Beam SQL’s
Schema &lt;a href="https://beam.apache.org/releases/javadoc/2.4.0/org/apache/beam/sdk/coders/RowCoder.html">RowCoder&lt;/a> has a fixed
data format and doesn&amp;rsquo;t handle schema evolution, so it is necessary to restart the jobs to regenerate their execution
plan. At a scale of 10K+ streaming jobs, Cortex Data Lake team wanted to avoid resubmitting the jobs as much as
possible.&lt;/p>
&lt;p>We created an internal workflow to identify the jobs with SQL queries relevant to the schema change. The schema update
workflow stores Reader schema of each job (Avro schema) and Writer schema of each Kafka message (metadata on Kafka
header) in the internal Schema Registry, compares them to the SQL queries of the running jobs, and restarts the affected
jobs only. This optimization allowed them to utilize resources more efficiently.&lt;/p>
&lt;h2 id="fine-tuning-performance-for-kafka-changes">Fine-tuning Performance for Kafka Changes&lt;/h2>
&lt;p>With multiple clusters and topics, and over 100K partitions in Kafka, Palo Alto Networks needed to make sure that
actively-running jobs are not being affected by the frequent Kafka infrastructure changes such as cluster migrations or
changes in partition count.&lt;/p>
&lt;p>The Cortex Data Lake team developed several internal Kafka lifecycle support tools, including a “Self Healing” service.
Depending on the amount of traffic per topic coming from a specific tenant, the internal service increases the number of
partitions or creates new topics with fewer partitions. The “Self Healing” service compares the Kafka states in the data
store and then finds and updates all related streaming Apache Beam jobs on Cloud Dataflow automatically.&lt;/p>
&lt;p>With the &lt;a href="https://beam.apache.org/blog/beam-2.28.0/">release of Apache Beam 2.28.0&lt;/a> in early
2021, &lt;a href="https://beam.apache.org/releases/javadoc/2.29.0/org/apache/beam/sdk/io/kafka/KafkaIO.html">the pre-built Kafka I/O dynamic read feature&lt;/a>
provides an out-of-the-box solution for detecting Kafka partition changes to enable cost savings and increased
performance. Kafka I/O uses WatchKafkaTopicPartitionDoFn to emit
new &lt;a href="https://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/common/TopicPartition.html">TopicPartitions&lt;/a>, and
allows reading from Kafka topics dynamically when certain partitions are added or stop reading from them once they are
deleted. This feature eliminated the need to create in-house Kafka monitoring tools.&lt;/p>
&lt;p>In addition to performance optimization, the Cortex Data Lake team has been exploring ways to optimize the Cloud
Dataflow costs. We looked into resource usage optimization in cases when streaming jobs consume very few incoming
events. For cost efficiency, Google Cloud Dataflow provides
the &lt;a href="https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#streaming-autoscaling">streaming autoscaling&lt;/a>
feature that adaptively changes the number of workers in response to changes in the load and resource utilization. For
some of Cortex Data Lake team’s use cases, where input data streams may quiesce for prolonged periods of time, we
implemented an internal “Cold Starter” service that analyzes Kafka topics traffic and hibernates pipelines whose input
dries up and reactivates them once their input resumes.&lt;/p>
&lt;p>Talat Uyarer presented the Cortex Data Lake’s experience of building and customizing the large-scale streaming
infrastructure during &lt;a href="https://2021.beamsummit.org/sessions/large-scale-streaming-infrastructure/">Beam Summit 2021&lt;/a>.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
I really enjoy working with Beam. If you understand its internals, the understanding empowers you to fine-tune the open source, customize it, so that it provides the best performance for your specific use case.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/case-study/paloalto/talat_uyarer.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Talat Uyarer
&lt;/div>
&lt;div class="case-study-quote-author-position">
Sr Principal Software Engineer
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>The level of abstraction of Apache Beam empowered the Cortex Data Lake team to create a common infrastructure across
their internal apps and tens of thousands of customers. With Apache Beam, we implement business logic just once and
dynamically generate 10K+ streaming pipelines running in parallel for over 10 use cases.&lt;/p>
&lt;p>The Cortex Data Lake team took advantage of Apache Beam’s portability and pluggability to fine-tune and enhance their
data processing infrastructure with custom libraries and services. Palo Alto Networks ultimately achieved high
performance and low latency, processing 3K+ streaming events per second per vCPU. Combining the benefits of open source
Apache Beam and Cloud Dataflow managed service, we were able to implement use-case specific customizations and reduced
their costs by more than 60%.&lt;/p>
&lt;p>The Apache Beam open source community welcomes and encourages the contributions of its numerous members, such as Palo
Alto Networks, that leverage the powerful capabilities of Apache Beam, bring new optimizations, and empower future
innovation by sharing their expertise and actively participating in the community.&lt;/p>
&lt;div class="case-study-feedback" id="case-study-feedback">
&lt;p class="case-study-feedback-title">Was this information useful?&lt;/p>
&lt;div>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(true, 'Palo Alto')">Yes&lt;/button>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(false, 'Palo Alto')">No&lt;/button>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="clear-nav">&lt;/div></description></item><item><title>Case-Studies: Beam visual pipeline development with Hop</title><link>/case-studies/hop/</link><pubDate>Tue, 15 Feb 2022 12:21:00 +0000</pubDate><guid>/case-studies/hop/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div class="case-study-opinion">
&lt;div class="case-study-opinion-img">
&lt;img src="/images/logos/powered-by/hop.svg"/>
&lt;/div>
&lt;blockquote class="case-study-quote-block">
&lt;p class="case-study-quote-text">
“Apache Beam and its abstraction of the execution engines is a big thing for us. The amount of work that that saves...it would be hard to build that support for Dataflow or Spark all by yourself. It is amazing that this technology exists in the first place, really amazing! Not having to worry about all those underlying platforms - that is tremendous!”
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/matt_casters_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Matt Casters
&lt;/div>
&lt;div class="case-study-quote-author-position">
Chief Solutions Architect, Neo4j, Apache Hop co-founder
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;/div>
&lt;div class="case-study-post">
&lt;h1 id="visual-apache-beam-pipeline-design-and-orchestration-with-apache-hop">Visual Apache Beam Pipeline Design and Orchestration with Apache Hop&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://hop.apache.org/">Apache Hop&lt;/a> is an open source data orchestration and data engineering
platform that aims to facilitate all aspects of data processing with visual pipeline development
environment. This easy-to-use, fast, and flexible platform enables developers to create and manage
Apache Beam batch and streaming pipelines in Hop GUI. Apache Hop uses metadata and kernel to
describe how the data should be processed, and Apache Beam to “design once, run anywhere”.&lt;/p>
&lt;p>&lt;a href="https://neo4j.com/">Neo4j’s&lt;/a> Chief Solutions
Architect, &lt;a href="https://be.linkedin.com/in/mattcasters">Matt Casters&lt;/a>, has been an early adopter of
Apache Beam and its abstraction of execution engines. Matt has been an active member of the Apache
open-source community for years and has leveraged Apache Beam as an execution engine to build Apache
Hop.&lt;/p>
&lt;h2 id="apache-hop-project">Apache Hop Project&lt;/h2>
&lt;p>Thriving popularity and the growing number of Apache Beam users across the globe inspired Matt
Casters to expand the idea of abstraction to visual pipeline lifecycle management and development.
Matt co-founded and incubated the
&lt;a href="https://hop.apache.org/">Apache Hop&lt;/a> project that became a top level project at
the &lt;a href="https://www.apache.org/">Apache Software Foundation&lt;/a>
in December 2021. The platform enables users of all skill levels to build, test, launch, and deploy
powerful data workflows without writing code. Apache Hop’s intuitive drag and drop interface
provides a visual representation of Apache Beam pipelines, simplifying pipeline design, execution,
preview, monitoring, and debugging.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
I was a big fan of Beam from the get go. Apache Beam is now a very important part of the Apache Hop project.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/matt_casters_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Matt Casters
&lt;/div>
&lt;div class="case-study-quote-author-position">
Chief Solutions Architect, Neo4j,
&lt;br>Apache Hop co-founder
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>The Apache Hop GUI allows data professionals to work visually and focus on “what” they need to do
rather than “how”, using metadata to describe how the Apache Beam pipelines should be processed.
Apache
Hop’s &lt;a href="https://hop.apache.org/manual/latest/pipeline/create-pipeline.html#_concepts">transform-agnostic&lt;/a>
action
plugins (&lt;a href="https://hop.apache.org/manual/latest/pipeline/create-pipeline.html#_concepts">“hops”&lt;/a>)
link transforms together, creating a pipeline. Various Apache Beam runners, such as
&lt;a href="https://hop.apache.org/manual/latest/pipeline/pipeline-run-configurations/beam-spark-pipeline-engine.html">Spark&lt;/a>
,
&lt;a href="https://hop.apache.org/manual/latest/pipeline/pipeline-run-configurations/beam-flink-pipeline-engine.html">Flink&lt;/a>
,
&lt;a href="https://hop.apache.org/manual/latest/pipeline/pipeline-run-configurations/beam-dataflow-pipeline-engine.html">Dataflow&lt;/a>
, and
the &lt;a href="https://hop.apache.org/manual/latest/pipeline/pipeline-run-configurations/beam-direct-pipeline-engine.html">Direct&lt;/a>
runner, read the metadata with help of Apache
Hop&amp;rsquo;s &lt;a href="https://hop.apache.org/dev-manual/latest/sdk/hop-sdk.html#_hop_metadata_providers">Metadata Provider&lt;/a>
and &lt;a href="https://hop.apache.org/dev-manual/latest/sdk/hop-sdk.html#_workflow_execution">workflow engines(plugins)&lt;/a>
, and execute the pipeline.&lt;/p>
&lt;p>Apache Hop’s custom plugins and metadata objects
for &lt;a href="https://hop.apache.org/manual/latest/technology/technology.html">some of the most popular technologies&lt;/a>
, such as &lt;a href="https://neo4j.com/">Neo4j&lt;/a>, empower users to execute database- and technology-specific
transforms inside the Apache Beam pipelines, which allows for native optimized connectivity and
flexible Apache Beam pipeline configurations. For instance, the Apache
Hop’s &lt;a href="https://hop.apache.org/manual/latest/technology/neo4j/index.html#_description">Neo4j plugin&lt;/a>
stores logging and execution lineage of Apache Beam pipelines in the Neo4j graph database and
enables users to query this information for more details, such as quickly jump to the place where an
error occurred. The combination of Apache Hop
transforms, &lt;a href="https://beam.apache.org/documentation/io/built-in/">Apache Beam built-in I/Os&lt;/a>, and
Apache Beam-powered data processing opens up new horizons for more sinks and sources and custom use
cases.&lt;/p>
&lt;p>Apache Hop aims to bring a no-code approach to Apache Beam data pipelines. Sometimes the choice of a
particular programming language, framework, or engine is driven by developers&amp;rsquo; preferences, which
results in businesses becoming tied to a specific technology skill set and stack. Apache Hop
eliminates this dependency by abstracting out the I/Os with a fully pluggable runtime support and
providing a graphic user interface on top of Apache Beam pipelines. All settings for pipeline
elements are performed in the Hop’s visual editor just once, and pipeline is automatically described
as metadata in JSON and CSV formats. Programming data pipelines’ source code becomes an option, not
a necessity. Apache Hop does not require knowledge of a particular programming language to create
pipelines, helping with the adoption of Apache Beam unified streaming and batch processing
technology.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
In general, a visual pipeline design interface is really valuable for a non-developer audience…
We categorically choose the side of the organization when it comes to lowering setup costs,
maintenance costs, increasing ROI, and safeguarding an investment over time.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/matt_casters_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Matt Casters
&lt;/div>
&lt;div class="case-study-quote-author-position">
Chief Solutions Architect, Neo4j,
&lt;br>Apache Hop co-founder
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Apache Beam continuously expands the number of use cases and scenarios it supports and makes it
possible to bring advanced technology solutions into a reality. Being an early adopter of Apache
Beam and its powerful abstraction, Matt Casters leveraged this knowledge and experience to create
Apache Hop. The platform creates a value-add for Apache Beam users by enabling visual pipeline
development and lifecycle management.&lt;/p>
&lt;p>Matt sees Apache Beam as a foundation and a driving force behind Apache Hop. Communication between
Apache Beam and Apache Hop projects keeps fostering co-creation and enriches both products with new
features.&lt;/p>
&lt;p>Apache Hop project is the example of the continuous improvement driven by the Apache open source
community and amplified by collaborative organizations.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
Knowledge sharing and collaboration is something that comes naturally in the community. If we
see some room for improvement, we exchange ideas and this way, we keep driving Apache Beam and
Apache Hop projects forward. Together, we can work with the most complex problems and just solve them.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/matt_casters_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Matt Casters
&lt;/div>
&lt;div class="case-study-quote-author-position">
Chief Solutions Architect, Neo4j,
&lt;br>Apache Hop co-founder
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;div class="case-study-feedback" id="case-study-feedback">
&lt;p class="case-study-feedback-title">Was this information useful?&lt;/p>
&lt;div>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(true, 'Hop')">Yes&lt;/button>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(false, 'Hop')">No&lt;/button>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="clear-nav">&lt;/div></description></item><item><title>Case-Studies: Scalability and Cost Optimization for Search Engine's Workloads</title><link>/case-studies/seznam/</link><pubDate>Tue, 15 Feb 2022 01:56:00 +0000</pubDate><guid>/case-studies/seznam/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div class="case-study-opinion">
&lt;div class="case-study-opinion-img">
&lt;img src="/images/logos/powered-by/seznam.png"/>
&lt;/div>
&lt;blockquote class="case-study-quote-block">
&lt;p class="case-study-quote-text">
“Apache Beam is a well-defined data processing model that lets you concentrate on business logic rather than low-level details of distributed processing.”
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/marek_simunek_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Marek Simunek
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Software Engineer @ seznam.cz
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;/div>
&lt;div class="case-study-post">
&lt;h1 id="scalability-and-cost-optimization-for-search-engines-workloads">Scalability and Cost Optimization for Search Engine&amp;rsquo;s Workloads&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://www.seznam.cz/">Seznam.cz&lt;/a> is a Czech search engine that serves over 25% of local organic search traffic.
Seznam employs over 1,500 people and runs a portfolio of more than 30 web services and associated brands,
processing around &lt;a href="https://www.searchenginejournal.com/seznam-interview/302851/#close">15 million queries a day&lt;/a>.&lt;/p>
&lt;p>Seznam continuously optimizes their big data infrastructure, web crawlers, algorithms,
and ML models on a mission to achieve excellence in accuracy, quality, and usefulness of search results for their users.
Seznam has been an early contributor and adopter of Apache Beam, and they migrated several petabyte-scale workloads
to Apache Beam pipelines running in Apache Spark and Apache Flink clusters in Seznam’s on-premises data center.&lt;/p>
&lt;h2 id="journey-to-apache-beam">Journey to Apache Beam&lt;/h2>
&lt;p>Seznam started using MapReduce in a Hadoop Yarn cluster back in 2010 to facilitate concurrent batch jobs processing
for the web crawler components of their search engine.
Within several years, their data infrastructure evolved to &lt;a href="https://www.youtube.com/watch?v=rJIpva0tD0g">over 40 billion rows with 400 terabytes&lt;/a>
in HBase, 2 on-premises data centers with over 1,100 bare metal servers, 13 PB storage, and 50 TB memory, which made their business logic more complex.
MapReduce no longer provided enough flexibility, &lt;a href="https://youtu.be/rJIpva0tD0g?t=130">cost efficiency, and performance&lt;/a>
to support this growth, and Seznam rewrote the jobs to native Spark.
Spark &lt;a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations">shuffle operations&lt;/a>
enabled Seznam to split large data keys into partitions, load them in-memory one by one, and process them iteratively.
However, exponential data skews and inability to fit all values for a single key into an in-memory buffer resulted in
&lt;a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#performance-impact">increased disk space utilization and memory overhead&lt;/a>.
Some tasks took unexpectedly long time to complete, and it was challenging
to debug Spark pipelines due to generic exceptions. Thus, Seznam needed a data processing framework that can scale more efficiently.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
To manage this kind of scale, you need the abstraction.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/marek_simunek_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Marek Simunek
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Software Engineer @ seznam.cz
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>In 2014, Seznam started work on Euphoria API - a proprietary programming model that can express business logic
in batch and streaming pipelines and allow for runner independent implementation.&lt;/p>
&lt;p>Apache Beam was released in 2016 and became a readily available and well-defined unified programming model.
This engine-independent model has been evolving very fast, supports multiple shuffle operators and fits perfectly
into Seznam’s existing on-premises data infrastructure. For a while, Seznam continued to develop Euphoria,
but soon the high cost and the amount of effort needed to maintain the solution and create their own
runners in-house surpassed the benefits of having a proprietary framework.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/seznam_scheme_1.png">
&lt;/div>
&lt;p>Seznam started migrating their key workloads to Apache Beam.
They decided to merge the &lt;a href="https://beam.apache.org/documentation/sdks/java/euphoria/">Euphoria API&lt;/a>
as a high-level DSL for Apache Beam Java SDK.
This significant contribution to Apache Beam was a starting point for Seznam’s active participation in the community,
later presenting their unique experience and findings at &lt;a href="https://www.youtube.com/watch?v=ZIFtmx8nBow">Beam Summit Europe 2019&lt;/a>
and developer conferences.&lt;/p>
&lt;h2 id="adopting-apache-beam">Adopting Apache Beam&lt;/h2>
&lt;p>Apache Beam enabled Seznam to execute batch and stream jobs much faster without increasing memory and disk space,
thus maximizing scalability, performance, and efficiency.&lt;/p>
&lt;p>Apache Beam offers a variety of ways to distribute skewed data evenly.
&lt;a href="https://beam.apache.org/documentation/programming-guide/#windowing">Windowing&lt;/a>
for processing unbounded and &lt;a href="https://beam.apache.org/documentation/transforms/java/elementwise/partition/">Partition&lt;/a>
for bounded data sets transform input into finite
collections of elements that can be reshuffled. Apache Beam provides a byte-based shuffle that can be
executed by Spark runner or Flink runner, without requiring Apache Spark or Apache Flink to deserialize the full key.
Apache Beam SDKs provide effective coders to serialize and deserialize elements and pass to distributed workers.
Using Apache Beam serialization and byte-based shuffle resulted in substantial performance gains for many of the
Seznam’s use cases and reduced memory required for the shuffling by Apache Spark execution environment.
Seznam’s infrastructure costs associated with &lt;a href="https://youtu.be/rJIpva0tD0g?t=522">disk I/O and memory splits&lt;/a>
decreased significantly.&lt;/p>
&lt;p>One of the most valuable use cases is Seznam’s LinkRevert job, which analyzes the web graph to improve search relevance.
This data pipeline figuratively “turns the Internet upside down”, processing over 150 TB daily,
extending redirect chains to identify every successor of a specific URL, and discovering backlinks that point to a specific web page.
The Apache Beam pipeline executes multiple large-scale skewed joins, and scores the URLs for search results based on the redirect and backlinking factors.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/seznam_scheme_2.png">
&lt;/div>
&lt;p>Apache Beam allows for a unified engine-independent execution, so Seznam was able to select between
Spark or Flink runner depending on the use case. For example, the Apache Beam batch pipeline executed by
Spark runner on a Hadoop Yarn cluster parses new web documents, enriches data with additional features,
and scores the web pages based on their relevance, ensuring timely database updates and accurate search results.
Apache Beam stream processing runs in the Apache Flink execution environment on a Kubernetes cluster for thumbnail
requests that are displayed in users’ search results. Another example of stream event processing is the Apache Beam Flink
runner pipeline that maps, joins, and processes search logs to calculate SLO metrics and other features.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/seznam_scheme_3.png">
&lt;/div>
&lt;div class="post-scheme">
&lt;img src="/images/seznam_scheme_4.png">
&lt;/div>
&lt;p>Over the years, Seznam’s approach has evolved. They have realized the tremendous benefits of Apache Beam
for balancing petabyte-size workloads and optimizing memory and compute resources in on-premises data centers.
Apache Beam is Seznam’s go-to platform for batch and stream pipelines that require multiple shuffle operations,
processing skewed data, and implementing complex business logic. Apache Beam unified model with sources
and sinks exposed as transforms, increased business logic maintainability and traceability with unit tests.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
One of the biggest benefits is Apache Beam sinks and sources. By exposing your source or sink as a transform, your implementation is hidden and later on, you can add additional functionality without breaking the existing implementation for users.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/marek_simunek_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Marek Simunek
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Software Engineer @ seznam.cz
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;h2 id="monitoring-and-debugging">Monitoring and Debugging&lt;/h2>
&lt;p>Apache Beam pipelines monitoring and debugging was critical for cases with complex business logic and
multiple data transformations. Seznam engineers identified optimal tools depending on the execution engine.
Seznam leveraged &lt;a href="https://github.com/criteo/babar">Babar from Criteo&lt;/a> to profile Apache Beam pipelines
on Spark runner and identify the root causes
of downtimes in their performance. Babar allows for easier monitoring, debugging, and performance optimization
by analyzing cluster resource utilization, memory allocated, CPU used, etc. For Apache Beam pipelines executed by Flink runner
on Kubernetes cluster, Seznam employs Elasticsearch to store, search, and analyze metrics.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Apache Beam offered a unified model for Seznam’s stream and batch processing that provided performance at scale.
Apache Beam supported multiple runners, language SDKs, and built-in and custom pluggable I/O transforms,
thus eliminating the need to invest into the development and support of proprietary runners and solutions.
After evaluation, Seznam transitioned their workloads to Apache Beam and integrated
&lt;a href="https://beam.apache.org/documentation/sdks/java/euphoria/">Euphoria API&lt;/a>
(a fast prototyping framework developed by Seznam), contributing to the Apache Beam open source community.&lt;/p>
&lt;p>The Apache Beam abstraction and execution model allowed Seznam to robustly scale their data processing.
It also provided the flexibility to write the business logic just once and keep freedom of choice between runners.
The model was especially valuable for pipeline maintainability in complex use cases.
Apache Beam helped overcome memory and compute resource constraints by reshuffling unevenly distributed data into manageable partitions.
&lt;div class="case-study-feedback" id="case-study-feedback">
&lt;p class="case-study-feedback-title">Was this information useful?&lt;/p>
&lt;div>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(true, 'Seznam')">Yes&lt;/button>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(false, 'Seznam')">No&lt;/button>
&lt;/div>
&lt;/div>
&lt;/p>
&lt;/div>
&lt;div class="clear-nav">&lt;/div></description></item><item><title>Case-Studies: Apache Beam Amplified Ricardo’s Real-time and ML Data Processing for eCommerce Platform</title><link>/case-studies/ricardo/</link><pubDate>Wed, 01 Dec 2021 01:36:00 +0000</pubDate><guid>/case-studies/ricardo/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div class="case-study-opinion">
&lt;div class="case-study-opinion-img">
&lt;img src="/images/logos/powered-by/ricardo.png"/>
&lt;/div>
&lt;blockquote class="case-study-quote-block">
&lt;p class="case-study-quote-text">
“Without Beam, without all this data and real time information, we could not deliver the services we are providing and handle the volumes of data we are processing.”
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;/div>
&lt;div class="case-study-post">
&lt;h1 id="apache-beam-amplified-ricardos-real-time-and-ml-data-processing-for-ecommerce-platform">Apache Beam Amplified Ricardo’s Real-time and ML Data Processing for eCommerce Platform.&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://www.ricardo.ch/">Ricardo&lt;/a> is a leading second hand marketplace in Switzerland. The site supports over 4 million
registered buyers and sellers, processing more than 6.5 million article transactions via the platform annually. Ricardo
needs to process high volumes of streaming events and manage over 5 TB of articles, assets, and analytical data.&lt;/p>
&lt;p>With the scale that came from 20 years in the market, Ricardo made the decision to migrate from their on-premises data
center to cloud to easily grow and evolve further and reduce operational costs through managed cloud services. Data
intelligence and engineering teams took the lead on this transformation and development of new AI/ML-enabled customer
experiences. Apache Beam has been a technology amplifier that expedited Ricardo’s transformation.&lt;/p>
&lt;h2 id="challenge">Challenge&lt;/h2>
&lt;p>Migrating from an on-premises data center to the cloud presented Ricardo with an opportunity to modernize their
marketplace from heavy legacy reliance on transactional SQL, switch to BigQuery for analytics, and take advantage of the
event-based streaming architecture.&lt;/p>
&lt;p>Ricardo’s data intelligence team identified two key success factors: a carefully designed data model and a framework
that provides unified stream and batch data pipelines execution, both on-premises and in the cloud.&lt;/p>
&lt;p>Ricardo needed a data processing framework that can scale easily, enrich event streams with historic data from multiple
sources, provide granular control on data freshness, and provide an abstract pipeline operational infrastructure, thus
helping their team focus on creating new value for customers and business&lt;/p>
&lt;h2 id="journey-to-beam">Journey to Beam&lt;/h2>
&lt;p>Ricardo’s data intelligence team began modernizing their stack in 2018. They selected frameworks that provide reliable
and scalable data processing both on-premises and in the cloud. Apache Beam enables users to create pipelines in their
favorite programming language offering SDKs in Java, Python, Go, SQL, Scala (SCIO).
A &lt;a href="https://beam.apache.org/documentation/#available-runners">Beam Runner&lt;/a> runs a Beam pipeline on a specific (often
distributed) data processing system. Ricardo selected the Apache Beam Flink runner for executing pipelines on-premises
and the Dataflow runner as a managed cloud service for the same pipelines developed using Apache Beam Java SDK. Apache
Flink is well known for its reliability and cost-efficiency and an on-premises cluster was spun up at Ricardo’s
datacenter as the initial environment.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
We wanted to implement a solution that would multiply our possibilities, and that’s exactly where Beam comes in. One of the major drivers in this decision was the ability to evolve without adding too much operational load.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>Beam pipelines for core business workloads to ingest events data from Apache Kafka into BigQuery were running stable in
just one month. As Ricardo’s cloud migration progressed, the data intelligence
team &lt;a href="https://www.youtube.com/watch?v=EcvnFH5LDE4">migrated Flink cluster from Kubernetes&lt;/a> in their on-premises
datacenter to GKE.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
I knew Beam, I knew it works. When you need to move from Kafka to BigQuery and you know that Beam is exactly the right tool, you just need to choose the right executor for it.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>The flexibility to refresh data every hour, minute, or stream data real-time, depending on the specific use case and
need, helped the team improve data freshness which was a significant advancement for Ricardo’s eCommerce platform
analytics and reporting.&lt;/p>
&lt;p>Ricardo’s team found benefits in Apache Beam Flink runner on self-managed Flink cluster in GKE for streaming pipelines.
Full control over Flink provisioning enabled to set up required connectivity from Flink cluster to an external peered
Kafka managed service. The data intelligence team optimized operating costs through cluster resource utilization
significantly. For batch pipelines, the team chose Dataflow managed service for its on-demand autoscaling and cost
reduction features like FlexRS, especially efficient for training ML models over TBs of historic data. This hybrid
approach has been serving Ricardo’s needs well and proved to be a reliable production solution.&lt;/p>
&lt;h2 id="evolution-of-use-cases">Evolution of Use Cases&lt;/h2>
&lt;p>Thinking of a stream as data in motion, and a table as data at rest provided a fortuitous chance to take a look at some
data model decisions that were made as far back as 20 years before. Articles that are on the marketplace have assets
that describe them, and for performance and cost optimizations purposes, data entities that belong together were split
into separate database instances. Apache Beam enabled Ricardo’s data intelligence team
to &lt;a href="https://youtu.be/PiwLC-YK_Zw">join assets and articles streams&lt;/a> and optimize BigQuery scans to reduce costs. When
designing the pipeline, the team created streams for assets and articles. Since the assets stream is the primary one,
they shifted the stream 5 minutes back and created a lookup schema with it in BigTable. This elegant solution ensures
that the assets stream is always processed first while BigTable allows for matching the latest asset to an article and
Apache Beam joins them both together.&lt;/p>
&lt;div class="post-scheme">
&lt;img src="/images/post_scheme.png">
&lt;/div>
&lt;p>The successful case of joining different data streams facilitated further Apache Beam adoption by Ricardo in areas like
data science and ML.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
Once you start laying out the simple use cases, you will always figure out the edge case scenarios. This pipeline has been running for a year now, and Beam handles it all, from super simple use cases to something crazy.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>As an eCommerce retailer, Ricardo faces the increasing scale and sophistication of fraud transactions and takes a
strategic approach by employing Beam pipelines for fraud detection and prevention. Beam pipelines act on an external
intelligent API to identify the signs of fraudulent behaviour, like device characteristics or user activity. Apache Beam
&lt;a href="https://beam.apache.org/documentation/programming-guide/#state-and-timers">stateful processing&lt;/a> feature enables Ricardo
to apply an associating operation to the streams of data (trigger banishing a user for example). Thus, Apache Beam saves
Ricardo’s customer care team&amp;rsquo;s time and effort on investigating duplicate cases. It also runs batch pipelines
to &lt;a href="https://www.youtube.com/watch?v=LXnh9jNNfYY">find linked accounts&lt;/a>, associate products to categories by
encapsulating a ML model, or calculates the likelihood something is going to sell, at a scale or precision that was
previously not possible.&lt;/p>
&lt;p>Originally implemented by Ricardo’s data intelligence team, Apache Beam has proven to be a powerful framework that
supports advanced scenarios and acts as a glue between Kafka, BigQuery, and platform and external APIs, which encouraged
other teams at Ricardo to adopt it.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
[Apache Beam] is a framework that is so good that other teams are picking up the idea and starting to work with it after we tested it.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Apache Beam has provided Ricardo with a scalable and reliable data processing framework that supported Ricardo’s
fundamental business scenarios and enabled new use cases to respond to events in real-time.&lt;/p>
&lt;p>Throughout Ricardo’s transformation, Apache Beam has been a unified framework that can run batch and stream pipelines,
offers on-premises and cloud managed services execution, and programming language options like Java and Python,
empowered data science and research teams to advance customer experience with new real-time scenarios fast-tracking time
to value.&lt;/p>
&lt;blockquote class="case-study-quote-block case-study-quote-wrapped">
&lt;p class="case-study-quote-text">
After this first pipeline, we are working on other use cases and planning to move them to Beam. I was always trying to spread the idea that this a framework that is reliable, it actually helps you to get the stuff done in a consistent way.
&lt;/p>
&lt;div class="case-study-quote-author">
&lt;div class="case-study-quote-author-img">
&lt;img src="/images/tobias_kaymak_photo.png">
&lt;/div>
&lt;div class="case-study-quote-author-info">
&lt;div class="case-study-quote-author-name">
Tobias Kaymak
&lt;/div>
&lt;div class="case-study-quote-author-position">
Senior Data Engineer @ Ricardo
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/blockquote>
&lt;p>Apache Beam has been a technology that multiplied possibilities, allowing Ricardo to maximize technology benefits at all
stages of their modernization and cloud journey.&lt;/p>
&lt;div class="case-study-feedback" id="case-study-feedback">
&lt;p class="case-study-feedback-title">Was this information useful?&lt;/p>
&lt;div>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(true, 'Ricardo')">Yes&lt;/button>
&lt;button class="btn case-study-feedback-btn" onclick="sendCaseStudyFeedback(false, 'Ricardo')">No&lt;/button>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;div class="clear-nav">&lt;/div></description></item><item><title>Case-Studies: Akvelon</title><link>/case-studies/akvelon/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/akvelon/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--></description></item><item><title>Case-Studies: Beam and Geocoding</title><link>/case-studies/goga/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/goga/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">Beam and Geocoding&lt;/h2>
&lt;/header>
&lt;p>GOGA Data Analysis and Consulting is a company based in Japan that specializes in analytics of geospatial and mapping data. They use Apache Beam and Cloud Dataflow for a smooth data transformation process for analytical purposes. This use case focuses on handling multiple extractions, geocoding, and insertion process by wrangling and requesting API call of each data based on the location provided.&lt;/p>
&lt;/div></description></item><item><title>Case-Studies: Cloud Dataflow</title><link>/case-studies/dataflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/dataflow/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">Cloud Dataflow&lt;/h2>
&lt;/header>
&lt;p>&lt;strong>&lt;a href="https://cloud.google.com/dataflow">Cloud Dataflow&lt;/a>:&lt;/strong> Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud Platform ecosystem.&lt;/p>
&lt;/div></description></item><item><title>Case-Studies: Feature Powered by Apache Beam - Beyond Lambda</title><link>/case-studies/ebay/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/ebay/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">Feature Powered by Apache Beam - Beyond Lambda&lt;/h2>
&lt;/header>
&lt;p>eBay is an American e-commerce company that provides business-to-consumer and consumer-to-consumer sales through the online website. They build feature pipelines with Apache Beam: unify feature extraction and selection in online and offline, speed up E2E iteration for model training, evaluation and serving, support different types (streaming, runtime, batch) of features, etc. eBay leverages Apache Beam for the streaming feature SDK as a foundation to integrate with Kafka, Hadoop, Flink, Airflow and others in eBay.&lt;/p>
&lt;/div></description></item><item><title>Case-Studies: From Apache Beam to Leukemia early detection</title><link>/case-studies/oriel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/oriel/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">From Apache Beam to Leukemia early detection&lt;/h2>
&lt;/header>
&lt;p>Oriel Research Therapeutics (ORT) is a startup company in the greater Boston area that provides early detection services for
multiple medical conditions, utilizing cutting edge Artificial Intelligence technologies and Next Generation Sequencing (NGS). ORT utilizes Apache Beam pipelines to process over 1 million samples of genomics and clinical information. The processed data is used by ORT in detecting Leukemia, Sepsis and other medical conditions.&lt;/p>
&lt;/div></description></item><item><title>Case-Studies: Kio</title><link>/case-studies/kio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/kio/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Kio is a set of Kotlin extensions for Apache Beam to implement fluent-like API for Java SDK.&lt;/p>
&lt;h2 id="word-count-example">Word Count example&lt;/h2>
&lt;pre>&lt;code>// Create Kio context
val kio = Kio.fromArguments(args)
// Configure a pipeline
kio.read().text(&amp;quot;~/input.txt&amp;quot;)
.map { it.toLowerCase() }
.flatMap { it.split(&amp;quot;\\W+&amp;quot;.toRegex()) }
.filter { it.isNotEmpty() }
.countByValue()
.forEach { println(it) }
// And execute it
kio.execute().waitUntilDone()
&lt;/code>&lt;/pre>&lt;h2 id="documentation">Documentation&lt;/h2>
&lt;p>For more information about Kio, please see the documentation here: &lt;a href="https://code.chermenin.ru/kio">https://code.chermenin.ru/kio&lt;/a>.&lt;/p></description></item><item><title>Case-Studies: Klio</title><link>/case-studies/klio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/klio/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Developed at Spotify and built on top of Apache Beam for Python, Klio is an open source framework that lets researchers and engineers build smarter data pipelines for processing audio and other media files, easily and at scale.&lt;/p></description></item><item><title>Case-Studies: Scio</title><link>/case-studies/scio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/scio/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Scio is a Scala API for Apache Beam and Google Cloud Dataflow inspired by Apache Spark and Scalding.&lt;/p></description></item><item><title>Case-Studies: TensorFlow Extended (TFX)</title><link>/case-studies/tfx/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/tfx/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>&lt;strong>&lt;a href="https://www.tensorflow.org/tfx">TensorFlow Extended (TFX)&lt;/a>:&lt;/strong> TensorFlow Extended (TFX) is an end-to-end platform
for deploying production ML pipelines based on Apache Beam.&lt;/p></description></item><item><title>Case-Studies: The Nitty-Gritty of Moving Data with Beam</title><link>/case-studies/mozilla/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/case-studies/mozilla/</guid><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;div>
&lt;header class="case-study-header">
&lt;h2 itemprop="name headline">The Nitty-Gritty of Moving Data with Beam&lt;/h2>
&lt;/header>
&lt;p>Mozilla is the non-profit Firefox browser. This use case focuses on complexity that comes from moving data from one system to another safely, modeling data as it passes from one transform to another, handling errors, testing the system, and organizing the code to make the pipeline configurable for different source and destination systems in their open source codebase for ingesting telemetry data from Firefox clients.&lt;/p>
&lt;/div></description></item></channel></rss>