<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Apache Beam</title><description>Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.</description><link>/</link><generator>Hugo -- gohugo.io</generator><item><title>Apache Beam 2.53.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.53.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.53.0, check out the &lt;a href="https://github.com/apache/beam/milestone/17">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python streaming users that use 2.47.0 and newer versions of Beam should update to version 2.53.0, which fixes a known issue: (&lt;a href="https://github.com/apache/beam/issues/27330">#27330&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>TextIO now supports skipping multiple header lines (Java) (&lt;a href="https://github.com/apache/beam/issues/17990">#17990&lt;/a>).&lt;/li>
&lt;li>Python GCSIO is now implemented with GCP GCS Client instead of apitools (&lt;a href="https://github.com/apache/beam/issues/25676">#25676&lt;/a>)&lt;/li>
&lt;li>Adding support for LowCardinality DataType in ClickHouse (Java) (&lt;a href="https://github.com/apache/beam/pull/29533">#29533&lt;/a>).&lt;/li>
&lt;li>Added support for handling bad records to KafkaIO (Java) (&lt;a href="https://github.com/apache/beam/pull/29546">#29546&lt;/a>)&lt;/li>
&lt;li>Add support for generating text embeddings in MLTransform for Vertex AI and Hugging Face Hub models.(&lt;a href="https://github.com/apache/beam/pull/29564">#29564&lt;/a>)&lt;/li>
&lt;li>NATS IO connector added (Go) (&lt;a href="https://github.com/apache/beam/issues/29000">#29000&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Python SDK now type checks &lt;code>collections.abc.Collections&lt;/code> types properly. Some type hints that were erroneously allowed by the SDK may now fail. (&lt;a href="https://github.com/apache/beam/pull/29272">#29272&lt;/a>)&lt;/li>
&lt;li>Running multi-language pipelines locally no longer requires Docker.
Instead, the same (generally auto-started) subprocess used to perform the
expansion can also be used as the cross-language worker.&lt;/li>
&lt;li>Framework for adding Error Handlers to composite transforms added in Java (&lt;a href="https://github.com/apache/beam/pull/29164">#29164&lt;/a>).&lt;/li>
&lt;li>Python 3.11 images now include google-cloud-profiler (&lt;a href="https://github.com/apache/beam/pull/29651">#29561&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Upgraded to go 1.21.5 to build, fixing &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-45285">CVE-2023-45285&lt;/a> and &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-39326">CVE-2023-39326&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Euphoria DSL is deprecated and will be removed in a future release (not before 2.56.0) (&lt;a href="https://github.com/apache/beam/issues/29451">#29451&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>(Python) Fixed sporadic crashes in streaming pipelines that affected some users of 2.47.0 and newer SDKs (&lt;a href="https://github.com/apache/beam/issues/27330">#27330&lt;/a>).&lt;/li>
&lt;li>(Python) Fixed a bug that caused MLTransform to drop identical elements in the output PCollection (&lt;a href="https://github.com/apache/beam/issues/29600">#29600&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.53.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Arun Pandian&lt;/p>
&lt;p>Balázs Németh&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Calvin Swenson Jr&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Georgii Zemlianyi&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jacob Tomlinson&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julian Braha&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Lawrence Qiu&lt;/p>
&lt;p>Mark Zitnik&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Mike Williamson&lt;/p>
&lt;p>Naireen&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Niel Markwick&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Radosław Stankiewicz&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Talat UYARER&lt;/p>
&lt;p>Tom Stepp&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jiang&lt;/p>
&lt;p>clmccart&lt;/p>
&lt;p>damccorm&lt;/p>
&lt;p>darshan-sj&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>lrakla&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>tvalentyn&lt;/p></description><link>/blog/beam-2.53.0/</link><pubDate>Thu, 04 Jan 2024 09:00:00 -0400</pubDate><guid>/blog/beam-2.53.0/</guid><category>blog</category><category>release</category></item><item><title>Scaling a streaming workload on Apache Beam, 1 million events per second and beyond</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="scaling-a-streaming-workload-on-apache-beam">Scaling a streaming workload on Apache Beam&lt;/h1>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/0-intro.png"
alt="Streaming Processing">&lt;/p>
&lt;p>Scaling a streaming workload is critical for ensuring that a pipeline can process large amounts of data while also minimizing latency and executing efficiently. Without proper scaling, a pipeline may experience performance issues or even fail entirely, delaying the time to insights for the business.&lt;/p>
&lt;p>Given the Apache Beam support for the sources and sinks needed by the workload, developing a streaming pipeline can be easy. You can focus on the processing (transformations, enrichments, or aggregations) and on setting the right configurations for each case.&lt;/p>
&lt;p>However, you need to identify the key performance bottlenecks and make sure that the pipeline has the resources it needs to handle the load efficiently. This can involve right-sizing the number of workers, understanding the settings needed for the source and sinks of the pipeline, optimizing the processing logic, and even determining the transport formats.&lt;/p>
&lt;p>This article illustrates how to manage the problem of scaling and optimizing a streaming workload developed in Apache Beam and run on Google Cloud using Dataflow. The goal is to reach one million events per second, while also minimizing latency and resource use during execution. The workload uses Pub/Sub as the streaming source and BigQuery as the sink. We describe the reasoning behind the configuration settings and code changes we used to help the workload achieve the desired scale and beyond.&lt;/p>
&lt;p>The progression described in this article maps to the evolution of a real-life workload, with simplifications. After the initial business requirements for the pipeline were achieved, the focus shifted to optimizing the performance and reducing the resources needed for the pipeline execution.&lt;/p>
&lt;h2 id="execution-setup">Execution setup&lt;/h2>
&lt;p>For this article, we created a test suite that creates the necessary components for the pipelines to execute. You can find the code in &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests">this Github repository&lt;/a>. You can find the subsequent configuration changes that are introduced on every run in this &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/scaling-streaming-workload-blog">folder&lt;/a> as scripts that you can run to achieve similar results.&lt;/p>
&lt;p>All of the execution scripts can also execute a Terraform-based automation to create a Pub/Sub topic and subscription as well as a BigQuery dataset and table to run the workload. Also, it launches two pipelines: one data generation pipeline that pushes events to the Pub/Sub topic, and an ingestion pipeline that demonstrates the potential improvement points.&lt;/p>
&lt;p>In all cases, the pipelines start with an empty Pub/Sub topic and subscription and an empty BigQuery table. The plan is to generate one million events per second and, after a few minutes, review how the ingestion pipeline scales with time. The data being autogenerated is based on provided schemas or IDL (or Interface Description Language) given the configuration, and the goal is to have messages ranging between 800 bytes and 2 KB, adding up to approximately 1 GB/s volume throughput. Also, the ingestion pipelines are using the same worker type configuration on all runs (&lt;code>n2d-standard-4&lt;/code> GCE machines) and are capping the maximum workers number to avoid very large fleets.&lt;/p>
&lt;p>All of the executions run on Google Cloud using Dataflow, but you can apply all of the configurations and format changes to the suite while executing on other supported Apache Beam runners. Changes and recommendations are not runner specific.&lt;/p>
&lt;h3 id="local-environment-requirements">Local environment requirements&lt;/h3>
&lt;p>Before launching the startup scripts, install the following items in your local environment:&lt;/p>
&lt;ul>
&lt;li>&lt;code>gcloud&lt;/code>, along with the correct permissions&lt;/li>
&lt;li>Terraform&lt;/li>
&lt;li>JDK 17 or later&lt;/li>
&lt;li>Maven 3.6 or later&lt;/li>
&lt;/ul>
&lt;p>For more information, see the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests#requisites">requirements&lt;/a> section in the GitHub repository.&lt;/p>
&lt;p>Also, review the service quotas and resources available in your Google Cloud project. Specifically: Pub/Sub regional capacity, BigQuery ingestion quota, and Compute Engine instances available in the selected region for the tests.&lt;/p>
&lt;h3 id="workload-description">Workload description&lt;/h3>
&lt;p>Focusing on the ingestion pipeline, our &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/canonical-streaming-pipelines/src/main/java/com/google/cloud/pso/beam/pipelines/StreamingSourceToBigQuery.java#L55">workload&lt;/a> is straightforward. It completes the following steps:&lt;/p>
&lt;ol>
&lt;li>reads data in a specific format from Pub/Sub (Apache Thrift in this case)&lt;/li>
&lt;li>deals with potential compression and batching settings (not enabled by default)&lt;/li>
&lt;li>executes a UDF (identity function by default)&lt;/li>
&lt;li>transforms the input format to one of the formats supported by the &lt;code>BigQueryIO&lt;/code> transform&lt;/li>
&lt;li>writes the data to the configured table&lt;/li>
&lt;/ol>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/0-pipeline.png"
alt="Example Workload">&lt;/p>
&lt;p>The pipeline we used for the tests is highly configurable. For more details about how to tweak the ingestion, see the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/canonical-streaming-pipelines/src/main/java/com/google/cloud/pso/beam/pipelines/StreamingSourceToBigQuery.java#L39">options&lt;/a> in the file. No code changes are needed on any of our steps. The execution scripts take care of the configurations needed.&lt;/p>
&lt;p>Although these tests are focused on reading data from Pub/Sub, the ingestion pipeline is capable of reading data from a generic streaming source. The repository contains other &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/example-suite-scripts">examples&lt;/a> that show how to launch this same test suite reading data from Pub/Sub Lite and Kafka. In all cases, the pipeline automation sets up the streaming infrastructure.&lt;/p>
&lt;p>Finally, you can see in the &lt;a href="https://github.com/prodriguezdefino/apache-beam-ptransforms/blob/a0dd229081625c7b593512543614daf995a9f870/common/src/main/java/com/google/cloud/pso/beam/common/formats/options/TransportFormatOptions.java">configuration options&lt;/a> that the pipeline supports many transport format options for the input, such as Thrift, Avro, and JSON. This suite focuses on Thrift, because it is a common open source format, and because it generates a format transformation need. The intent is to put some strain in the workload processing. You can run similar tests for Avro and JSON input data. The streaming data generator pipeline can generate random data for the &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/tree/main/streaming-data-generator/src/main/java/com/google/cloud/pso/beam/generator/formats">three supported formats&lt;/a> by walking directly on the schema (Avro and JSON) or IDL (Thrift) provided for execution.&lt;/p>
&lt;h2 id="first-run-default-settings">First run: default settings&lt;/h2>
&lt;p>The default values for the execution writes the data to BigQuery using &lt;code>STREAMING_INSERTS&lt;/code> mode for &lt;code>BigQueryIO&lt;/code>. This mode correlates with the &lt;a href="https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll">&lt;code>tableData insertAll&lt;/code> API&lt;/a> for BigQuery. This API supports data in JSON format. From the Apache Beam perspective, using the &lt;code>BigQueryIO.writeTableRows&lt;/code> method lets us resolve the writes into BigQuery.&lt;/p>
&lt;p>For our ingestion pipeline, the Thrift format needs to be transformed into &lt;code>TableRow&lt;/code>. To do that, we need to translate the Thrift IDL into a BigQuery table schema. That can be achieved by translating the Thrift IDL into an Avro schema, and then using Beam utilities to translate the table schema for BigQuery. We can do this at bootstrap. The schema transformation is cached at the &lt;code>DoFn&lt;/code> level.&lt;/p>
&lt;p>After setting up the data generation and ingestion pipelines, and after letting the pipelines run for some minutes, we see that the pipeline is unable to sustain the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/1-default-ps.png"
alt="PubSub metrics">&lt;/p>
&lt;p>The previous image shows that the number of messages that are not being processed by the ingestion pipeline start to show as unacknowledged messages in Pub/Sub metrics.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/1-default-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Reviewing the per stage performance metrics, we see that the pipeline shows a saw-like shape, which is often associated with the throttling mechanisms the Dataflow runner uses when some of the stages are acting as bottlenecks for the throughput. Also, we see that the &lt;code>Reshuffle&lt;/code> step on the &lt;code>BigQueryIO&lt;/code> write transform does not scale as expected.&lt;/p>
&lt;p>This behavior happens because by default the &lt;a href="https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java#L57">&lt;code>BigQueryOptions&lt;/code>&lt;/a> uses 50 different keys to shuffle data to workers before the writes happen on BigQuery. To solve this problem, we can add a configuration to our launch script that enables the write operations to scale to a larger number of workers, which improves performance.&lt;/p>
&lt;h2 id="second-run-improve-the-write-bottleneck">Second run: improve the write bottleneck&lt;/h2>
&lt;p>After increasing the number of streaming keys to a higher number, 512 keys in our case, we restarted the test suite. The Pub/Sub metrics started to improve. After an initial ramp on the size of the backlog, the curve started to ease out.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/2-skeys-ps.png"
alt="PubSub metrics">&lt;/p>
&lt;p>This is good, but we should take a look at the throughput per stage numbers to understand if we are achieving the goal we set up for this exercise.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/2-skeys-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Although the performance has clearly improved, and the Pub/Sub backlog no longer increases monotonically, we are still far from the goal of processing one million events per second (1 GB/s) for our ingestion pipeline. In fact, the throughput metrics jump all over, indicating that bottlenecks are preventing the processing from scaling further.&lt;/p>
&lt;h2 id="third-run-unleash-autoscale">Third run: unleash autoscale&lt;/h2>
&lt;p>Luckily for us, when writing into BigQuery, we can autoscale the writes. This step simplifies the configuration so that we don&amp;rsquo;t have to guess the right number of shards. We switched the pipeline’s configuration and enabled this setting for the next &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests/blob/main/scaling-streaming-workload-blog/3-ps2bq-si-tr-streamingautoshard.sh">launch script&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-parallelism.png"
alt="Key Parallelism">&lt;/p>
&lt;p>Immediately, we see that the autosharding mechanism tweaks the number of keys very aggressively and in a dynamic way. This change is good, because different moments in time might have different scale needs, such as early backlog recoveries and spikes in the execution.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-throughput-tr.png"
alt="Throughput">&lt;/p>
&lt;p>Inspecting the throughput performance per stage, we see that as the number of keys increases, the performance of the writes also increases. In fact, it reaches very large numbers!&lt;/p>
&lt;p>After the initial backlog was consumed and the pipeline stabilized, we saw that the desired performance numbers were reached. The pipeline can sustain processing many more than a million events per second from Pub/Sub and several GB/s of BigQuery ingestion. Yay!&lt;/p>
&lt;p>Still, we want to see if we can do better. We can introduce several improvements to the pipeline to make the execution more efficient. In most cases, the improvements are configuration changes. We just need to know where to focus next.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-autoscale.png"
alt="Resources">&lt;/p>
&lt;p>The previous image shows that the number of workers needed to sustain this throughput is still quite high. The workload itself is not CPU intensive. Most of the cost is spent on transforming formats and on I/O interactions, such as shuffles and the actual writes. To understand what to improve, we first investigate the transport formats.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-input.png"
alt="Thrift Input Size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-output.png"
alt="TableRow Output Size">&lt;/p>
&lt;p>Looking at the input size, right before the identity UDF execution, the data format is binary Thrift, which is a decently compact format even when no compression is used. However, while comparing the &lt;code>PCollection&lt;/code> approximated size with the &lt;code>TableRow&lt;/code> format needed for BigQuery ingestion, a clear size increase is visible. This is something we can improve by changing the BigQuery write API in use.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/3-autoshard-tr-overhead.png"
alt="Translation Overhead">&lt;/p>
&lt;p>When we inspect the &lt;code>StoreInBigQuery&lt;/code> transform, we see that the majority of the wall time is spent on the actual writes. Also, the wall time spent converting data to the destination format (&lt;code>TableRows&lt;/code>) compared with how much is spent in the actual writes is quite large: 13 times bigger for the writes. To improve this behavior, we can switch the pipeline write mode.&lt;/p>
&lt;h2 id="fourth-run-in-with-the-new">Fourth run: in with the new&lt;/h2>
&lt;p>In this run, we use the &lt;code>StorageWrite&lt;/code> API. Enabling the &lt;code>StorageWrite&lt;/code> API for this pipeline is straightforward. We set the write mode as &lt;code>STORAGE_WRITE_API&lt;/code> and define a write triggering frequency. For this test, we write data at most every ten seconds. The write triggering frequency controls how long the per-stream data accumulate. A higher number defines a larger output to be written after the stream assignment but also imposes a larger end-to-end latency for every element read from Pub/Sub. Similar to the &lt;code>STREAMING_WRITES&lt;/code> configuration, &lt;code>BigQueryIO&lt;/code> can handle autosharding for the writes, which we already demonstrated to be the best setting for performance.&lt;/p>
&lt;p>After both pipelines become stable, the performance benefits seen when using the &lt;code>StorageWrite&lt;/code> API in &lt;code>BigQueryIO&lt;/code> are apparent. After enabling the new implementation, the wall time rate between the format transformation and write operation decreases. The wall time spent on writes is only about 34 percent larger than the format transformation.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-format-transformation.png"
alt="Translation Overhead">&lt;/p>
&lt;p>After stabilization, the pipeline throughput is also quite smooth. The runner can quickly and steadily downscale the pipeline resources needed to sustain the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-throughput.png"
alt="Throughput">&lt;/p>
&lt;p>Looking at the resource scale needed to process the data, another dramatic improvement is visible. Whereas the streaming inserts-based pipeline needed more than 80 workers to sustain the throughput, the storage writes pipeline only needs 49, a 40 percent improvement.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/4-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>We can use the data generation pipeline as reference. This pipeline only needs to randomly generate data and write the events to Pub/Sub. It runs steadily with an average of 40 workers. The improvements on the ingestion pipeline using the right configuration for the workload makes it closer to those resources needed for the generation.&lt;/p>
&lt;p>Similar to the streaming inserts-based pipeline, writing the data into BigQuery requires running a format translation, from Thrift to &lt;code>TableRow&lt;/code> in the former and from Thrift to Protocol Buffers (protobuf) in the latter. Because we are using the &lt;code>BigQueryIO.writeTableRows&lt;/code> method, we add another step in the format translation. Because the &lt;code>TableRow&lt;/code> format also increases the size of the &lt;code>PCollection&lt;/code> being processed, we want to see if we can improve this step.&lt;/p>
&lt;h2 id="fifth-run-a-better-write-format">Fifth run: a better write format&lt;/h2>
&lt;p>When using &lt;code>STORAGE_WRITE_API&lt;/code>, the &lt;code>BigQueryIO&lt;/code> transform exposes a method that we can use to write the Beam row type directly into BigQuery. This step is useful because of the flexibility that the row type provides for interoperability and schema management. Also, it&amp;rsquo;s both efficient for shuffling and denser than &lt;code>TableRow&lt;/code>, so our pipeline will have smaller &lt;code>PCollection&lt;/code> sizes.&lt;/p>
&lt;p>For the next run, because our data volume is not small, we decrease the triggering frequency when writing to BigQuery. Because we use a different format, slightly different code runs. For this change, the test pipeline script is configured with the flag &lt;code>--formatToStore=BEAM_ROW&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-input-size.png"
alt="Thrift input size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-output-size.png"
alt="Row output size">&lt;/p>
&lt;p>The &lt;code>PCollection&lt;/code> size written into BigQuery is considerably smaller than on previous executions. In fact, for this particular execution, the Beam row format is a smaller size than the Thrift format. A larger &lt;code>PCollection&lt;/code> conformed by bigger per-element sizes can put nontrivial memory pressure in smaller worker configurations, reducing the overall throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-format-trasformation.png"
alt="Translation overhead">&lt;/p>
&lt;p>The wall clock rate for the format transformation and the actual BigQuery writes also maintain a very similar rate. Handling the Beam row format does not impose a performance penalty in the format translation and subsequent writes. This is confirmed by the number of workers in use by the pipeline when throughput becomes stable, slightly smaller than the previous run but clearly in the same range.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/5-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>Although we are in a much better position than when we started, given our test pipeline input format, there&amp;rsquo;s still room for improvement.&lt;/p>
&lt;h2 id="sixth-run-further-reduce-the-format-translation-effort">Sixth run: further reduce the format translation effort&lt;/h2>
&lt;p>Another supported format for the input &lt;code>PCollection&lt;/code> in the &lt;code>BigQueryIO&lt;/code> transform might be advantageous for our input format. The method &lt;code>writeGenericRecords&lt;/code> enables the transform to transform Avro &lt;code>GenericRecords&lt;/code> directly into protobuf before the write operation. Apache Thrift can be transformed into Avro &lt;code>GenericRecords&lt;/code> very efficiently. We can make another test run configuring our test ingestion pipeline by setting the option &lt;code>--formatToStore=AVRO_GENERIC_RECORD&lt;/code> on our execution script.&lt;/p>
&lt;p>This time, the difference between format translation and writes increases significantly, improving performance. The translation to Avro &lt;code>GenericRecords&lt;/code> is only 20 percent of the write effort spent on writing those records into BigQuery. Given that the test pipelines had similar runtimes and that the wall clock seen in the &lt;code>WriteIntoBigQuery&lt;/code> stage is also aligned with other &lt;code>StorageWrite&lt;/code> related runs, using this format is appropriate for this workload.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-format-transformation.png"
alt="Translation overhead">&lt;/p>
&lt;p>We see further gains when we look at resource utilization. We need less CPU time to execute the format translations for our workload while achieving the desired throughput.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-ingestion-scale.png"
alt="Resources">&lt;/p>
&lt;p>This pipeline improves upon the previous run, running steadily on 42 workers when throughput is stable. Given the worker configuration used (&lt;code>nd2-standard-4&lt;/code>), and the volume throughput of the workload process (about 1 GB/s), we are achieving about 6 MB/s throughput per CPU core, which is quite impressive for a streaming pipeline with exactly-once semantics.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/6-latencies.png"
alt="Latencies">&lt;/p>
&lt;p>When we add up all of the stages executed in the main path of the pipeline, the latency seen at this scale achieves sub-second end-to-end latencies during sustained periods of time.&lt;/p>
&lt;p>Given the workload requirements and the implemented pipeline code, this performance is the best that we can extract without further tuning the runner’s specific settings.&lt;/p>
&lt;h2 id="seventh-run--lets-just-relax-at-least-some-constraints">Seventh run : lets just relax (at least some constraints)&lt;/h2>
&lt;p>When using the &lt;code>STORAGE\_WRITE\_API&lt;/code> setting for &lt;code>BigQueryIO&lt;/code>, we enforce exactly-once semantics on the writes. This configuration is great for use cases that need strong consistency on the data that gets processed, but it imposes a performance and cost penalty.&lt;/p>
&lt;p>From a high-level perspective, writes into BigQuery are made in batches, which are released based on the current sharding and the triggering frequency. If a write fails during the execution of a particular bundle, it is retried. A bundle of data is committed into BigQuery only when all the data in that particular bundle is correctly appended to a stream. This implementation needs to shuffle the full volume of data to create the batches that are written, and also the information of the finished batches for later commit (although this last piece is very small compared with the first).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-previous-data-input.png"
alt="Read data size">&lt;/p>
&lt;p>Looking at the previous pipeline execution, the total data being processed for the pipeline by Streaming Engine is larger than the data being read from Pub/Sub. For example, 7 TB of data is read from Pub/Sub, whereas the processing of data for the whole execution of the pipeline moves 25 TB of data to and from Streaming Engine.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-previous-shuffle-total.png"
alt="Streamed data size">&lt;/p>
&lt;p>When data consistency is not a hard requirement for ingestion, you can use at-least-once semantics with &lt;code>BigQueryIO&lt;/code> write mode. This implementation avoids shuffling and grouping data for the writes. However, this change might cause a small number of repeated rows to be written into the destination table. This can happen with append errors, infrequent worker restarts, and other even less frequent errors.&lt;/p>
&lt;p>Therefore, we add the configuration to use &lt;code>STORAGE_API_AT_LEAST_ONCE&lt;/code> write mode. To instruct the &lt;code>StorageWrite&lt;/code> client to reuse connections while writing data, we also add the configuration flag &lt;code>–useStorageApiConnectionPool&lt;/code>. This configuration option only works with &lt;code>STORAGE_API_AT_LEAST_ONCE&lt;/code> mode, and it reduces the occurrences of warnings similars to &lt;code>Storage Api write delay more than 8 seconds&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-resources.png"
alt="Resources">&lt;/p>
&lt;p>When pipeline throughput stabilizes, we see a similar pattern for resource utilization for the workload. The number of workers in use reaches 40, a small improvement compared with the last run. However, the amount of data being moved from Streaming Engine is much closer to the amount of data read from Pub/Sub.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-current-input.png"
alt="Read data size">
&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-current-shuffle-total.png"
alt="Streamed data size">&lt;/p>
&lt;p>Considering all of these factors, this change further optimizes the workload, achieving a throughput of 6.4 MB/s per CPU core. This improvement is small compared to the same workload when using consistent writes into BigQuery, but it uses less streaming data resources. This configuration represents the most optimal setup for our workload, with the highest throughput per resource and the lowest streaming data across workers.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/scaling-streaming-workload/7-latency.png"
alt="Streamed data size">&lt;/p>
&lt;p>This configuration also has impressively low latency for the end-to-end processing. Given that the main path of our pipeline has been fused in a single execution stage from reads to writes, we see that even at p99, the latency tends to be below 300 milliseconds at a quite large volume throughput (as previously mentioned around 1 GB/s).&lt;/p>
&lt;h2 id="recap">Recap&lt;/h2>
&lt;p>Optimizing Apache Beam streaming workloads for low latency and efficient execution requires careful analysis and decision-making, and the right configurations.&lt;/p>
&lt;p>Considering the scenario discussed in this article, it is essential to consider factors like overall CPU utilization, throughput and latency per stage, &lt;code>PCollection&lt;/code> sizes, wall time per stage, write mode, and transport formats, in addition to writing the right pipeline for the workload.&lt;/p>
&lt;p>Our experiments revealed that using the &lt;code>StorageWrite&lt;/code> API, autosharding for writes, and Avro &lt;code>GenericRecords&lt;/code> as the transport format yielded the most efficient results. Relaxing the consistency for writes can further improve performance.&lt;/p>
&lt;p>The accompanying &lt;a href="https://github.com/prodriguezdefino/apache-beam-streaming-tests">Github repository&lt;/a> contains a test suite that you can use to replicate the analysis on your Google Cloud project or with a different runner setup. Feel free to take it for a spin. Comments and PRs are always welcome.&lt;/p></description><link>/blog/scaling-streaming-workload/</link><pubDate>Wed, 03 Jan 2024 00:00:01 -0800</pubDate><guid>/blog/scaling-streaming-workload/</guid><category>blog</category></item><item><title>Build a scalable, self-managed streaming infrastructure with Beam and Flink: Tackling Autoscaling Challenges - Part 2</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="build-a-scalable-self-managed-streaming-infrastructure-with-flink-tackling-autoscaling-challenges---part-2">Build a scalable, self-managed streaming infrastructure with Flink: Tackling Autoscaling Challenges - Part 2&lt;/h1>
&lt;p>Welcome to Part 2 of our in-depth series about building and managing a service for Apache Beam Flink on Kubernetes. In this segment, we&amp;rsquo;re taking a closer look at the hurdles we encountered while implementing autoscaling. These challenges weren&amp;rsquo;t just roadblocks. They were opportunities for us to innovate and enhance our system. Let’s break down these issues, understand their context, and explore the solutions we developed.&lt;/p>
&lt;h2 id="understand-apache-beam-backlog-metrics-in-the-flink-runner-environment">Understand Apache Beam backlog metrics in the Flink runner environment&lt;/h2>
&lt;p>&lt;strong>The Challenge:&lt;/strong> In our current setup, we are using Apache Flink for processing data streams. However, we&amp;rsquo;ve encountered a puzzling issue: our Flink job isn&amp;rsquo;t showing the backlog metrics from Apache Beam. These metrics are critical for understanding the state and performance of our data pipelines.&lt;/p>
&lt;p>&lt;strong>What We Found:&lt;/strong> Interestingly, we noticed that the metrics are actually being generated in &lt;code>KafkaIO&lt;/code>, which is a part of our data pipeline that handles Kafka streams. But when we try to monitor these metrics through the Apache Flink Metric system, we can&amp;rsquo;t find them. We suspected that there might be an issue with the integration (or &amp;lsquo;wiring&amp;rsquo;) between Apache Beam and Apache Flink.&lt;/p>
&lt;p>&lt;strong>Digging Deeper:&lt;/strong> On closer inspection, we found that the metrics should be emitted during the &amp;lsquo;Checkpointing&amp;rsquo; phase of the data stream processing. During this crucial step, the system takes a snapshot of the stream&amp;rsquo;s state, and the metrics are typically metrics that are generated for unbounded sources. Unbounded sources are sources that continuously stream data, like Kafka.&lt;/p>
&lt;p>&lt;strong>A Potential Solution:&lt;/strong> We believe the root of the problem lies in how the metric context is set during the checkpointing phase. A disconnect appears to prevent the Beam metrics from being properly captured in the Flink Metric system. We proposed a fix for this issue, which you can review and contribute to on our GitHub pull request: &lt;a href="https://github.com/apache/beam/pull/29793">Apache Beam PR #29793&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/flink-backlog-metrics.png"
alt="Apache Flink Beam Backlog Metrics">&lt;/p>
&lt;h2 id="overcoming-challenges-in-checkpoint-size-reduction-for-autoscaling-beam-jobs">Overcoming challenges in checkpoint size reduction for autoscaling Beam jobs&lt;/h2>
&lt;p>In this section we will discuss strategies for reducing the size of checkpoints in autoscaling Apache Beam jobs, focusing on efficient checkpointing in Apache Flink and optimizing bundle sizes and PipelineOptions to manage frequent checkpoint timeouts and large-scale job requirements.&lt;/p>
&lt;h3 id="understand-the-basics-of-checkpointing-in-apache-flink">Understand the basics of checkpointing in Apache Flink&lt;/h3>
&lt;p>In stream processing, maintaining state consistency and fault tolerance is crucial. Apache Flink achieves this through a process called &lt;em>checkpointing&lt;/em>. Checkpointing periodically captures the state of a job&amp;rsquo;s operators and stores it in a stable storage location, like Google Cloud Storage or AWS S3. Specifically, Flink checkpoints a job every ten seconds and allows up to one minute for this process to complete. This process is vital for ensuring that, in case of failures, the job can resume from the last checkpoint, providing exactly-once semantics and fault tolerance.&lt;/p>
&lt;h3 id="the-role-of-bundles-in-apache-beam">The role of bundles in Apache Beam&lt;/h3>
&lt;p>Apache Beam introduces the concept of a &lt;em>bundle&lt;/em>. A bundle is essentially a group of elements that are processed together. This step enhances processing efficiency and throughput by reducing the overhead of handling each element separately. For more information, see &lt;a href="https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence">Bundling and persistence&lt;/a>. In the Flink runner &lt;a href="https://beam.apache.org/releases/javadoc/2.52.0/org/apache/beam/runners/flink/FlinkPipelineOptions.html#getMaxBundleSize--">default configuration&lt;/a>, a bundle&amp;rsquo;s default size is 1000 elements with a one-second timeout. However, based on our performance tests, we adjusted the bundle size to &lt;em>10,000 elements with a 10-second timeout&lt;/em>.&lt;/p>
&lt;h3 id="challenge-frequent-checkpoint-timeouts">Challenge: frequent checkpoint timeouts&lt;/h3>
&lt;p>When we configured checkpointing every 10 seconds, we faced frequent checkpoint timeouts, often exceeding 1 minute. This was due to the large size of the checkpoints.&lt;/p>
&lt;h3 id="solution-manage-checkpoint-size">Solution: Manage checkpoint size&lt;/h3>
&lt;p>In Apache Beam Flink jobs, the &lt;code>finishBundleBeforeCheckpointing&lt;/code> option plays a pivotal role. When enabled, it ensures that all bundles are completely processed before initiating a checkpoint. This results in checkpoints that only contain the state post-bundle completion, significantly reducing checkpoint size. Initially, our checkpoints were around 2 MB per pipeline. With this change, they consistently dropped to 150 KB.&lt;/p>
&lt;h3 id="address-the-checkpoint-size-in-large-scale-jobs">Address the checkpoint size in large-scale jobs&lt;/h3>
&lt;p>Despite reducing checkpoint sizes, a 150 KB checkpoint every ten seconds can still be substantial, especially in jobs that run multiple pipelines. For instance, with 100 pipelines in a single job, this size balloons to 15 MB per 10-second interval.&lt;/p>
&lt;h3 id="further-optimization-reduce-checkpoint-size-with-pipelineoptions">Further optimization: reduce checkpoint size with PipelineOptions&lt;/h3>
&lt;p>We discovered that due to a specific issue (BEAM-8577), our Flink runner was including our large &lt;code>PipelineOptions&lt;/code> objects in every checkpoint. We solved this problem by removing unnecessary application-related options from &lt;code>PipelineOptions&lt;/code>, further reducing the checkpoint size to a more manageable 10 KB per pipeline.&lt;/p>
&lt;h2 id="kafka-reader-wait-time-solving-autoscaling-challenges-in-beam-jobs">Kafka Reader wait time: solving autoscaling challenges in Beam jobs&lt;/h2>
&lt;h3 id="understand-unaligned-checkpointing">Understand unaligned checkpointing&lt;/h3>
&lt;p>In our system, we use unaligned checkpointing to speed up the process of checkpointing, which is essential for ensuring data consistency in distributed systems. However, when we activated the &lt;code>finishBundleBeforeCheckpointing&lt;/code> feature, we began facing checkpoint timeout issues and delays in checkpointing steps. Apache Beam leverages Apache Flink&amp;rsquo;s legacy source implementation for processing unbounded sources. In Flink, tasks are categorized into two types: source tasks and non-source tasks.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Source tasks&lt;/strong>: fetch data from external systems into a Flink job&lt;/li>
&lt;li>&lt;strong>Non-source tasks&lt;/strong>: process the incoming data&lt;/li>
&lt;/ul>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/kafkaio-wait-reader.png"
alt="Apache Flink Task Types">&lt;/p>
&lt;p>In the standard configuration, non-source tasks check for an available buffer before pulling data. If source tasks don&amp;rsquo;t perform this check, they might experience checkpointing delays in writing data to the output buffer. This delay affects the efficiency of unaligned checkpoints, which are only recognized by legacy source tasks when an output buffer is available.&lt;/p>
&lt;h3 id="address-the-challenge-with-unboundedsourcewrapper-in-beam">Address the challenge with UnboundedSourceWrapper in Beam&lt;/h3>
&lt;p>To solve this problem, Apache Flink introduced a new source implementation that operates in a pull mode. In this mode, a task checks for a free buffer before fetching data, aligning with the approach of non-source tasks.&lt;/p>
&lt;p>However, the legacy source, still used by Apache Beam&amp;rsquo;s Flink Runner, operates in a push mode. It sends data to downstream tasks immediately. This setup might create bottlenecks when buffers are full, causing delays in detecting unaligned checkpoint barriers.&lt;/p>
&lt;h3 id="our-solution">Our solution&lt;/h3>
&lt;p>Despite its deprecation, Apache Beam&amp;rsquo;s Flink Runner still uses the legacy source implementation. To address its issues, we implemented our modifications and the quick workarounds suggested in &lt;a href="https://issues.apache.org/jira/browse/FLINK-26759">FLINK-26759&lt;/a>. These enhancements are detailed in our &lt;a href="#">Pull Request&lt;/a>. You can also find more information about unaligned checkpoint issues in the &lt;a href="https://blog.51cto.com/u_14286418/7000028">Flink Unaligned Checkpoint&lt;/a> blog post.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/checkpoint_monitoring-history-subtasks.png"
alt="Apache Flink UI Checkpoint History">&lt;/p>
&lt;h2 id="address-slow-reads-in-high-traffic-scenarios">Address slow reads in high-traffic scenarios&lt;/h2>
&lt;p>In our journey with Apache Beam and the Flink Runner, we encountered a significant challenge similar to one documented in the post &lt;a href="https://antonio-si.medium.com/how-intuit-debug-consumer-lags-in-apache-beam-22ca3b39602e">How Intuit Debug Consumer Lags in Apache Beam&lt;/a> by &lt;a href="https://antonio-si.medium.com/">Antonio Si&lt;/a> in his experience at Intuit. Their real-time data processing pipelines had increasing Kafka consumer lag, particularly with topics experiencing high message traffic. This issue was traced to Apache Beam&amp;rsquo;s handling of Kafka partitions through &lt;code>UnboundedSourceWrapper&lt;/code> and &lt;code>KafkaUnboundedReader&lt;/code>. Specifically, for topics with lower traffic, the processing thread paused unnecessarily, delaying the processing of high-traffic topics. We faced a parallel situation in our system, where the imbalance in processing speeds between high- and low-traffic topics led to inefficiencies.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/adaptive-timeout-kafka.png"
alt="UnboundedSourceWrapper Design">&lt;/p>
&lt;p>To resolve this issue, we developed an innovative solution: an adaptive timeout strategy in &lt;code>KafkaIO&lt;/code>. This strategy dynamically adjusts the timeout duration based on the traffic of each topic. For low-traffic topics, it shortens the timeout, preventing unnecessary delays. For high-traffic topics, it extends the timeout, providing more processing opportunities. This approach is detailed in our recent pull request.&lt;/p>
&lt;h2 id="unbalanced-partition-distribution-in-beam-job-autoscaling">Unbalanced partition distribution in Beam job autoscaling&lt;/h2>
&lt;p>At the heart of this system is the adaptive scheduler, a component designed for rapid resource allocation. It intelligently adjusts the number of parallel tasks (parallelism) a job performs based on the availability of computing slots. These slots are like individual workstations, each capable of handling certain parts of the job.&lt;/p>
&lt;p>However, we encountered a problem. Our jobs consist of multiple independent pipelines, each needing its own set of resources. Initially, the system tended to overburden the first few workers by assigning them more tasks, while others remained underutilized. This issue was due to the way Flink allocated tasks, favoring the first workers for each pipeline.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes-part2/flink-partition-assignment.png"
alt="Flink split assignment on slots">&lt;/p>
&lt;p>To address this issue, we developed a custom patch for Flink&amp;rsquo;s &lt;em>SlotSharingSlotAllocator&lt;/em>, a component responsible for task distribution. This patch ensures a more balanced workload distribution across all available workers, improving efficiency and preventing bottlenecks.
With this improvement, each worker gets a fair share of tasks, leading to better resource utilization and smoother operation of our Beam Jobs.&lt;/p>
&lt;h2 id="drain-support-in-kubernetes-operator-with-flink">Drain support in Kubernetes Operator with Flink&lt;/h2>
&lt;h3 id="the-challenge">The challenge&lt;/h3>
&lt;p>In the world of data processing with Apache Flink, a common task is to manage and update data-processing jobs. These jobs could be either stateful, where they remember past data, or stateless, where they don&amp;rsquo;t.&lt;/p>
&lt;p>In the past, when we needed to update or delete a Flink job managed by the Kubernetes Operator, the system saved the current state of the job using a savepoint or checkpoint. However, a crucial step was missing: the system didn&amp;rsquo;t stop the job from processing new data (this is what we mean by draining the job). This oversight could lead to two major issues:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>For stateful jobs:&lt;/strong> potential data inconsistencies, because the job might process new data that wasn&amp;rsquo;t accounted for in the savepoint&lt;/li>
&lt;li>&lt;strong>For stateless jobs:&lt;/strong> data duplication, because the job might reprocess data it already processed&lt;/li>
&lt;/ol>
&lt;h3 id="the-solution-drain-function">The solution: drain function&lt;/h3>
&lt;p>This is where the update referenced as &lt;a href="https://issues.apache.org/jira/browse/FLINK-32700">FLINK-32700&lt;/a> is needed. This update introduced a drain function. Think of it as telling the job, &amp;ldquo;Finish what you&amp;rsquo;re currently processing, but don&amp;rsquo;t take on anything new.&amp;rdquo; Here&amp;rsquo;s how it works:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Stop new data:&lt;/strong> The job stops reading new input.&lt;/li>
&lt;li>&lt;strong>Mark the source:&lt;/strong> The job marks the source with an infinite watermark. Think of this watermark as a marker that tells the system that there&amp;rsquo;s no more new data to process.&lt;/li>
&lt;li>&lt;strong>Propagate through the pipeline:&lt;/strong> This marker is then passed through the job&amp;rsquo;s processing pipeline, ensuring that every part of the job knows not to expect any new data.&lt;/li>
&lt;/ol>
&lt;p>This seemingly small change has a big impact. It ensures that when a job is updated or deleted, the data it processes remains consistent and accurate. This is crucial for any data-processing task, because it maintains the integrity and reliability of the data. Furthermore, in cases where the drainage fails, you can cancel the job without needing a savepoint, which adds a layer of flexibility and safety to the whole process.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>As we conclude Part 2 of our series on building and managing Apache Beam Flink services on Kubernetes, it&amp;rsquo;s evident that the journey of implementing autoscaling has been both challenging and enlightening. The obstacles we faced, from understanding Apache Beam backlog metrics in the Flink Runner environment to addressing slow reads in high-traffic scenarios, pushed us to develop innovative solutions and deepen our understanding of streaming infrastructure.&lt;/p>
&lt;p>Our exploration into the intricacies of checkpointing, Kafka Reader wait times, and unbalanced partition distribution revealed the complexities of autoscaling Beam jobs. These challenges prompted us to devise strategies like the adaptive timeout in &lt;code>KafkaIO&lt;/code> and the balanced workload distribution in Flink&amp;rsquo;s &lt;code>SlotSharingSlotAllocator&lt;/code>. Additionally, the introduction of the drain support in Kubernetes Operator with Flink marks a significant advancement in managing stateful and stateless jobs effectively.&lt;/p>
&lt;p>This journey has not only enhanced the robustness and efficiency of our system but has also contributed valuable insights to the broader community working with Apache Beam and Flink. We hope that our experiences and solutions will aid others facing similar challenges in their projects.&lt;/p>
&lt;p>Stay tuned for our next blog post, where we&amp;rsquo;ll delve into the specifics of autoscaling in Apache Beam. We&amp;rsquo;ll break down the concepts, strategies, and best practices to effectively scale your Beam jobs. Thank you for following our series, and we look forward to sharing more of our journey and learnings with you.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed, Flink-based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitk Dani, and David He.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>Explore More:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/blog/apache-beam-flink-and-kubernetes/">Part 1: Introduction to Building and Managing Apache Beam Flink Services on Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Join the conversation and share your experiences on our &lt;a href="https://beam.apache.org/community/">Community&lt;/a> or contribute to our ongoing projects on &lt;a href="https://github.com/apache/beam">GitHub&lt;/a>. Your feedback is invaluable. If you have any comments or questions about this series, please feel free to reach out to us via &lt;a href="https://beam.apache.org/community/contact-us/">User Mailist&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;em>Stay connected with us for more updates and insights into Apache Beam, Flink, and Kubernetes.&lt;/em>&lt;/p></description><link>/blog/apache-beam-flink-and-kubernetes-part2/</link><pubDate>Mon, 18 Dec 2023 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes-part2/</guid><category>blog</category></item><item><title>Apache Beam 2.52.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.52.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2520-2023-11-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.52.0, check out the &lt;a href="https://github.com/apache/beam/milestone/16">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Previously deprecated Avro-dependent code (Beam Release 2.46.0) has been finally removed from Java SDK &amp;ldquo;core&amp;rdquo; package.
Please, use &lt;code>beam-sdks-java-extensions-avro&lt;/code> instead. This will allow to easily update Avro version in user code without
potential breaking changes in Beam &amp;ldquo;core&amp;rdquo; since the Beam Avro extension already supports the latest Avro versions and
should handle this. (&lt;a href="https://github.com/apache/beam/issues/25252">#25252&lt;/a>).&lt;/li>
&lt;li>Publishing Java 21 SDK container images now supported as part of Apache Beam release process. (&lt;a href="https://github.com/apache/beam/issues/28120">#28120&lt;/a>)
&lt;ul>
&lt;li>Direct Runner and Dataflow Runner support running pipelines on Java21 (experimental until tests fully setup). For other runners (Flink, Spark, Samza, etc) support status depend on runner projects.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Add &lt;code>UseDataStreamForBatch&lt;/code> pipeline option to the Flink runner. When it is set to true, Flink runner will run batch
jobs using the DataStream API. By default the option is set to false, so the batch jobs are still executed
using the DataSet API.&lt;/li>
&lt;li>&lt;code>upload_graph&lt;/code> as one of the Experiments options for DataflowRunner is no longer required when the graph is larger than 10MB for Java SDK (&lt;a href="https://github.com/apache/beam/pull/28621">PR#28621&lt;/a>).&lt;/li>
&lt;li>state amd side input cache has been enabled to a default of 100 MB. Use &lt;code>--max_cache_memory_usage_mb=X&lt;/code> to provide cache size for the user state API and side inputs. (Python) (&lt;a href="https://github.com/apache/beam/issues/28770">#28770&lt;/a>).&lt;/li>
&lt;li>Beam YAML stable release. Beam pipelines can now be written using YAML and leverage the Beam YAML framework which includes a preliminary set of IO&amp;rsquo;s and turnkey transforms. More information can be found in the YAML root folder and in the &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/yaml/README.md">README&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>org.apache.beam.sdk.io.CountingSource.CounterMark&lt;/code> uses custom &lt;code>CounterMarkCoder&lt;/code> as a default coder since all Avro-dependent
classes finally moved to &lt;code>extensions/avro&lt;/code>. In case if it&amp;rsquo;s still required to use &lt;code>AvroCoder&lt;/code> for &lt;code>CounterMark&lt;/code>, then,
as a workaround, a copy of &amp;ldquo;old&amp;rdquo; &lt;code>CountingSource&lt;/code> class should be placed into a project code and used directly
(&lt;a href="https://github.com/apache/beam/issues/25252">#25252&lt;/a>).&lt;/li>
&lt;li>Renamed &lt;code>host&lt;/code> to &lt;code>firestoreHost&lt;/code> in &lt;code>FirestoreOptions&lt;/code> to avoid potential conflict of command line arguments (Java) (&lt;a href="https://github.com/apache/beam/pull/29201">#29201&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &amp;ldquo;Desired bundle size 0 bytes must be greater than 0&amp;rdquo; in Java SDK&amp;rsquo;s BigtableIO.BigtableSource when you have more cores than bytes to read (Java) &lt;a href="https://github.com/apache/beam/issues/28793">#28793&lt;/a>.&lt;/li>
&lt;li>&lt;code>watch_file_pattern&lt;/code> arg of the &lt;a href="https://github.com/apache/beam/blob/104c10b3ee536a9a3ea52b4dbf62d86b669da5d9/sdks/python/apache_beam/ml/inference/base.py#L997">RunInference&lt;/a> arg had no effect prior to 2.52.0. To use the behavior of arg &lt;code>watch_file_pattern&lt;/code> prior to 2.52.0, follow the documentation at &lt;a href="https://beam.apache.org/documentation/ml/side-input-updates/">https://beam.apache.org/documentation/ml/side-input-updates/&lt;/a> and use &lt;code>WatchFilePattern&lt;/code> PTransform as a SideInput. (&lt;a href="https://github.com/apache/beam/pulls/28948">#28948&lt;/a>)&lt;/li>
&lt;li>&lt;code>MLTransform&lt;/code> doesn&amp;rsquo;t output artifacts such as min, max and quantiles. Instead, &lt;code>MLTransform&lt;/code> will add a feature to output these artifacts as human readable format - &lt;a href="https://github.com/apache/beam/issues/29017">#29017&lt;/a>. For now, to use the artifacts such as min and max that were produced by the eariler &lt;code>MLTransform&lt;/code>, use &lt;code>read_artifact_location&lt;/code> of &lt;code>MLTransform&lt;/code>, which reads artifacts that were produced earlier in a different &lt;code>MLTransform&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/29016/">#29016&lt;/a>)&lt;/li>
&lt;li>Fixed a memory leak, which affected some long-running Python pipelines: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Fixed &lt;a href="https://www.cve.org/CVERecord?id=CVE-2023-39325">CVE-2023-39325&lt;/a> (Java/Python/Go) (&lt;a href="https://github.com/apache/beam/issues/29118">#29118&lt;/a>).&lt;/li>
&lt;li>Mitigated &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2023-47248">CVE-2023-47248&lt;/a> (Python) &lt;a href="https://github.com/apache/beam/issues/29392">#29392&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.52.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Devansh Modi&lt;/p>
&lt;p>Dominik Dębowczyk&lt;/p>
&lt;p>Ferran Fernández Garrido&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Israel Herraiz&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>JayajP&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Jiangjie Qin&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>Joar Wandborg&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Luís Bianchin&lt;/p>
&lt;p>Minbo Bae&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vivek Sumanth&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>aku019&lt;/p>
&lt;p>brucearctor&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>damccorm&lt;/p>
&lt;p>ddebowczyk92&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>dpcollins-google&lt;/p>
&lt;p>edman124&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>illoise&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>jonathan-lemos&lt;/p>
&lt;p>kennknowles&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>nancyxu123&lt;/p>
&lt;p>pablo rodriguez defino&lt;/p>
&lt;p>tvalentyn&lt;/p></description><link>/blog/beam-2.52.0/</link><pubDate>Fri, 17 Nov 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.52.0/</guid><category>blog</category><category>release</category></item><item><title>Contributor Spotlight: Johanna Öjeling</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Johanna Öjeling is a Senior Software Engineer at &lt;a href="https://normative.io/">Normative&lt;/a>. She started using Apache Beam in 2020 at her previous company &lt;a href="http://datatonic.com">Datatonic&lt;/a> and began contributing in 2022 at a personal capacity. We interviewed Johanna to learn more about her interests and we hope that this will inspire new, future, diverse set of contributors to participate in OSS projects.&lt;/p>
&lt;p>&lt;strong>What areas of interest are you passionate about in your career?&lt;/strong>&lt;/p>
&lt;p>My core interest lies in distributed and data-intensive systems, and I enjoy working on challenges related to performance, scalability and maintainability. I also feel strongly about developer experience, and like to build tools and frameworks that make developers happier and more productive. Aside from that, I take pleasure in mentoring and coaching other software engineers to grow their skills and pursue a fulfilling career.&lt;/p>
&lt;p>&lt;strong>What motivated you to make your first contribution?&lt;/strong>&lt;/p>
&lt;p>I was already a user of the Apache Beam Java and Python SDKs and Google Cloud Dataflow in my previous job, and had started to play around with the Go SDK to learn Go. When I noticed that a feature I wanted was missing, it seemed like a great opportunity to implement it. I had been curious about developing open source software for some time, but did not have a good idea until then of what to contribute with.&lt;/p>
&lt;p>&lt;strong>In which way have you contributed to Apache Beam?&lt;/strong>&lt;/p>
&lt;p>I have primarily worked on the Go SDK with implementation of new features, bug fixes, tests, documentation and code reviews. Some examples include a MongoDB I/O connector with dynamically scalable reads and writes, a file I/O connector supporting continuous file discovery, and an Amazon S3 file system implementation.&lt;/p>
&lt;p>&lt;strong>How has your open source engagement impacted your personal or professional growth?&lt;/strong>&lt;/p>
&lt;p>Contributing to open source is one of the best decisions I have taken professionally. The Beam community has been incredibly welcoming and appreciative, and it has been rewarding to collaborate with talented people around the world to create software that is free for anyone to benefit from. Open source has opened up new opportunities to challenge myself, dive deeper into technologies I like, and learn from highly skilled professionals. To me, it has served as an outlet for creativity, problem solving and purposeful work.&lt;/p>
&lt;p>&lt;strong>How have you noticed contributing to open source is different from contributing to closed source/proprietary software?&lt;/strong>&lt;/p>
&lt;p>My observation has been that there are higher requirements for software quality in open source, and it is more important to get things right the first time. My closed source software experience is from startups/scale-ups where speed is prioritized. When not working on public facing APIs or libraries, one can also more easily change things, whereas we need to be mindful about breaking changes in Beam. I care for software quality and value the high standards the Beam committers hold.&lt;/p>
&lt;p>&lt;strong>What do you like to do with your spare time when you&amp;rsquo;re not contributing to Beam?&lt;/strong>&lt;/p>
&lt;p>Coding is a passion of mine so I tend to spend a lot of my free time on hobby projects, reading books and articles, listening to talks and attending events. When I was younger I loved learning foreign languages and studied English, French, German and Spanish. Later I discovered an interest in computer science and switched focus to programming languages. I decided to change careers to software engineering and have tried to learn as much as possible ever since. I love that it never ends.&lt;/p>
&lt;p>&lt;strong>What future features/improvements are you most excited about, or would you like to see on Beam?&lt;/strong>&lt;/p>
&lt;p>The multi-language pipeline support is an impressive feature of Beam, and I like that new SDKs such as TypeScript and Swift are emerging, which enables developers to write pipelines in their preferred language. Naturally, I am also excited to see where the Go SDK is headed and how we can make use of newer features of the Go language.&lt;/p>
&lt;p>&lt;strong>What types of contributions or support do you think the Beam community needs more of?&lt;/strong>&lt;/p>
&lt;p>Many data and machine learning engineers feel more comfortable with Python than Java and wish the Python SDK were as feature rich as the Java SDK. This presents great opportunities for Python developers to start contributing to Beam. As an SDK author, one can take advantage of Beam&amp;rsquo;s multiple SDKs. When I have developed in Go I have often studied the Java and Python implementations to get ideas for how to solve specific problems and make sure the Go SDK follows a similar pattern.&lt;/p>
&lt;p>&lt;strong>What advice would you give to someone who wants to contribute but does not know where to begin?&lt;/strong>&lt;/p>
&lt;p>Start with asking yourself what prior knowledge you have and what you would like to learn, then look for opportunities that match that. The contribution guidelines will tell you where to find open issues and what the process looks like. There are tasks labeled as &amp;ldquo;good first issue&amp;rdquo; which can be a good starting point. I was quite nervous about making my first contribution and had my mentor pre-review my PR. There was no need to worry though, as people will be grateful for your effort to improve the project. The pride I felt when a committer approved my PR and welcomed me to Beam is something I still remember.&lt;/p>
&lt;p>&lt;strong>What advice would you give to the Beam community? What could we improve?&lt;/strong>&lt;/p>
&lt;p>We can make it easier for new community members to get involved by providing more examples of tasks that we need help with, both in the form of code and non-code contributions. I will take it as an action point myself to label more issues accordingly and tailor the descriptions for newcomers. However, this is contingent on community members visiting the GitHub project. To address this, we could also proactively promote opportunities through social channels and the user mailing list.&lt;/p>
&lt;p>&lt;em>We thank Johanna for the interview and for her contributions! If you would like to learn more about contributing to Beam you can learn more about it here: &lt;a href="https://beam.apache.org/contribute/">https://beam.apache.org/contribute/&lt;/a>.&lt;/em>&lt;/p></description><link>/blog/contributor-spotlight-johanna-ojeling/</link><pubDate>Sat, 11 Nov 2023 15:00:00 -0800</pubDate><guid>/blog/contributor-spotlight-johanna-ojeling/</guid><category>blog</category></item><item><title>Build a scalable, self-managed streaming infrastructure with Beam and Flink</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog series, &lt;a href="https://www.linkedin.com/in/talatuyarer/">Talat Uyarer (Architect / Senior Principal Engineer)&lt;/a>, &lt;a href="https://www.linkedin.com/in/rishabhkedia/">Rishabh Kedia (Principal Engineer)&lt;/a>, and &lt;a href="https://www.linkedin.com/in/davidqhe/">David He (Engineering Director)&lt;/a> describe how we built a self-managed streaming platform by using Apache Beam and Flink. In this part of the series, we describe why and how we built a large-scale, self-managed streaming infrastructure and services based on Flink by migrating from a cloud managed streaming service. We also outline the learnings for operational scalability and observability, performance, and cost effectiveness. We summarize techniques that we found useful in our journey.&lt;/p>
&lt;h1 id="build-a-scalable-self-managed-streaming-infrastructure-with-flink---part-1">Build a scalable, self-managed streaming infrastructure with Flink - part 1&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Palo Alto Networks (PANW) is a leader in cybersecurity, providing products, services and solutions to our customers. Data is the center of our products and services. We stream and store exabytes of data in our data lake, with near real-time ingestion, data transformation, data insertion to data store, and forwarding data to our internal ML-based systems and external SIEM’s. We support multi-tenancy in each component so that we can isolate tenants and provide optimal performance and SLA. Streaming processing plays a critical role in the pipelines.&lt;/p>
&lt;p>In the second part of the series, we provide a more thorough description of the core building blocks of our streaming infrastructure, such as autoscaler. We also give more details about our customizations, which enabled us to build a high-performance, large-scale streaming system. Finally, we explain how we solved challenging problems.&lt;/p>
&lt;h2 id="the-importance-of-self-managed-streaming-infrastructure">The importance of self-managed streaming Infrastructure&lt;/h2>
&lt;p>We built a large-scale data platform on Google Cloud. We used Dataflow as a managed streaming service. With Dataflow, we used the streaming engine running our application using Apache Beam and observability tools such as Cloud Logging and Cloud Monitoring. For more details, see [1]. The system can handle 15 million of events per second and one trillion events daily, at four petabytes of data volume daily. We run about 30,000 Dataflow jobs. Each job can have one or hundreds of workers, depending on the customer’s event throughputs.&lt;/p>
&lt;p>We support various applications using different endpoints: BigQuery data store, HTTPS-based external SIEMs or internal endpoints, Syslog based SIEMs, and Google Cloud Storage endpoints. Our customers and products rely on this data platform to handle cybersecurity postures and reactions. Our streaming infrastructure is highly flexible to add, update, and delete use cases through a streaming job subscription. For example, a customer wants to ingest log events from a firewall device into the data lake buffered in Kafka topics. A streaming job is subscribed to extract and filter the data, transform the data format, and do a streaming insert to our BigQuery data warehouse endpoint in real-time. The customer can use our visualization and dashboard products to view traffic or threads captured by this firewall. The following diagram illustrates the event producer, the use case subscription workflow, and the key components of the streaming platform:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/image1.png"
alt="Streaming service design">&lt;/p>
&lt;p>This managed, Dataflow-based streaming infrastructure runs fine, but with some caveats:&lt;/p>
&lt;ol>
&lt;li>Cost is high, because it is a managed service. For the same resources used in a Dataflow application, such as vCPU and memory, the cost is much more expensive than using an open source streaming engine such as Flink running the same Beam application code.&lt;/li>
&lt;li>It&amp;rsquo;s not easy to achieve our latency and SLA goals, because it&amp;rsquo;s difficult to extend features, such as autoscaling based on different applications, endpoints, or different parameters within one application.&lt;/li>
&lt;li>The pipeline only runs on Google Cloud.&lt;/li>
&lt;/ol>
&lt;p>The uniqueness of PANW’s streaming use cases is another reason that we use a self-managed service. We support multi-tenancy. A tenant (a customer) can ingest data at a very high rate (&amp;gt;100k requests per second), or at a very low rate (&amp;lt; 100 requests per second). A Dataflow job runs on VMs instead of Kubernetes, requiring a minimal one vCPU core. With a small tenant, this wastes resources. Our streaming infrastructure supports thousands of jobs, and the CPU utilization is more efficient if we do not have to use one core for a job. It is natural for us to use a streaming engine running on Kubernetes, so that we can allocate minimal resources for a small tenant, for example, using a Google Kubernetes Engine (GKE) pod with ½ or less vCPU core.&lt;/p>
&lt;h2 id="the-choice-of-apache-flink-and-kubernetes">The choice of Apache Flink and Kubernetes&lt;/h2>
&lt;p>In an effort to handle the problems already stated and to find the most efficient solution, we evaluated various streaming frameworks, including Apache Samza, Apache Flink, and Apache Spark, against Dataflow.&lt;/p>
&lt;h3 id="performance">Performance&lt;/h3>
&lt;ul>
&lt;li>One notable factor was Apache Flink’s native Kubernetes support. Unlike Samza, which lacked native Kubernetes support and required Apache Zookeeper for coordination, Flink seamlessly integrated with Kubernetes. This integration eliminated unnecessary complexities. In terms of performance, both Samza and Flink were close competitors.&lt;/li>
&lt;li>Apache Spark, while popular, proved to be significantly slower in our tests. A presentation at the Beam Summit revealed that Apache Beam’s Spark Runner was approximately ten times slower than Native Apache Spark [3]. We could not afford such a drastic performance hit. Rewriting our entire Beam codebase with native Spark was not a viable option, especially given the extensive codebase we had built over the past four years with Apache Beam.&lt;/li>
&lt;/ul>
&lt;h3 id="community">Community&lt;/h3>
&lt;p>The robustness of community support played a pivotal role in our decision making. Dataflow provided excellent support, but we needed assurance in our choice of an open-source framework. Apache Flink’s vibrant community and active contributions from multiple companies offered a level of confidence that was unmatched. This collaborative environment meant that bug identification and fixes were ongoing processes. In fact, in our journey, we have patched our system using many Flink fixes from the community:&lt;/p>
&lt;ul>
&lt;li>We fixed the Google Cloud Storage file reading exceptions by merging Flink 1.15 open source fix &lt;a href="https://issues.apache.org/jira/browse/FLINK-26063?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&amp;amp;focusedCommentId=17504555#comment-17504555">FLINK-26063&lt;/a> (we are using 1.13).&lt;/li>
&lt;li>We fixed an issue with workers restarting for stateful jobs from &lt;a href="https://issues.apache.org/jira/browse/FLINK-31963">FLINK-31963&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>We also contributed to the community during our journey by founding and fixing bugs in the open source code. For details, see &lt;a href="https://issues.apache.org/jira/browse/FLINK-32700">FLINK-32700&lt;/a> for Flink Kubernetes Operator. We also created a new GKE Auth support for Kubernetes clients and merged it to GitHub at [4].&lt;/p>
&lt;h3 id="integration">Integration&lt;/h3>
&lt;p>The seamless integration of Apache Flink with Kubernetes provided us with a flexible and scalable platform for orchestration. The synergy between Apache Flink and Kubernetes not only optimized our data processing workflows but also future-proofed our system.&lt;/p>
&lt;h2 id="architecture-and-deployment-workflow">Architecture and deployment workflow&lt;/h2>
&lt;p>In the realm of real-time data processing and analytics, Apache Flink distinguishes itself as a powerful and versatile framework. When combined with Kubernetes, the industry-standard container orchestration system, Flink applications can scale horizontally and have robust management capabilities. We explore a cutting-edge design where Apache Flink and Kubernetes synergize seamlessly, thanks to the Apache Flink Kubernetes Operator.&lt;/p>
&lt;p>At its core, the Flink Kubernetes Operator serves as a control plane, mirroring the knowledge and actions of a human operator managing Flink deployments. Unlike traditional methods, the Operator automates critical activities, from starting and stopping applications to handling upgrades and errors. Its versatile feature set includes fully-automated job lifecycle management, support for different Flink versions, and multiple deployment modes, such as application clusters and session jobs. Moreover, the Operator&amp;rsquo;s operational prowess extends to metrics, logging, and even dynamic scaling by using the Job Autoscaler.&lt;/p>
&lt;h3 id="build-a-seamless-deployment-workflow">Build a seamless deployment workflow&lt;/h3>
&lt;p>Imagine a robust system where Flink jobs are deployed effortlessly, monitored diligently, and managed proactively. Our team created this workflow by integrating Apache Flink, Apache Flink Kubernetes Operator, and Kubernetes. Central to this setup is our custom-built Apache Flink Kubernetes Operator Client Library. This library acts as a bridge, enabling atomic operations such as starting, stopping, updating, and canceling Flink jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/stream-service-changes.png"
alt="Streaming service changes">&lt;/p>
&lt;h3 id="the-deployment-process">The deployment process&lt;/h3>
&lt;p>In our code, the client provides Apache Beam pipeline options, which include essential information such as the Kubernetes cluster&amp;rsquo;s API endpoint, authentication details, the Google Cloud/S3 temporary location for uploading the JAR file, and worker type specifications. The Kubernetes Operator Library uses this information to orchestrate a seamless deployment process. The following sections explain the steps taken. Most of the core steps are automated in our code base.&lt;/p>
&lt;p>&lt;strong>Step 1:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>The client wants to start a job for a customer and a specific application.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 2:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Generate a unique job ID:&lt;/strong> The library generates a unique job ID, which is set as a Kubernetes label. This identifier helps track and manage the deployed Flink job.&lt;/li>
&lt;li>&lt;strong>Configuration and code upload:&lt;/strong> The library uploads all necessary configurations and user code to a designated location on Google Cloud Storage or Amazon S3. This step ensures that the Flink application&amp;rsquo;s resources are available for deployment.&lt;/li>
&lt;li>&lt;strong>YAML payload generation:&lt;/strong> After the upload process completes, the library constructs a YAML payload. This payload contains crucial deployment information, including resource settings based on the specified worker type.&lt;/li>
&lt;/ul>
&lt;p>We used a convention for naming our worker VM instance types. Our convention is similar to the naming convention that Google Cloud uses. The name &lt;code>n1-standard-1&lt;/code> refers to a specific, predefined VM machine type. Let’s break down what each component of the name means:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>n1&lt;/strong> indicates the CPU type of the instance. In this case, it refers to the Intel based on instances in the N1 series. Google Cloud has multiple generations of instances with varying hardware and performance characteristics.&lt;/li>
&lt;li>&lt;strong>standard&lt;/strong> signifies the machine type family. Standard machine types offer a balanced ratio of 1 virtual CPU (vCPUs) and 4 GB of memory for Task Manager, and 0.5 vCPU and 2 GB memory for Job Manager.&lt;/li>
&lt;li>&lt;strong>1&lt;/strong> represents the number of vCPUs available in the instance. In the case of n1-standard-1, it means the instance has 1 vCPU.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 3:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Calling the Kubernetes API with Fabric8&lt;/strong>: To initiate the deployment, the library interacts with the Kubernetes API using Fabric8. Fabric8 initially lacked support for authentication in Google Kubernetes Engine or Amazon Elastic Kubernetes Service (EKS). To address this limitation, our team implemented the necessary authentication support, which can be found in our merge request on GitHub PR [4].&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 4:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Flink Operator deployment&lt;/strong>: When it receives the YAML payload, the Flink Operator takes charge of deploying the various components of the Flink job. Tasks include provisioning resources and managing the deployment of the Flink Job Manager, Task Manager, and Job Service.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 5:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Job submission and execution&lt;/strong>: When the Flink Job Manager is running, it fetches the JAR file and configurations from the designated Google Cloud Storage or S3 location. With all necessary resources in place, it submits the Flink job to the standalone Flink cluster for execution.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Step 6&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Continuous monitoring&lt;/strong>: Post-deployment, our operator continuously monitors the status of the running Flink job. This real-time feedback loop enables us to promptly address any issues that arise, ensuring the overall health and optimal performance of our Flink applications.&lt;/li>
&lt;/ul>
&lt;p>In summary, our deployment process leverages Apache Beam pipeline options, integrates seamlessly with Kubernetes and the Flink Operator, and employs custom logic to handle configuration uploads and authentication. This end-to-end workflow ensures a reliable and efficient deployment of Flink applications in Kubernetes clusters while maintaining vigilant monitoring for smooth operation. The following sequence diagram shows the steps.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/job-start-activity-diagram.png"
alt="Job Start Activity Diagram">&lt;/p>
&lt;h2 id="develope-an-autoscaler">Develope an autoscaler&lt;/h2>
&lt;p>Having an autoscaler is critical to having a self-managed streaming service. There are not enough resources available on the internet for us to learn to build our own autoscaler, which makes this part of the workflow difficult.&lt;/p>
&lt;p>The autoscaler scales up the number of task managers to drain the lag and to keep up with the throughput. It also scales down the minimum number of resources required to process the incoming traffic to reduce costs. We need to do this frequently while keeping the processing disruption to minimum.&lt;/p>
&lt;p>We extensively tuned the autoscaler to meet the SLA for latency. This tuning involved a cost trade off. We also made the autoscaler application-specific to meet specific needs for certain applications. Every decision has a hidden cost. The second part of this blog provides more details about the autoscaler.&lt;/p>
&lt;h2 id="create-a-client-library-for-steaming-job-development">Create a client library for steaming job development&lt;/h2>
&lt;p>To deploy the job using the Flink Kubernetes Operator, you need to know about how Kubernetes works. The following steps explain how to create a single Flink job.&lt;/p>
&lt;ol>
&lt;li>Define a YAML file with proper specifications. The following image provides an example.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink.apache.org/v1beta1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">FlinkDeployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">basic-reactive-example&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink:1.13&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1_13&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">flinkConfiguration&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">scheduler-mode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">REACTIVE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskmanager.numberOfTaskSlots&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state.savepoints.dir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/savepoints&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state.checkpoints.dir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/checkpoints&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">high-availability&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">org.apache.flink.kubernetes.highavailability.KubernetesHaServicesFactory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">high-availability.storageDir&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">file:///flink-data/ha&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">serviceAccount&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jobManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">taskManager&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resource&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;2048m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">podTemplate&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-main-container&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumeMounts&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">mountPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/flink-data&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">volumes&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">flink-volume&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">hostPath&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># directory location on host&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">path&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">/tmp/flink&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="c"># this field is optional&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">type&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Directory&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">job&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">jarURI&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">local:///opt/flink/examples/streaming/StateMachineExample.jar&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">parallelism&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">upgradeMode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">savepoint&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">state&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">running&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">savepointTriggerNonce&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">mode&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">standalone&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>SSH into your Flink cluster and run the command following command:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl create -f job1.yaml
&lt;/code>&lt;/pre>&lt;ol start="3">
&lt;li>Use the following command to check the status of the job:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>kubectl get flinkdeployment job1
&lt;/code>&lt;/pre>&lt;p>This process impacts our scalability. Because we frequently update our jobs, we can&amp;rsquo;t manually follow these steps for every running job. To do so would be highly error prone and time consuming. One wrong space in the YAML can fail the deployment. This approach also acts as a barrier to innovation, because you need to know Kubernetes to interact with Flink jobs.&lt;/p>
&lt;p>We built a library to provide an interface for any teams and applications that want to to start, delete, update, or get the status of their jobs.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/fko-library.png"
alt="Flink Kubernetes Operator Library">&lt;/p>
&lt;p>This library extends the Fabric8 client and FlinkDeployment CRD. FlinkDeployment CRD is exposed by the Flink Kubernetes Operator. CRD lets you store and retrieve structured data. By extending the CRD, we get access to POJO, making it easier to manipulate the YAML file.&lt;/p>
&lt;p>The library supports the following tasks:&lt;/p>
&lt;ol>
&lt;li>Authentication to ensure that you are allowed to perform actions on the Flink cluster.&lt;/li>
&lt;li>Validation (fetches the template from AWS/Google Cloud Storage for validation) takes user variable input and validates it against the policy, rules, YAML format.&lt;/li>
&lt;li>Action execution converts the Java call to invoke the Kubernetes operation.&lt;/li>
&lt;/ol>
&lt;p>During this process, we learned the following lessons:&lt;/p>
&lt;ol>
&lt;li>App specific operator service: At our large scale, the operator was unable to handle such a large number of jobs. Kubernetes calls started to time out and fail. To solve this problem, we created multiple operators (about 4) in high-traffic regions to handle each application.&lt;/li>
&lt;li>Kube call caching: To prevent overloading, we cached the results of Kubernetes calls for thirty to sixty seconds.&lt;/li>
&lt;li>Label support: Providing label support to search jobs using client-specific variables reduced the load on Kube and improved the job search speed by 5x.&lt;/li>
&lt;/ol>
&lt;p>The following are some of the biggest wins we achieved by exposing the library:&lt;/p>
&lt;ol>
&lt;li>Standardized job management: Users can start, delete, and get status updates for their Flink jobs in a Kubernetes environment using a single library.&lt;/li>
&lt;li>Abstracted Kubernetes complexity: Teams no longer need to worry about the inner workings of Kubernetes or the formatting job deployment YAML files. The library handles these details internally.&lt;/li>
&lt;li>Simplified upgrades: With the underlying Kubernetes infrastructure, the library brings robustness and fault tolerance to Flink job management, ensuring minimal downtime and efficient recovery.&lt;/li>
&lt;/ol>
&lt;h2 id="observability-and-alerting">Observability and alerting&lt;/h2>
&lt;p>Observability is important when runing a production system at a large scale. We have about 30,000 streaming jobs in PANW. Each job serves a customer for a specific application. Each job also reads data from multiple topics in Kafka, performs transformations, and then writes the data to various sinks and endpoints.&lt;/p>
&lt;p>Constraints can occur anywhere in the pipeline or its endpoints, such as the customer API, BigQuery, and so on. We want to make sure the latency of streaming meets the SLA. Therefore, understanding if a job is healthy, meeting SLA, and alerting and intervening when needed is very challenging.&lt;/p>
&lt;p>To achieve our operational goals, we built a sophisticated observability and alerting capability. We provide three kinds of observability and debugging tools, described in the following sections.&lt;/p>
&lt;h3 id="flink-job-list-and-job-insights-from-prometheus-and-grafana">Flink job list and job insights from Prometheus and Grafana&lt;/h3>
&lt;p>Each Flink job sends various metrics to our Prometheus with cardinality details, such as application name, customer Id, and regions, so that we can look at each job. Critical metrics include the input traffic rate, output throughput, backlogs in Kafka, timestamp-based latency, task CPU usage, task numbers, OOM counts, and so on.&lt;/p>
&lt;p>The following charts provide a few examples. The charts provide details about the ingestion traffic rate to Kafka for a specific customer, the streaming job’s overall throughput, each vCPU’s throughput, backlogs in Kafka, and worker autoscaling based on the observed backlog.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/job-metrics.png"
alt="Flink Job Metrics">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/autoscaling-metrics.png"
alt="Flink Job Autoscaling Metrics">&lt;/p>
&lt;p>The following chart shows streaming latency based on the timestamp watermark. In addition to the numbers of events in Kafka as backlogs, it is important to know the time latency for end-to-end streaming so that we can define and monitor the SLA. The latency is defined as the time taken for the streaming processing, starting from ingestion timestamp, to the timestamp sending to the streaming endpoint. A watermark is the last processed event’s time. With the watermark, we are tracking P100 latency. We track each event’s stream latency, so that we can understand each Kafka topic and partition or Flink job pipeline issue. The following example shows each event stream and its latency:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/watermark-metrics.png"
alt="Apache Beam Watermark Metrics">&lt;/p>
&lt;h3 id="flink-open-source-ui">Flink open source UI&lt;/h3>
&lt;p>We use and extend the Apache Flink dashboard UI to monitor jobs and tasks, such as the checkpoint duration, size, and failure. One important extension we used is a job history page that lets us see a job&amp;rsquo;s start and update timeline and details, which helps us to debug issues.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/flink-checkpoint-ui.png"
alt="Flink Checkpoint UI">&lt;/p>
&lt;h3 id="dashboards-and-alerting-for-backlog-and-latency">Dashboards and alerting for backlog and latency&lt;/h3>
&lt;p>We have about 30,000 jobs, and we want to closely monitor the jobs and receive alerts for jobs in abnormal states so that we can intervene. We created dashboards for each application so that we can show the list of jobs with the highest latency and create thresholds for alerts. The following example shows the timestamp-based latency dashboard for one application. We can set the alerting if the latency is larger than a threshold, such as 10 minutes, for a certain time continuously:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/latency-graph.png"
alt="Latency Graph">&lt;/p>
&lt;p>The following example shows more backlog-based dashboards:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/backlog-graph.png"
alt="Backlog Graph">&lt;/p>
&lt;p>The alerts are based on thresholds, and we frequently check metrics. If a threshold is met and continues for a certain amount of times, we alert our internal Slack channels or PagerDuty for immediate attention. We tune the alerting so that the accuracy is high.&lt;/p>
&lt;h2 id="cost-optimization-strategies-and-tuning">Cost optimization strategies and tuning&lt;/h2>
&lt;p>We also moved to a self-managed streaming service to improve cost efficiency. Several minor tunings have allowed us to reduce costs by half, and we have more opportunities for improvement.&lt;/p>
&lt;p>The following list includes a few tips that have helped us:&lt;/p>
&lt;ul>
&lt;li>Use Google Cloud Storage as checkpointing storage.&lt;/li>
&lt;li>Reduce the write frequency to Google Cloud Storage.&lt;/li>
&lt;li>Use appropriate machine types. For example, in Google Cloud, N2D machines are 15% less expensive than N2 machines.&lt;/li>
&lt;li>Autoscale tasks to use optimal resources while maintaining the latency SLA.&lt;/li>
&lt;/ul>
&lt;p>The following sections provide more details about the first two tips.&lt;/p>
&lt;h3 id="google-cloud-storage-and-checkpointing">Google Cloud Storage and checkpointing&lt;/h3>
&lt;p>We use Google Cloud Storage as our checkpoint store because it is cost-effective, scalable, and durable. When working with Google Cloud Storage, the following design considerations and best practices can help you optimize scaling and performance:&lt;/p>
&lt;ul>
&lt;li>Use data partitioning methods like range partitioning, which divides data based on specific attributes, and hash partitioning, which distributes data evenly using hash functions.&lt;/li>
&lt;li>Avoid sequential key names, especially timestamps, to avoid hotspots and uneven data distribution. Instead, introduce random prefixes for object distribution.&lt;/li>
&lt;li>Use a hierarchical folder structure to improve data management and reduce the number of objects in a single directory.&lt;/li>
&lt;li>Combine small files into larger ones to improve read throughput. Minimizing the number of small files reduces inefficient storage use and metadata operations.&lt;/li>
&lt;/ul>
&lt;h3 id="tune-the-frequency-of-writing-to-google-cloud-storage">Tune the frequency of writing to Google Cloud Storage&lt;/h3>
&lt;p>Scaling jobs efficiently was one of our primary challenges. Stateless jobs, which are relatively simpler, still present hurdles, especially in scenarios where Flink needed to process an overwhelming number of workers. To overcome this challenge, We increased the &lt;code>state.storage.fs.memory-threshold&lt;/code> settings to 1 MB from 20KB (??). This configuration allowed us to combine small checkpoint files into larger ones at the Job Manager level and to reduce metadata calls.&lt;/p>
&lt;p>Optimizing the performance of Google Cloud operations was another challenge. Although Google Cloud Storage is excellent for streaming large amounts of data, it has limitations when it comes to handling high-frequency I/O requests. To mitigate this issue, we introduced random prefixes in key names, avoided sequential key names, and optimized our Google Cloud Storage sharding techniques. These methods significantly enhanced our Google Cloud Storage performance, enabling the smooth operation of our stateless jobs.&lt;/p>
&lt;p>The following chart shows the Google Cloud Storage writes reduction after changing the memory-threshold:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-beam-flink-and-kubernetes/gcs-write-graph.png"
alt="GCS write Graph">&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>Palo Alto Networks® Cortex Data Lake is fully migrated from Dataflow streaming engine to Flink self managed streaming engine infrastructure. We have achieved our goals to run the system more cost efficiently (more than half cost cut), and run the infrastructure on multiple clouds such as GCP and AWS. We have learned how to build a large scale reliable production system based on open sources. We see large potentials to customize the system based on our specific needs as we have a lot of freedom to customize the open source code and configuration. In the next Part 2 post we will give more details on autoscaling and performance tuning parts. We hope our experience will be helpful for readers who will explore similar solutions for their own organizations.&lt;/p>
&lt;h1 id="additional-resources">Additional Resources&lt;/h1>
&lt;p>We provide links here for related presentations as further reading for readers interested in implementing similar solutions. By adding this section, we hope you can find more details to build a fully managed streaming infrastructure, making it easier for readers to follow our stories and learnings.&lt;/p>
&lt;p>[1] Streaming framework at PANW published at Apache Beam: &lt;a href="https://beam.apache.org/case-studies/paloalto/">https://beam.apache.org/case-studies/paloalto/&lt;/a>&lt;/p>
&lt;p>[2] PANW presentation at Beam Summit 2023: &lt;a href="https://youtu.be/IsGW8IU3NfA?feature=shared">https://youtu.be/IsGW8IU3NfA?feature=shared&lt;/a>&lt;/p>
&lt;p>[3] Benchmark presented at Beam Summit 2021: &lt;a href="https://2021.beamsummit.org/sessions/tpc-ds-and-apache-beam/">https://2021.beamsummit.org/sessions/tpc-ds-and-apache-beam/&lt;/a>&lt;/p>
&lt;p>[4] PANW open source contribution to Flink for GKE Auth support: &lt;a href="https://github.com/fabric8io/kubernetes-client/pull/4185">https://github.com/fabric8io/kubernetes-client/pull/4185&lt;/a>&lt;/p>
&lt;h1 id="acknowledgements">Acknowledgements&lt;/h1>
&lt;p>This is a large effort to build the new infrastructure and to migrate the large customer based applications from cloud provider managed streaming infrastructure to self-managed Flink based infrastructure at scale. Thanks the Palo Alto Networks CDL streaming team who helped to make this happen: Kishore Pola, Andrew Park, Hemant Kumar, Manan Mangal, Helen Jiang, Mandy Wang, Praveen Kumar Pasupuleti, JM Teo, Rishabh Kedia, Talat Uyarer, Naitk Dani, and David He.&lt;/p></description><link>/blog/apache-beam-flink-and-kubernetes/</link><pubDate>Fri, 03 Nov 2023 09:00:00 -0400</pubDate><guid>/blog/apache-beam-flink-and-kubernetes/</guid><category>blog</category></item><item><title>Apache Beam 2.51.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.51.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2510-2023-10-03">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.51.0, check out the &lt;a href="https://github.com/apache/beam/milestone/15">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>In Python, &lt;a href="https://beam.apache.org/documentation/sdks/python-machine-learning/#why-use-the-runinference-api">RunInference&lt;/a> now supports loading many models in the same transform using a &lt;a href="https://beam.apache.org/documentation/sdks/python-machine-learning/#use-a-keyed-modelhandler">KeyedModelHandler&lt;/a> (&lt;a href="https://github.com/apache/beam/issues/27628">#27628&lt;/a>).&lt;/li>
&lt;li>In Python, the &lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.vertex_ai_inference.html#apache_beam.ml.inference.vertex_ai_inference.VertexAIModelHandlerJSON">VertexAIModelHandlerJSON&lt;/a> now supports passing in inference_args. These will be passed through to the Vertex endpoint as parameters.&lt;/li>
&lt;li>Added support to run &lt;code>mypy&lt;/code> on user pipelines (&lt;a href="https://github.com/apache/beam/issues/27906">#27906&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Removed fastjson library dependency for Beam SQL. Table property is changed to be based on jackson ObjectNode (Java) (&lt;a href="https://github.com/apache/beam/issues/24154">#24154&lt;/a>).&lt;/li>
&lt;li>Removed TensorFlow from Beam Python container images &lt;a href="https://github.com/apache/beam/pull/28424">PR&lt;/a>. If you have been negatively affected by this change, please comment on &lt;a href="https://github.com/apache/beam/issues/20605">#20605&lt;/a>.&lt;/li>
&lt;li>Removed the parameter &lt;code>t reflect.Type&lt;/code> from &lt;code>parquetio.Write&lt;/code>. The element type is derived from the input PCollection (Go) (&lt;a href="https://github.com/apache/beam/issues/28490">#28490&lt;/a>)&lt;/li>
&lt;li>Refactor BeamSqlSeekableTable.setUp adding a parameter joinSubsetType. &lt;a href="https://github.com/apache/beam/issues/28283">#28283&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed exception chaining issue in GCS connector (Python) (&lt;a href="https://github.com/apache/beam/issues/26769#issuecomment-1700422615">#26769&lt;/a>).&lt;/li>
&lt;li>Fixed streaming inserts exception handling, GoogleAPICallErrors are now retried according to retry strategy and routed to failed rows where appropriate rather than causing a pipeline error (Python) (&lt;a href="https://github.com/apache/beam/issues/21080">#21080&lt;/a>).&lt;/li>
&lt;li>Fixed a bug in Python SDK&amp;rsquo;s cross-language Bigtable sink that mishandled records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="security-fixes">Security Fixes&lt;/h2>
&lt;ul>
&lt;li>Python containers updated, fixing &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30474">CVE-2021-30474&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30475">CVE-2021-30475&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30473">CVE-2021-30473&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36133">CVE-2020-36133&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36131">CVE-2020-36131&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36130">CVE-2020-36130&lt;/a>, and &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36135">CVE-2020-36135&lt;/a>&lt;/li>
&lt;li>Used go 1.21.1 to build, fixing &lt;a href="https://security-tracker.debian.org/tracker/CVE-2023-39320">CVE-2023-39320&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python pipelines using BigQuery Storage Read API must pin &lt;code>fastavro&lt;/code> dependency to 1.8.3
or earlier: &lt;a href="https://github.com/apache/beam/issues/28811">#28811&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.50.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Adam Whitmore&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Arvind Ram&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Bulat&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hao Xu&lt;/p>
&lt;p>Haruka Abe&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeffrey Kinard&lt;/p>
&lt;p>Joey Tran&lt;/p>
&lt;p>Johanna Öjeling&lt;/p>
&lt;p>Julien Tournay&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kerry Donny-Clark&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Melissa Pashniak&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Rebecca Szper&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Ruwann&lt;/p>
&lt;p>Ryan Tam&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sereana Seim&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tim Grein&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zbynek Konecny&lt;/p>
&lt;p>Zechen Jiang&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>caneff&lt;/p>
&lt;p>dependabot[bot]&lt;/p>
&lt;p>gDuperran&lt;/p>
&lt;p>gabry.wu&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>kberezin-nshl&lt;/p>
&lt;p>kennknowles&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>lostluck&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>olalamichelle&lt;/p>
&lt;p>tvalentyn&lt;/p>
&lt;p>xqhu&lt;/p>
&lt;p>Łukasz Spyra&lt;/p></description><link>/blog/beam-2.51.0/</link><pubDate>Wed, 11 Oct 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.51.0/</guid><category>blog</category><category>release</category></item><item><title>DIY GenAI Content Discovery Platform with Apache Beam</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h1 id="diy-genai-content-discovery-platform-with-apache-beam">DIY GenAI Content Discovery Platform with Apache Beam&lt;/h1>
&lt;p>Your digital assets, such as documents, PDFs, spreadsheets, and presentations, contain a wealth of valuable information, but sometimes it&amp;rsquo;s hard to find what you&amp;rsquo;re looking for. This blog post explains how to build a DIY starter architecture, based on near real-time ingestion processing and large language models (LLMs), to extract meaningful information from your assets. The model makes the information available and discoverable through a simple natural language query.&lt;/p>
&lt;p>Building a near real-time processing pipeline for content ingestion might seem like a complex task, and it can be. To make pipeline building easier, the Apache Beam framework exposes a set of powerful constructs. These constructs remove the following complexities: interacting with multiple types of content sources and destinations, error handling, and modularity. They also maintain resiliency and scalability with minimal effort. You can use an Apache Beam streaming pipeline to complete the following tasks:&lt;/p>
&lt;ul>
&lt;li>Connect to the many components of a solution.&lt;/li>
&lt;li>Quickly process content ingestion requests of documents.&lt;/li>
&lt;li>Make the information in the documents available a few seconds after ingestion.&lt;/li>
&lt;/ul>
&lt;p>LLMs are often used to extract content and summarize information stored in many different places. Organizations can use LLMs to quickly find relevant information disseminated in multiple documents written across the years. The information might be in different formats, or the documents might be too long and complex to read and understand quickly. Use LLMs to process this content to make it easier for people to find the information that they need.&lt;/p>
&lt;p>Follow the steps in this guide to create a custom scalable solution for data extraction, content ingestion, and storage. Learn how to kickstart the development of a LLM-based solution using Google Cloud products and generative AI offerings. Google Cloud is designed to be simple to use, scalable, and flexible, so you can use it as a starting point for further expansion or experimentation.&lt;/p>
&lt;h3 id="high-level-flow">High-level Flow&lt;/h3>
&lt;p>In this workflow, content uptake and query interactions are completely separated. An external content owner can send documents stored in Google Docs or in a binary text format and receive a tracking ID for the ingestion request. The ingestion process gets the content of the document and creates chunks that are configurable in size. Each document chunk is used to generate embeddings. These embeddings represent the content semantics, in the form of a vector of 768 dimensions. Given the document identifier and the chunk identifier, you can store the embeddings in a Vector database for semantic matching. This process is central to contextualizing user inquiries.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/cdp-highlevel.png"
alt="Content Discovery Platform Overview">&lt;/p>
&lt;p>The query resolution process doesn&amp;rsquo;t depend directly on information ingestion. The user receives relevant answers based on the content ingested until the moment of the query request. Even if the platform doesn&amp;rsquo;t have any relevant content stored, the platform returns an answer stating that it doesn&amp;rsquo;t have relevant content. Therefore, the query resolution process first generates embeddings from the query content and from the previously existing context, like previous exchanges with the platform, then matches these embeddings with the existing embedding vectors stored from the content. When the platform has positive matches, it retrieves the plain-text content represented by the content embeddings. Finally, by using the textual representation of the query and the textual representation of the matched content, the platform formulates a request to the LLM to provide a final answer to the original user inquiry.&lt;/p>
&lt;h2 id="components-of-the-solution">Components of the solution&lt;/h2>
&lt;p>Use the low-ops capabilities of the Google Cloud services to create a set of highly scalable features. You can separate the solution into two main components: the service layer and the content ingestion pipeline. The service layer acts as the entry point for document ingestion and user queries. It’s a simple set of REST resources exposed through Cloud Run and implemented by using &lt;a href="https://quarkus.io/">Quarkus&lt;/a> and the client libraries to access other services (Vertex AI models, Cloud Bigtable and Pub/Sub). The content ingestion pipeline includes the following components:&lt;/p>
&lt;ul>
&lt;li>A streaming pipeline that captures user content from wherever it resides.&lt;/li>
&lt;li>A process that extracts meaning from this content as a set of multi-dimensional vectors (text embeddings).&lt;/li>
&lt;li>A storage system that simplifies context matching between knowledge content and user inquiries (a Vector Database).&lt;/li>
&lt;li>Another storage system that maps knowledge representation with the actual content, forming the aggregated context of the inquiry.&lt;/li>
&lt;li>A model capable of understanding the aggregated context and, through prompt engineering, delivering meaningful answers.&lt;/li>
&lt;li>HTTP and gRPC-based services.&lt;/li>
&lt;/ul>
&lt;p>Together, these components provide a comprehensive and simple implementation for a content discovery platform.&lt;/p>
&lt;h2 id="workflow-architecture">Workflow Architecture&lt;/h2>
&lt;p>This section explains how the different components interact.&lt;/p>
&lt;h3 id="dependencies-of-the-components">Dependencies of the components&lt;/h3>
&lt;p>The following diagram shows all of the components that the platform integrates with. It also shows all of the dependencies that exist between the components of the solution and the Google Cloud services.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/cdp-arch.png"
alt="Content Discovery Platform Interactions">&lt;/p>
&lt;p>As seen in the diagram, the context-extraction component is the central aspect in charge of retrieving the document’s content, also their semantic meaning from the embedding’s model and storing the relevant data (chunks text content, chunks embeddings, JSON-L content) in the persistent storage systems for later use. PubSub resources are the glue between the streaming pipeline and the asynchronous processing, capturing the user ingestion requests, retries from potential errors from the ingestion pipeline (like the cases on where documents have been sent for ingestion but the permission has not been granted yet, triggering a retry after some minutes) and content refresh events (periodically the pipeline will scan the ingested documents, review the latest editions and define if a content refresh should be triggered).&lt;/p>
&lt;p>The context-extraction component retrieves the content of the documents, diving it in chunks. It also computes embeddings, using the LLM interaction, from the extracted content. Then it stores the relevant data (chunks text content, chunks embeddings, JSON-L content) in the persistent storage systems for later use. Pub/Sub resources connect the streaming pipeline and the asynchronous processing, capturing the following actions:&lt;/p>
&lt;ul>
&lt;li>user ingestion requests&lt;/li>
&lt;li>retries from errors from the ingestion pipeline, such as when documents are sent for ingestion but access permissions are missing&lt;/li>
&lt;li>content refresh events (periodically the pipeline scans the ingested documents, reviews the latest editions, and decides whether to trigger a content refresh)&lt;/li>
&lt;/ul>
&lt;p>Also, CloudRun plays an important role exposing the services, interacting with many Google Cloud services to resolve the user query or ingestion requests. For example, while resolving a query request the service will:&lt;/p>
&lt;ul>
&lt;li>Request the computation of embeddings from the user’s query by interacting with the embeddings model&lt;/li>
&lt;li>Find near neighbor matches from the Vertex AI Vector Search (formerly Matching Engine) using the query embeddings representation&lt;/li>
&lt;li>Retrieve the text content from BigTable for those matched vectors, using their identifier, in order contextualize a LLM prompt&lt;/li>
&lt;li>And finally create a request to the VertexAI Chat-Bison model, generating the response the system will delivery to the user’s query.&lt;/li>
&lt;/ul>
&lt;h3 id="google-cloud-products">Google Cloud products&lt;/h3>
&lt;p>This section describes the Google Cloud products and services used in the solution and what purpose they serve.&lt;/p>
&lt;p>&lt;strong>Cloud Build:&lt;/strong> All container images, including services and pipelines, are built directly from source code by using Cloud Build. Using Cloud Build simplifies code distribution during the deployment of the solution.&lt;/p>
&lt;p>&lt;strong>CloudRun:&lt;/strong> The solution&amp;rsquo;s service entry points are deployed and automatically scaled by CloudRun.&lt;/p>
&lt;p>&lt;strong>Pub/Sub:&lt;/strong> A Pub/Sub topic and subscription queue all of the ingestion requests for Google Drive or self-contained content and deliver the requests to the pipeline.&lt;/p>
&lt;p>&lt;strong>Dataflow:&lt;/strong> A multi-language, streaming Apache Beam pipeline processes the ingestion requests. These requests are sent to the pipeline from the Pub/Sub subscription. The pipeline extracts content from Google Docs, Google Drive URLs, and self-contained binary encoded text content. It then produces content chunks. These chunks are sent to one of the Vertex AI foundational models for the embedding representation. The embeddings and chunks from the documents are sent to Vertex AI Vector Search and to Cloud Bigtable for indexing and rapid access. Finally, the ingested documentation is stored in Google Cloud Storage in JSON-L format, which can be used to fine-tune the Vertex AI models. By using Dataflow to run the Apache Beam streaming pipeline, you minimize the ops needed to scale resources. If you have a burst on ingestion requests, Dataflow can keep the latency less than a minute.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Vector Search:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/matching-engine/overview">Vector Search&lt;/a> is a high-performance, low-latency vector database. These vector databases are often called vector similarity search or approximate nearest neighbor (ANN) services. We use a Vector Search Index to store all the ingested documents embeddings as a meaning representation. These embeddings are indexed by chunk and document id. Later on, these identifiers can be used to contextualize the user queries and enrich the requests made to a LLM by providing knowledge extracted directly from the document’s content mappings stored on BigTable (using the same chunk-document identifiers).&lt;/p>
&lt;p>&lt;strong>Cloud BigTable:&lt;/strong> This storage system provides a low latency search by identifier at a predictable scale. Is a perfect fit, given the low latency of the requests resolution, for online exchanges between user queries and the platform component interactions. It used to store the content extracted from the documents since it&amp;rsquo;s indexed by chunk and document identifier. Every time a user makes a request to the query service, and after the query text embeddings are resolved and matched with the existing context, the document and chunk ids are used to retrieve the document’s content that will be used as context to request an answer to the LLM in use. Also, BigTable is used to keep track of the conversational exchanges between users and the platform, furthermore enriching the context included on the requests sent to the LLMs (embeddings, summarization, chat Q&amp;amp;A).&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Embedding Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings">Text embeddings&lt;/a> are a condensed vector (numeric) representation of a piece of text. If two pieces of text are semantically similar, their corresponding embeddings will be located close together in the embedding vector space. For more details please see &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings">get text embeddings&lt;/a>. These embeddings are directly used by the ingestion pipeline when processing the document’s content and the query service as an input to match the users query semantic with existing content indexed in Vector Search.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Summarization Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text">Text-bison&lt;/a> is the name of the PaLM 2 LLM that understands, summarizes and generates text. The types of content that text-bison can create include document summaries, answers to questions, and labels that classify the provided input content. We used this LLM to summarize the previously maintained conversation with the goal of enriching the user’s queries and better embedding matching. In summary, the user does not have to include all the context of his question, we extract and summarize it from the conversation history.&lt;/p>
&lt;p>&lt;strong>Vertex AI - Text Chat Model:&lt;/strong> &lt;a href="https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat">Chat-bison&lt;/a> is the PaLM 2 LLM that excels at language understanding, language generation, and conversations. This chat model is fine-tuned to conduct natural multi-turn conversations, and is ideal for text tasks about code that require back-and-forth interactions. We use this LLM to provide answers to the queries made by users of the solution, including the conversation history between both parties and enriching the model’s context with the content stored in the solution.&lt;/p>
&lt;h3 id="extraction-pipeline">Extraction Pipeline&lt;/h3>
&lt;p>The content extraction pipeline is the platform&amp;rsquo;s centerpiece. It takes care of handling content ingestion requests, extracting documents content and computing embeddings from that content, to finally store the data in specialized storage systems that will be used in the query service components for rapid access.&lt;/p>
&lt;h4 id="high-level-view">High Level View&lt;/h4>
&lt;p>As previously mentioned the pipeline is implemented using Apache Beam framework and runs in streaming fashion on GCP&amp;rsquo;s &lt;a href="https://cloud.google.com/dataflow">Dataflow&lt;/a> service.&lt;/p>
&lt;p>By using Apache Beam and Dataflow we can ensure minimal latency (sub minute processing times), low ops (no need to manually scale up or down the pipeline when traffic spikes occur with time, worker recycle, updates, etc.) and with high level of observability (clear and abundant performance metrics are available).&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-1.png"
alt="Apache Beam Pipeline">&lt;/p>
&lt;p>On a high level, the pipeline separates the extraction, computing, error handling and storage responsibilities on different components or PTransforms. As seen in the diagram, the messages are read from a PubSub subscription and immediately afterwards are included in the window definition before the content extraction.&lt;/p>
&lt;p>Each of those PTransforms can be expanded to reveal more details regarding the underlying stages for the implementation. We will dive into each in the following sections.&lt;/p>
&lt;p>The pipeline was implemented using a multi-language approach, with the main components written in the Java language (JDK version 17) and those related with the embeddings computations implemented in Python (version 3.11) since the Vertex AI API clients are available for this language.&lt;/p>
&lt;h4 id="content-extraction">Content Extraction&lt;/h4>
&lt;p>The content extraction component is in charge of reviewing the ingestion request payload and deciding (given the event properties) if it will need to retrieve the content from the event itself (self-contained content, text based document binary encoded) or retrieve it from Google Drive.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-2-extractcontent.png"
alt="Pipeline's Content Extraction">&lt;/p>
&lt;p>In case of a self-contained document, the pipeline will extract the document id and format the document in paragraphs for later embedding processing.&lt;/p>
&lt;p>When in need of retrieval from Google Drive, the pipeline will inspect if the provided URL in the event refers to a Google Drive folder or a single file format (supported formats are Documents, Spreadsheets and Presentations). In the case of a folder, the pipeline will crawl the folder’s content recursively extracting all the files for the supported formats, in case of a single document will just return that one.&lt;/p>
&lt;p>Finally, with all the file references retrieved from the ingestion request, textual content is extracted from the files (no image support implemented for this PoC). That content will also be passed to the embedding processing stages including the document’s identifier and the content as paragraphs.&lt;/p>
&lt;h4 id="error-handling">Error Handling&lt;/h4>
&lt;p>On every stage of the content extraction process multiple errors can be encountered, malformed ingestion requests, non-conformant URLs, lack of permissions for Drive resources, lack of permissions for File data retrieval.&lt;/p>
&lt;p>In all those cases a dedicated component will capture those potential errors and define, given the nature of the error, if the event should be retried or sent to a dead letter GCS bucket for later inspection.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-3-errorhandling.png"
alt="Pipeline's Error Handling">&lt;/p>
&lt;p>The final errors, or those which won’t be retried, are those errors related with bad request formats (the event itself or the properties content, like malformed or wrong URLs, etc.).&lt;/p>
&lt;p>The retryable errors are those related with content access and lack of permissions. A request may have been resolved faster than the manual process of providing the right permissions to the Service Account that runs the pipeline to access the resources included in the ingestion request (Google Drive folders or files). In case of detecting a retryable error, the pipeline will hold the retry for 10 minutes before re-sending the message to the upstream PubSub topic; each error is retried at most 5 times before being sent to the dead letter GCS bucket.&lt;/p>
&lt;p>In all cases of events ending on the dead letter destination, the inspection and re-processing must be done in a manual process.&lt;/p>
&lt;h4 id="process-embeddings">Process Embeddings&lt;/h4>
&lt;p>Once the content has been extracted from the request, or captured from Google Drive files, the pipeline will trigger the embeddings computation process. As previously mentioned the interactions with the Vertex AI Foundational Models API is implemented in Python language. For this reason we need to format the extracted content in Java types that have a direct translation to those existing in the Python world. Those are key-values (in Python those are 2-element tuples), Strings (available in both languages), and iterables (also available in both languages). We could have implemented coders in both languages to support custom transport types, but we opted out of that in favor of clarity and simplicity.&lt;/p>
&lt;p>Before computing the content’s embeddings we decided to introduce a Reshuffle step, making the output consistent to downstream stages, with the idea of avoiding the content extraction step being repeated in case of errors. This should avoid putting pressure on existing access quotas on Google Drive related APIs.&lt;/p>
&lt;p>The pipeline will then chunk the content in configurable sizes and also configurable overlapping, good parameters are hard to get for generic effective data extraction, so we opted to use smaller chunks with small overlapping factor as the default settings to favor diversity on the document results (at least that’s what we see from the empirical results obtained).&lt;/p>
&lt;p class="center-block">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-4-processembeddings1.png"
alt="Embeddings Processing">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-4-processembeddings2.png"
alt="Embeddings Processing">
&lt;/p>
&lt;p>Once the embeddings vectors are retrieved from the embeddings Vertex AI LLM, we will consolidate them again avoiding repetition of this step in case of downstream errors.&lt;/p>
&lt;p>Worth to notice that this pipeline is interacting directly with Vertex AI models using the client SDKs, Apache Beam already provides supports for this interactions through the RunInference PTransform (see an example &lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/inference/vertex_ai_llm_text_classification.py">here&lt;/a>).&lt;/p>
&lt;h4 id="content-storage">Content Storage&lt;/h4>
&lt;p>Once the embeddings are computed for the content chunks extracted from the ingested documents, we need to store the vectors in a searchable storage and also the textual content that correlates with those embeddings. We will be using the embeddings vectors as a semantic match later from the query service, and the textual content that corresponds to those embeddings for LLM context as a way to improve and guide the response expectations.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-5-storecontent.png"
alt="Content Storage">&lt;/p>
&lt;p>With that in mind is that in mind we split the consolidated embeddings into 3 paths, one that stores the vectors into Vertex AI Vector Search (using simple REST calls), another storing the textual content into BigTable (for low latency retrieval after semantic matching) and the final one as a potential clean up of content refresh or re ingestion (more on that later). The three paths are using the ingested document identifier as the correlating data on the actions, this key is formed by the document name (in case of available), the document identifier and the chunk sequence number. The reason for using identifiers for the chunk comes behind the idea of subsequent updates. An increase in the content will generate a larger number of chunks, and upserting all the chunks will enable always fresh data; on the contrary, a decrease in content will generate a smaller chunk count for the document’s content, this number difference can be used to delete the remaining orphan indexed chunks (from content no longer existing in the latest version of the document).&lt;/p>
&lt;h4 id="content-refresh">Content Refresh&lt;/h4>
&lt;p>The last pipeline component is the simplest, at least conceptually. After the documents from Google Drive gets ingested, an external user can produce updates in them, causing the indexed content to become out of date. We implemented a simple periodic process, inside the same streaming pipeline, that will take care of the review of already ingested documents and see if there are content updates needed. We use a GenerateSequence transform to produce a periodic impulse (every 6 hours by default), that will trigger a scan on BigTable retrieving all the ingested document identifiers. Given those identifiers we can then query Google Drive for the latest update timestamp of each document and use that marker to decide if an update is needed.&lt;/p>
&lt;p>In case of needing to update the document’s content, we can simply send an ingestion request to the upstream PubSub topic and let the pipeline run its course for this new event. Since we are taking care of upserting embeddings and cleaning up those that no longer exist, we should be capable of taking care of the majority of the additions (as long those are text updates, image based content is not being processed as of now).&lt;/p>
&lt;p class="center-block">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh1.png"
alt="Content Refresh">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh2.png"
alt="Content Refresh">
&lt;img class="center-block"
src="/images/blog/dyi-cdp-genai-beam/pipeline-6-refresh3.png"
alt="Content Refresh">
&lt;/p>
&lt;p>This task could be performed as a separate job, possibly one that is periodically scheduled in batch form. This would result in lower costs, a separate error domain, and more predictable auto scaling behavior. However, for the purposes of this demonstration, it is simpler to have a single job.&lt;/p>
&lt;p>Next, we will be focusing on how the solution interacts with external clients for ingestion and content discovery use cases.&lt;/p>
&lt;h2 id="interaction-design">Interaction Design&lt;/h2>
&lt;p>The solution aims to make the interactions for ingesting and querying the platform as simple as possible. Also, since the ingestion part may imply interacting with several services and imply retries or content refresh, we decided to make both separated and asynchronous, freeing the external users of blocking themselves while waiting for requests resolutions.&lt;/p>
&lt;h3 id="example-interactions">Example Interactions&lt;/h3>
&lt;p>Once the platform is deployed in a GCP project, a simple way to interact with the services is through the use of a web client, curl is a good example. Also, since the endpoints are authenticated, a client needs to include its credentials in the request header to have its access granted.&lt;/p>
&lt;p>Here is an example of an interaction for content ingestion:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST -H &amp;#34;Content-Type: application/json&amp;#34; -H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; https://&amp;lt;service-address&amp;gt;/ingest/content/gdrive -d $&amp;#39;{&amp;#34;url&amp;#34;:&amp;#34;https://drive.google.com/drive/folders/somefolderid&amp;#34;}&amp;#39; | jq .
# response from service
{
&amp;#34;status&amp;#34;: &amp;#34;Ingestion trace id: &amp;lt;some identifier&amp;gt;&amp;#34;
}
&lt;/code>&lt;/pre>&lt;p>In this case, after the ingestion request has been sent to the PubSub topic for processing, the service will return the tracking identifier, which maps with the PubSub message identifier. Note the provided URL can be one of a Google Doc or a Google Drive folder, in the later case the ingestion process will crawl the folder’s content recursively to retrieve all the contained documents and their contents.&lt;/p>
&lt;p>Next, an example of a content query interaction, very similar to the previous one:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;summarize the benefits of using VertexAI foundational models for Generative AI applications&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;VertexAI Foundation Models are a set of pre-trained models that can be used to accelerate the development of machine learning applications. They are available for a variety of tasks, including natural language processing, computer vision, and recommendation systems.\n\nVertexAI Foundation Models can be used to improve the performance of Generative AI applications by providing a starting point for model development. They can also be used to reduce the amount of time and effort required to train a model.\n\nIn addition, VertexAI Foundation Models can be used to improve the accuracy and robustness of Generative AI applications. This is because they are trained on large datasets and are subject to rigorous quality control.\n\nOverall, VertexAI Foundation Models can be a valuable resource for developers who are building Generative AI applications. They can help to accelerate the development process, reduce the cost of development, and improve the performance and accuracy of applications.&amp;#34;,
&amp;#34;previousConversationSummary&amp;#34;: &amp;#34;&amp;#34;,
&amp;#34;sourceLinks&amp;#34;: [
{
&amp;#34;link&amp;#34;: &amp;#34;&amp;lt;possibly some ingested doc url/id&amp;gt;&amp;#34;,
&amp;#34;distance&amp;#34;: 0.7233397960662842
}
],
&amp;#34;citationMetadata&amp;#34;: [
{
&amp;#34;citations&amp;#34;: []
}
],
&amp;#34;safetyAttributes&amp;#34;: [
{
&amp;#34;categories&amp;#34;: [],
&amp;#34;scores&amp;#34;: [],
&amp;#34;blocked&amp;#34;: false
}
]
}
&lt;/code>&lt;/pre>&lt;p>The platform will answer the request with a textual response from the LLM and include as well information about the categorization, citation metadata and source links (if available) of the content used to generate the response (this are for example, Google Docs links of the documents previously ingested by the platform).&lt;/p>
&lt;p>When interacting with the services, a good query will generally return good results, the clearer the query the easier it will be to contextualize its meaning and more accurate information will be sent to the LLMs to retrieve answers. But having to include all the details of the query context in a phrase on every exchange with the service can be very cumbersome and difficult. For that case the platform can use a provided session identifier that will be used to store all the previous exchanges between a user and the platform. This should help the implementation to better contextualize the initial query embeddings matching and even provide more concise contextual information in the model requests. Here is an example of a contextual exchange:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;summarize the benefits of using VertexAI foundational models for Generative AI applications?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;VertexAI Foundational Models are a suite of pre-trained models that can be used to accelerate the development of Generative AI applications. These models are available in a variety of languages and domains, and they can be used to generate text, images, audio, and other types of content.\n\nUsing VertexAI Foundational Models can help you to:\n\n* Reduce the time and effort required to develop Generative AI applications\n* Improve the accuracy and quality of your models\n* Access the latest research and development in Generative AI\n\nVertexAI Foundational Models are a powerful tool for developers who want to create innovative and engaging Generative AI applications.&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;describe the available LLM models?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;The VertexAI Foundational Models suite includes a variety of LLM models, including:\n\n* Text-to-text LLMs: These models can generate text based on a given prompt. They can be used for tasks such as summarization, translation, and question answering.\n* Image-to-text LLMs: These models can generate text based on an image. They can be used for tasks such as image captioning and description generation.\n* Audio-to-text LLMs: These models can generate text based on an audio clip. They can be used for tasks such as speech recognition and transcription.\n\nThese models are available in a variety of languages, including English, Spanish, French, German, and Japanese. They can be used to create a wide range of Generative AI applications, such as chatbots, customer service applications, and creative writing tools.&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;do rate limit apply for those LLMs?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;Yes, there are rate limits for the VertexAI Foundational Models. The rate limits are based on the number of requests per second and the total number of requests per day. For more information, please see the [VertexAI Foundational Models documentation](https://cloud.google.com/vertex-ai/docs/foundational-models#rate-limits).&amp;#34;,
…
}
$ &amp;gt; curl -X POST \
-H &amp;#34;Content-Type: application/json&amp;#34; \
-H &amp;#34;Authorization: Bearer $(gcloud auth print-identity-token)&amp;#34; \
https://&amp;lt;service-address&amp;gt;/query/content \
-d $&amp;#39;{&amp;#34;text&amp;#34;:&amp;#34;care to share the price?&amp;#34;, &amp;#34;sessionId&amp;#34;: &amp;#34;some-session-id&amp;#34;}&amp;#39; \
| jq .
# response from service
{
&amp;#34;content&amp;#34;: &amp;#34;The VertexAI Foundational Models are priced based on the number of requests per second and the total number of requests per day. For more information, please see the [VertexAI Foundational Models pricing page](https://cloud.google.com/vertex-ai/pricing#foundational-models).&amp;#34;,
…
}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Usage Tip:&lt;/strong> in case of abruptly changing topics, sometimes is better to use a new session identifier.&lt;/p>
&lt;h3 id="deployment">Deployment&lt;/h3>
&lt;p>As part of the platform solution, there are a set of scripts that help with the deployment of all the different components. By running the &lt;code>start.sh&lt;/code> and setting the right parameters (GCP project, terraform state bucket and name for the platform instance) the script will take care of building the code, deploying the needed containers (service endpoint container and Dataflow python custom container), deploying all the GCP resources using Terraform and finally deploying the pipeline. There is also the possibility of modifying the pipeline’s execution by passing an extra parameter to the startup script, for example: &lt;code>start.sh &amp;lt;gcp project&amp;gt; &amp;lt;state-bucket-name&amp;gt; &amp;lt;a run name&amp;gt; &amp;quot;--update&amp;quot;&lt;/code> will update the content extraction pipeline in-place.&lt;/p>
&lt;p>Also, in case of wanting to focus only on the deployment of specific components other scripts have been included to help with those specific tasks (build the solution, deploy the infrastructure, deploy the pipeline, deploy the services, etc.).&lt;/p>
&lt;h3 id="solutions-notes">Solution&amp;rsquo;s Notes&lt;/h3>
&lt;p>This solution is designed to serve as an example for learning purposes. Many of the configuration values for the extraction pipeline and security restrictions are provided only as examples. The solution doesn&amp;rsquo;t propagate the existing access control lists (ACLs) of the ingested content. As a result, all users that have access to the service endpoints have access to summarizations of the ingested content from those original documents.&lt;/p>
&lt;h3 id="notes-about-the-source-code">Notes about the source code&lt;/h3>
&lt;p>The source code for the content discovery platform is available in &lt;a href="https://github.com/prodriguezdefino/content-dicovery-platform-gcp">Github&lt;/a>. You can run it in any Google Cloud project. The repository includes the source code for the integration services, the multi-language ingestion pipeline, and the deployment automation through Terraform. If you deploy this example, it might take up to 90 minutes to create and configure all the needed resources. The README file contains additional documentation about the deployment prerequisites and example REST interactions.&lt;/p></description><link>/blog/dyi-content-discovery-platform-genai-beam/</link><pubDate>Mon, 02 Oct 2023 00:00:01 -0800</pubDate><guid>/blog/dyi-content-discovery-platform-genai-beam/</guid><category>blog</category></item><item><title>Apache Beam 2.50.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.50.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2500-2023-08-30">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.50.0, check out the &lt;a href="https://github.com/apache/beam/milestone/14">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Spark 3.2.2 is used as default version for Spark runner (&lt;a href="https://github.com/apache/beam/issues/23804">#23804&lt;/a>).&lt;/li>
&lt;li>The Go SDK has a new default local runner, called Prism (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Java KafkaIO now supports picking up topics via topicPattern (&lt;a href="https://github.com/apache/beam/pull/26948">#26948&lt;/a>)&lt;/li>
&lt;li>Support for read from Cosmos DB Core SQL API (&lt;a href="https://github.com/apache/beam/issues/23604">#23604&lt;/a>)&lt;/li>
&lt;li>Upgraded to HBase 2.5.5 for HBaseIO. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">#27711&lt;/a>)&lt;/li>
&lt;li>Added support for GoogleAdsIO source (Java) (&lt;a href="https://github.com/apache/beam/pull/27681">#27681&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>The Go SDK now requires Go 1.20 to build. (&lt;a href="https://github.com/apache/beam/issues/27558">#27558&lt;/a>)&lt;/li>
&lt;li>The Go SDK has a new default local runner, Prism. (&lt;a href="https://github.com/apache/beam/issues/24789">#24789&lt;/a>).
&lt;ul>
&lt;li>Prism is a portable runner that executes each transform independantly, ensuring coders.&lt;/li>
&lt;li>At this point it supercedes the Go direct runner in functionality. The Go direct runner is now deprecated.&lt;/li>
&lt;li>See &lt;a href="https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md">https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/runners/prism/README.md&lt;/a> for the goals and features of Prism.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Hugging Face Model Handler for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/26632">#26632&lt;/a>)&lt;/li>
&lt;li>Hugging Face Pipelines support for RunInference added to Python SDK. (&lt;a href="https://github.com/apache/beam/pull/27399">#27399&lt;/a>)&lt;/li>
&lt;li>Vertex AI Model Handler for RunInference now supports private endpoints (&lt;a href="https://github.com/apache/beam/pull/27696">#27696&lt;/a>)&lt;/li>
&lt;li>MLTransform transform added with support for common ML pre/postprocessing operations (&lt;a href="https://github.com/apache/beam/pull/26795">#26795&lt;/a>)&lt;/li>
&lt;li>Upgraded the Kryo extension for the Java SDK to Kryo 5.5.0. This brings in bug fixes, performance improvements, and serialization of Java 14 records. (&lt;a href="https://github.com/apache/beam/issues/27635">#27635&lt;/a>)&lt;/li>
&lt;li>All Beam released container images are now &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/build-multi-arch-for-arm#what_is_a_multi-arch_image">multi-arch images&lt;/a> that support both x86 and ARM CPU architectures. (&lt;a href="https://github.com/apache/beam/issues/27674">#27674&lt;/a>). The multi-arch container images include:
&lt;ul>
&lt;li>All versions of Go, Python, Java and Typescript SDK containers.&lt;/li>
&lt;li>All versions of Flink job server containers.&lt;/li>
&lt;li>Java and Python expansion service containers.&lt;/li>
&lt;li>Transform service controller container.&lt;/li>
&lt;li>Spark3 job server container.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Added support for batched writes to AWS SQS for improved throughput (Java, AWS 2).(&lt;a href="https://github.com/apache/beam/issues/21429">#21429&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Python SDK: Legacy runner support removed from Dataflow, all pipelines must use runner v2.&lt;/li>
&lt;li>Python SDK: Dataflow Runner will no longer stage Beam SDK from PyPI in the &lt;code>--staging_location&lt;/code> at pipeline submission. Custom container images that are not based on Beam&amp;rsquo;s default image must include Apache Beam installation.(&lt;a href="https://github.com/apache/beam/issues/26996">#26996&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>The Go Direct Runner is now Deprecated. It remains available to reduce migration churn.
&lt;ul>
&lt;li>Tests can be set back to the direct runner by overriding TestMain: &lt;code>func TestMain(m *testing.M) { ptest.MainWithDefault(m, &amp;quot;direct&amp;quot;) }&lt;/code>&lt;/li>
&lt;li>It&amp;rsquo;s recommended to fix issues seen in tests using Prism, as they can also happen on any portable runner.&lt;/li>
&lt;li>Use the generic register package for your pipeline DoFns to ensure pipelines function on portable runners, like prism.&lt;/li>
&lt;li>Do not rely on closures or using package globals for DoFn configuration. They don&amp;rsquo;t function on portable runners.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed DirectRunner bug in Python SDK where GroupByKey gets empty PCollection and fails when pipeline option &lt;code>direct_num_workers!=1&lt;/code>.(&lt;a href="https://github.com/apache/beam/pull/27373">#27373&lt;/a>)&lt;/li>
&lt;li>Fixed BigQuery I/O bug when estimating size on queries that utilize row-level security (&lt;a href="https://github.com/apache/beam/pull/27474">#27474&lt;/a>)&lt;/li>
&lt;li>Beam Python containers rely on a version of Debian/aom that has several security vulnerabilities: &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30474">CVE-2021-30474&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30475">CVE-2021-30475&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2021-30473">CVE-2021-30473&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36133">CVE-2020-36133&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36131">CVE-2020-36131&lt;/a>, &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36130">CVE-2020-36130&lt;/a>, and &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2020-36135">CVE-2020-36135&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;li>Python Pipelines using BigQuery IO or &lt;code>orjson&lt;/code> dependency might experience segmentation faults or get stuck: &lt;a href="https://github.com/apache/beam/issues/28318">#28318&lt;/a>.&lt;/li>
&lt;li>Python SDK&amp;rsquo;s cross-language Bigtable sink mishandles records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>. To avoid this issue, set explicit timestamps for all records before writing to Bigtable.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.50.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abacn&lt;/p>
&lt;p>acejune&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>ahmedabu98&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>al97&lt;/p>
&lt;p>Aleksandr Dudko&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrey Devyatkin&lt;/p>
&lt;p>Anton Shalkovich&lt;/p>
&lt;p>ArjunGHUB&lt;/p>
&lt;p>Bjorn Pedersen&lt;/p>
&lt;p>BjornPrime&lt;/p>
&lt;p>Brett Morgan&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Buqian Zheng&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>case-k&lt;/p>
&lt;p>Celeste Zeng&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Connor Brett&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Damon Douglas&lt;/p>
&lt;p>Dan Hansen&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmytro Sadovnychyi&lt;/p>
&lt;p>Florent Biville&lt;/p>
&lt;p>Gabriel Lacroix&lt;/p>
&lt;p>Hai Joey Tran&lt;/p>
&lt;p>Hong Liang Teoh&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jeff Kinard&lt;/p>
&lt;p>Jeff Zhang&lt;/p>
&lt;p>Jing&lt;/p>
&lt;p>johnjcasey&lt;/p>
&lt;p>jon esperanza&lt;/p>
&lt;p>Josef Šimánek&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Laksh&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>liferoad&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>Mahmud Ridwan&lt;/p>
&lt;p>Manav Garg&lt;/p>
&lt;p>Marco Vela&lt;/p>
&lt;p>martin trieu&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Moritz Mack&lt;/p>
&lt;p>mosche&lt;/p>
&lt;p>Peter Sobot&lt;/p>
&lt;p>Pranav Bhandari&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>RyuSA&lt;/p>
&lt;p>Saba Sathya&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Steven Niemitz&lt;/p>
&lt;p>Steven van Rossum&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Tony Tang&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vitaly Terentyev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>Yichi Zhang&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zechen Jiang&lt;/p></description><link>/blog/beam-2.50.0/</link><pubDate>Wed, 30 Aug 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.50.0/</guid><category>blog</category><category>release</category></item><item><title>Apache Beam 2.49.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.49.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2490-2023-07-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.49.0, check out the &lt;a href="https://github.com/apache/beam/milestone/13">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for Bigtable Change Streams added in Java &lt;code>BigtableIO.ReadChangeStream&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/27183">#27183&lt;/a>).&lt;/li>
&lt;li>Added Bigtable Read and Write cross-language transforms to Python SDK ((&lt;a href="https://github.com/apache/beam/issues/26593">#26593&lt;/a>), (&lt;a href="https://github.com/apache/beam/issues/27146">#27146&lt;/a>)).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Allow prebuilding large images when using &lt;code>--prebuild_sdk_container_engine=cloud_build&lt;/code>, like images depending on &lt;code>tensorflow&lt;/code> or &lt;code>torch&lt;/code> (&lt;a href="https://github.com/apache/beam/pull/27023">#27023&lt;/a>).&lt;/li>
&lt;li>Disabled &lt;code>pip&lt;/code> cache when installing packages on the workers. This reduces the size of prebuilt Python container images (&lt;a href="https://github.com/apache/beam/pull/27035">#27035&lt;/a>).&lt;/li>
&lt;li>Select dedicated avro datum reader and writer (Java) (&lt;a href="https://github.com/apache/beam/issues/18874">#18874&lt;/a>).&lt;/li>
&lt;li>Timer API for the Go SDK (Go) (&lt;a href="https://github.com/apache/beam/issues/22737">#22737&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Python 3.7 support. (&lt;a href="https://github.com/apache/beam/issues/26447">#26447&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed KinesisIO &lt;code>NullPointerException&lt;/code> when a progress check is made before the reader is started (IO) (&lt;a href="https://github.com/apache/beam/issues/23868">#23868&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Long-running Python pipelines might experience a memory leak: &lt;a href="https://github.com/apache/beam/issues/28246">#28246&lt;/a>.&lt;/li>
&lt;li>Python SDK&amp;rsquo;s cross-language Bigtable sink mishandles records that don&amp;rsquo;t have an explicit timestamp set: &lt;a href="https://github.com/apache/beam/issues/28632">#28632&lt;/a>. To avoid this issue, set explicit timestamps for all records before writing to Bigtable.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.49.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abzal Tuganbay&lt;/p>
&lt;p>AdalbertMemSQL&lt;/p>
&lt;p>Ahmed Abualsaud&lt;/p>
&lt;p>Ahmet Altay&lt;/p>
&lt;p>Alan Zhang&lt;/p>
&lt;p>Alexey Romanenko&lt;/p>
&lt;p>Anand Inguva&lt;/p>
&lt;p>Andrei Gurau&lt;/p>
&lt;p>Arwin Tio&lt;/p>
&lt;p>Bartosz Zablocki&lt;/p>
&lt;p>Bruno Volpato&lt;/p>
&lt;p>Burke Davison&lt;/p>
&lt;p>Byron Ellis&lt;/p>
&lt;p>Chamikara Jayalath&lt;/p>
&lt;p>Charles Rothrock&lt;/p>
&lt;p>Chris Gavin&lt;/p>
&lt;p>Claire McGinty&lt;/p>
&lt;p>Clay Johnson&lt;/p>
&lt;p>Damon&lt;/p>
&lt;p>Daniel Dopierała&lt;/p>
&lt;p>Danny McCormick&lt;/p>
&lt;p>Darkhan Nausharipov&lt;/p>
&lt;p>David Cavazos&lt;/p>
&lt;p>Dip Patel&lt;/p>
&lt;p>Dmitry Repin&lt;/p>
&lt;p>Gavin McDonald&lt;/p>
&lt;p>Jack Dingilian&lt;/p>
&lt;p>Jack McCluskey&lt;/p>
&lt;p>James Fricker&lt;/p>
&lt;p>Jan Lukavský&lt;/p>
&lt;p>Jasper Van den Bossche&lt;/p>
&lt;p>John Casey&lt;/p>
&lt;p>John Gill&lt;/p>
&lt;p>Joseph Crowley&lt;/p>
&lt;p>Kanishk Karanawat&lt;/p>
&lt;p>Katie Liu&lt;/p>
&lt;p>Kenneth Knowles&lt;/p>
&lt;p>Kyle Galloway&lt;/p>
&lt;p>Liam Miller-Cushon&lt;/p>
&lt;p>MakarkinSAkvelon&lt;/p>
&lt;p>Masato Nakamura&lt;/p>
&lt;p>Mattie Fu&lt;/p>
&lt;p>Michel Davit&lt;/p>
&lt;p>Naireen Hussain&lt;/p>
&lt;p>Nathaniel Young&lt;/p>
&lt;p>Nelson Osacky&lt;/p>
&lt;p>Nick Li&lt;/p>
&lt;p>Oleh Borysevych&lt;/p>
&lt;p>Pablo Estrada&lt;/p>
&lt;p>Reeba Qureshi&lt;/p>
&lt;p>Reuven Lax&lt;/p>
&lt;p>Ritesh Ghorse&lt;/p>
&lt;p>Robert Bradshaw&lt;/p>
&lt;p>Robert Burke&lt;/p>
&lt;p>Rouslan&lt;/p>
&lt;p>Saadat Su&lt;/p>
&lt;p>Sam Rohde&lt;/p>
&lt;p>Sam Whittle&lt;/p>
&lt;p>Sanil Jain&lt;/p>
&lt;p>Shunping Huang&lt;/p>
&lt;p>Smeet nagda&lt;/p>
&lt;p>Svetak Sundhar&lt;/p>
&lt;p>Timur Sultanov&lt;/p>
&lt;p>Udi Meiri&lt;/p>
&lt;p>Valentyn Tymofieiev&lt;/p>
&lt;p>Vlado Djerek&lt;/p>
&lt;p>WuA&lt;/p>
&lt;p>XQ Hu&lt;/p>
&lt;p>Xianhua Liu&lt;/p>
&lt;p>Xinyu Liu&lt;/p>
&lt;p>Yi Hu&lt;/p>
&lt;p>Zachary Houfek&lt;/p>
&lt;p>alexeyinkin&lt;/p>
&lt;p>bigduu&lt;/p>
&lt;p>bullet03&lt;/p>
&lt;p>bzablocki&lt;/p>
&lt;p>jonathan-lemos&lt;/p>
&lt;p>jubebo&lt;/p>
&lt;p>magicgoody&lt;/p>
&lt;p>ruslan-ikhsan&lt;/p>
&lt;p>sultanalieva-s&lt;/p>
&lt;p>vitaly.terentyev&lt;/p></description><link>/blog/beam-2.49.0/</link><pubDate>Mon, 17 Jul 2023 09:00:00 -0400</pubDate><guid>/blog/beam-2.49.0/</guid><category>blog</category><category>release</category></item></channel></rss>