<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Apache Beam</title><description>Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.</description><link>/</link><generator>Hugo -- gohugo.io</generator><item><title>Apache Beam 2.39.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.39.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2390-2022-05-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.39.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ConfigureReleaseNote.jspa?projectId=12319527&amp;amp;version=12351170">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>JmsIO gains the ability to map any kind of input to any subclass of &lt;code>javax.jms.Message&lt;/code> (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).&lt;/li>
&lt;li>JmsIO introduces the ability to write to dynamic topics (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).
&lt;ul>
&lt;li>A &lt;code>topicNameMapper&lt;/code> must be set to extract the topic name from the input value.&lt;/li>
&lt;li>A &lt;code>valueMapper&lt;/code> must be set to convert the input value to JMS message.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reduce number of threads spawned by BigqueryIO StreamingInserts (
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14283">BEAM-14283&lt;/a>).&lt;/li>
&lt;li>Implemented Apache PulsarIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8218">BEAM-8218&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Support for flink scala 2.12, because most of the libraries support version 2.12 onwards. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14386">beam-14386&lt;/a>)&lt;/li>
&lt;li>&amp;lsquo;Manage Clusters&amp;rsquo; JupyterLab extension added for users to configure usage of Dataproc clusters managed by Interactive Beam (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>Pipeline drain support added for Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>). &lt;strong>Note: this feature is not yet fully validated and should be treated as experimental in this release.&lt;/strong>&lt;/li>
&lt;li>&lt;code>DataFrame.unstack()&lt;/code>, &lt;code>DataFrame.pivot() &lt;/code> and &lt;code>Series.unstack()&lt;/code>
implemented for DataFrame API (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13948">BEAM-13948&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13966">BEAM-13966&lt;/a>).&lt;/li>
&lt;li>Support for impersonation credentials added to dataflow runner in the Java and Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14014">BEAM-14014&lt;/a>).&lt;/li>
&lt;li>Implemented Jupyterlab extension for managing Dataproc clusters (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>ExternalPythonTransform API added for easily invoking Python transforms from
Java (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14143">BEAM-14143&lt;/a>).&lt;/li>
&lt;li>Added Add support for Elasticsearch 8.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14003">BEAM-14003&lt;/a>).&lt;/li>
&lt;li>Shard aware Kinesis record aggregation (AWS Sdk v2), (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14104">BEAM-14104&lt;/a>).&lt;/li>
&lt;li>Upgrade to ZetaSQL 2022.04.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14348">BEAM-14348&lt;/a>).&lt;/li>
&lt;li>Fixed ReadFromBigQuery cannot be used with the interactive runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14112">BEAM-14112&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Unused functions &lt;code>ShallowCloneParDoPayload()&lt;/code>, &lt;code>ShallowCloneSideInput()&lt;/code>, and &lt;code>ShallowCloneFunctionSpec()&lt;/code> have been removed from the Go SDK&amp;rsquo;s pipelinex package (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13739">BEAM-13739&lt;/a>).&lt;/li>
&lt;li>JmsIO requires an explicit &lt;code>valueMapper&lt;/code> to be set (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>). You can use the &lt;code>TextMessageMapper&lt;/code> to convert &lt;code>String&lt;/code> inputs to JMS &lt;code>TestMessage&lt;/code>s:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="n">JmsIO&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">write&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">withConnectionFactory&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">jmsConnectionFactory&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">withValueMapper&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">TextMessageMapper&lt;/span>&lt;span class="o">());&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Coders in Python are expected to inherit from Coder. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14351">BEAM-14351&lt;/a>).&lt;/li>
&lt;li>New abstract method &lt;code>metadata()&lt;/code> added to io.filesystem.FileSystem in the
Python SDK. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14314">BEAM-14314&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Flink 1.11 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14139">BEAM-14139&lt;/a>).&lt;/li>
&lt;li>Python 3.6 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13657">BEAM-13657&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java Spanner IO NPE when ProjectID not specified in template executions (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14405">BEAM-14405&lt;/a>).&lt;/li>
&lt;li>Fixed potential NPE in BigQueryServicesImpl.getErrorInfo (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14133">BEAM-14133&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/browse/BEAM-14412?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.39.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.39.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,
Ahmet Altay,
Aizhamal Nurmamat kyzy,
Alexander Zhuravlev,
Alexey Romanenko,
Anand Inguva,
Andrei Gurau,
Andrew Pilloud,
Andy Ye,
Arun Pandian,
Arwin Tio,
Aydar Farrakhov,
Aydar Zainutdinov,
AydarZaynutdinov,
Balázs Németh,
Benjamin Gonzalez,
Brian Hulette,
Buqian Zheng,
Chamikara Jayalath,
Chun Yang,
Daniel Oliveira,
Daniela Martín,
Danny McCormick,
David Huntsperger,
Deepak Nagaraj,
Denise Case,
Esun Kim,
Etienne Chauchot,
Evan Galpin,
Hector Miuler Malpica Gallegos,
Heejong Lee,
Hengfeng Li,
Ilango Rajagopal,
Ilion Beyst,
Israel Herraiz,
Jack McCluskey,
Kamil Bregula,
Kamil Breguła,
Ke Wu,
Kenneth Knowles,
KevinGG,
Kiley,
Kiley Sok,
Kyle Weaver,
Liam Miller-Cushon,
Luke Cwik,
Marco Robles,
Matt Casters,
Michael Li,
MiguelAnzoWizeline,
Milan Patel,
Minbo Bae,
Moritz Mack,
Nick Caballero,
Niel Markwick,
Ning Kang,
Oskar Firlej,
Pablo Estrada,
Pavel Avilov,
Reuven Lax,
Reza Rokni,
Ritesh Ghorse,
Robert Bradshaw,
Robert Burke,
Ryan Thompson,
Sam Whittle,
Steven Niemitz,
Thiago Nunes,
Tomo Suzuki,
Valentyn Tymofieiev,
Victor,
Yi Hu,
Yichi Zhang,
Yiru Tang,
ahmedabu98,
andoni-guzman,
brachipa,
bulat safiullin,
bullet03,
dannymartinm,
daria.malkova,
dpcollins-google,
egalpin,
emily,
fbeevikm,
johnjcasey,
kileys,
&lt;a href="mailto:msbukal@google.com">msbukal@google.com&lt;/a>,
nguyennk92,
pablo rodriguez defino,
rszper,
rvballada,
sachinag,
tvalentyn,
vachan-shetty,
yirutang&lt;/p></description><link>/blog/beam-2.39.0/</link><pubDate>Wed, 25 May 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.39.0/</guid><category>blog</category></item><item><title>Running Beam SQL in notebooks</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>&lt;a href="https://beam.apache.org/documentation/dsls/sql/overview/">Beam SQL&lt;/a> allows a
Beam user to query PCollections with SQL statements.
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/runners/interactive#interactive-beam">Interactive Beam&lt;/a>
provides an integration between Apache Beam and
&lt;a href="https://docs.jupyter.org/en/latest/">Jupyter Notebooks&lt;/a> (formerly known as
IPython Notebooks) to make pipeline prototyping and data exploration much faster
and easier.
You can set up your own notebook user interface (for example,
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html">JupyterLab&lt;/a>
or classic &lt;a href="https://docs.jupyter.org/en/latest/install.html">Jupyter Notebooks&lt;/a>)
on your own device following their documentations. Alternatively, you can
choose a hosted solution that does everything for you. You are free to select
whichever notebook user interface you prefer. For simplicity, this
post does not go through the notebook environment setup and uses
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development">Apache Beam Notebooks&lt;/a>
that provides a cloud-hosted
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/">JupyterLab&lt;/a> environment and lets
a Beam user iteratively develop pipelines, inspect pipeline graphs, and parse
individual PCollections in a read-eval-print-loop (REPL) workflow.&lt;/p>
&lt;p>In this post, you will see how to use &lt;code>beam_sql&lt;/code>, a notebook
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">magic&lt;/a>, to
execute Beam SQL in notebooks and inspect the results.&lt;/p>
&lt;p>By the end of the post, it also demonstrates how to use the &lt;code>beam_sql&lt;/code> magic
with a production environment, such as running it as a one-shot job on
Dataflow. It&amp;rsquo;s optional. To follow those steps, you should have a project in
Google Cloud Platform with
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#before_you_begin">necessary APIs enabled&lt;/a>
, and you should have enough permissions to create a Google Cloud Storage bucket
(or to use an existing one), query a public Google Cloud BigQuery dataset, and
run Dataflow jobs.&lt;/p>
&lt;p>If you choose to use the cloud hosted notebook solution, once you have your
Google Cloud project ready, you will need to create an Apache Beam Notebooks
instance and open the JupyterLab web interface. Please follow the instructions
given at:
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance">https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance&lt;/a>&lt;/p>
&lt;h2 id="getting-familiar-with-the-environment">Getting familiar with the environment&lt;/h2>
&lt;h3 id="landing-page">Landing page&lt;/h3>
&lt;p>After starting your own notebook user interface: for example, if using Apche
Beam Notebooks, after clicking the &lt;code>OPEN JUPYTERLAB&lt;/code> link, you will land on
the default launcher page of the notebook environment.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image1.png"
alt="Beam SQL in Notebooks: landing page">&lt;/p>
&lt;p>On the left side, there is a file explorer to view examples, tutorials and
assets on the notebook instance. To easily navigate the files, you may
double-click the &lt;code>00-Start_Here.md&lt;/code> (#1 in the screenshot) file to view detailed
information about the files.&lt;/p>
&lt;p>On the right side, it displays the default launcher page of JupyterLab. To
create and open a completely new notebook file and code with a selected version
of Apache Beam, click one of (#2) the items with Apache Beam &amp;gt;=2.34.0 (because
&lt;code>beam_sql&lt;/code> was introduced in 2.34.0) installed.&lt;/p>
&lt;h3 id="createopen-a-notebook">Create/open a notebook&lt;/h3>
&lt;p>For example, if you clicked the image button with Apache Beam 2.36.0, you would
see an &lt;code>Untitled.ipynb&lt;/code> file created and opened.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image2.png"
alt="Beam SQL in Notebooks: create/open a notebook ">&lt;/p>
&lt;p>In the file explorer, your new notebook file has been created as
&lt;code>Untitled.ipynb&lt;/code>.&lt;/p>
&lt;p>On the right side, in the opened notebook, there are 4 buttons on top that you
may interact most frequently with:&lt;/p>
&lt;ul>
&lt;li>#1: insert an empty code block after the selected / highlighted code block&lt;/li>
&lt;li>#2: execute the code in the block that is selected / highlighted&lt;/li>
&lt;li>#3: interrupt code execution if your code execution is stuck&lt;/li>
&lt;li>#4: “Restart the kernel”: clear all states from code executions and start
from fresh&lt;/li>
&lt;/ul>
&lt;p>There is a button on the top-right (#5) for you to choose a different Apache
Beam version if needed, so it’s not set in stone.&lt;/p>
&lt;p>You can always double-click a file from the file explorer to open it without
creating a new one.&lt;/p>
&lt;h2 id="beam-sql">Beam SQL&lt;/h2>
&lt;h3 id="beam_sql-magic">&lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>&lt;code>beam_sql&lt;/code> is an IPython
&lt;a href="https://ipython.readthedocs.io/en/stable/config/custommagics.html">custom magic&lt;/a>.
If you&amp;rsquo;re not familiar with magics, here are some
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">built-in examples&lt;/a>.
It&amp;rsquo;s a convenient way to validate your queries locally against known/test data
sources when prototyping a Beam pipeline with SQL, before productionizing it on
remote cluster/services.&lt;/p>
&lt;p>The Apache Beam Notebooks environment has preloaded the &lt;code>beam_sql&lt;/code> magic and
basic &lt;code>apache-beam&lt;/code> modules so you can directly use them without additional
imports. You can also explicitly load the magic via
&lt;code>%load_ext apache_beam.runners.interactive.sql.beam_sql_magics&lt;/code> and
&lt;code>apache-beam&lt;/code> modules if you set up your own notebook elsewhere.&lt;/p>
&lt;p>You can type:&lt;/p>
&lt;pre>&lt;code>%beam_sql -h
&lt;/code>&lt;/pre>&lt;p>and then execute the code to learn how to use the magic:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image3.png"
alt="Beam SQL in Notebooks: beam_sql magic help message ">&lt;/p>
&lt;p>The selected/highlighted block is called a notebook cell. It mainly has 3
components:&lt;/p>
&lt;ul>
&lt;li>#1: The execution count. &lt;code>[1]&lt;/code> indicates this block is the first executed
code. It increases by 1 for each piece of code you execute even if you
re-execute the same piece of code. &lt;code>[ ]&lt;/code> indicates this block is not
executed.&lt;/li>
&lt;li>#2: The cell input: the code gets executed.&lt;/li>
&lt;li>#3: The cell output: the output of the code execution. Here it contains the
help documentation of the &lt;code>beam_sql&lt;/code> magic.&lt;/li>
&lt;/ul>
&lt;h3 id="create-a-pcollection">Create a PCollection&lt;/h3>
&lt;p>There are 3 scenarios for Beam SQL when creating a PCollection:&lt;/p>
&lt;ol>
&lt;li>Use Beam SQL to create a PCollection from constant values&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>%%beam_sql -o pcoll
SELECT CAST(1 AS INT) AS id, CAST('foo' AS VARCHAR) AS str, CAST(3.14 AS DOUBLE) AS flt
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image4.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from raw values.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic creates and outputs a PCollection named &lt;code>pcoll&lt;/code> with
element_type like &lt;code>BeamSchema_...(id: int32, str: str, flt: float64)&lt;/code>.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> that you have &lt;strong>not&lt;/strong> explicitly created a Beam pipeline. You get a
PCollection because the &lt;code>beam_sql&lt;/code> magic always implicitly creates a pipeline to
execute your SQL query. To hold the elements with each field&amp;rsquo;s type info, Beam
automatically creates a
&lt;a href="https://beam.apache.org/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>
as the &lt;code>element_type&lt;/code> for the created PCollection. You will learn more about
schema-aware PCollections later.&lt;/p>
&lt;ol start="2">
&lt;li>Use Beam SQL to query a PCollection&lt;/li>
&lt;/ol>
&lt;p>You can chain another SQL using the output from a previous SQL (or any
schema-aware PCollection produced by any normal Beam PTransforms) as the input
to produce a new PCollection.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: if you name the output PCollection, make sure that it’s unique in your
notebook to avoid overwriting a different PCollection.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o id_pcoll
SELECT id FROM pcoll
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image5.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from another.">&lt;/p>
&lt;ol start="3">
&lt;li>Use Beam SQL to join multiple PCollections&lt;/li>
&lt;/ol>
&lt;p>You can query multiple PCollections from a single query.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o str_with_same_id
SELECT id, str FROM pcoll JOIN id_pcoll USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image6.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from multiple PCollections.">&lt;/p>
&lt;p>Now you have learned how to use the &lt;code>beam_sql&lt;/code> magic to create PCollections and
inspect their results.&lt;/p>
&lt;p>&lt;strong>Tip&lt;/strong>: if you accidentally delete some of the notebook cell outputs, you can
always check the content of a PCollection by invoking &lt;code>ib.show(pcoll_name)&lt;/code> or
&lt;code>ib.collect(pcoll_name)&lt;/code> where &lt;code>ib&lt;/code> stands for “Interactive Beam”
(&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#reading_and_visualizing_the_data">learn more&lt;/a>).&lt;/p>
&lt;h3 id="schema-aware-pcollections">Schema-aware PCollections&lt;/h3>
&lt;p>The &lt;code>beam_sql&lt;/code> magic provides the flexibility to seamlessly mix SQL and non-SQL
Beam statements to build pipelines and even run them on Dataflow. However, each
PCollection queried by Beam SQL needs to have a
&lt;a href="https://beam.apache.org/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>.
For the &lt;code>beam_sql&lt;/code> magic, it’s recommended to use &lt;code>typing.NamedTuple&lt;/code> when a
schema is desired. You can go through the below example to learn more details
about schema-aware PCollections.&lt;/p>
&lt;h4 id="setup">Setup&lt;/h4>
&lt;p>In the setup of this example, you will:&lt;/p>
&lt;ul>
&lt;li>Install PyPI package &lt;code>names&lt;/code> using the built-in &lt;code>%pip&lt;/code> magic: you will use
the module to generate some random English names as the raw data input.&lt;/li>
&lt;li>Define a schema with &lt;code>NamedTuple&lt;/code> that has 2 attributes: &lt;code>id&lt;/code> - an unique
numeric identifier of a person; &lt;code>name&lt;/code> - a string name of a person.&lt;/li>
&lt;li>Define a pipeline with an &lt;code>InteractiveRunner&lt;/code> to utilize notebook related
features of Apache Beam.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="o">%&lt;/span>&lt;span class="n">pip&lt;/span> &lt;span class="n">install&lt;/span> &lt;span class="n">names&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">names&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="nb">id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;span class="n">name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is no visible output for the code execution.&lt;/p>
&lt;h4 id="create-schema-aware-pcollections-without-using-sql">Create schema-aware PCollections without using SQL&lt;/h4>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">persons&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image7.png"
alt="Beam SQL in Notebooks: create a schema-aware PCollection without SQL.">&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">persons_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">15&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons_2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image8.png"
alt="Beam SQL in Notebooks: create another schema-aware PCollection without SQL.">&lt;/p>
&lt;p>Now you have 2 PCollections both with the same schema defined by the &lt;code>Person&lt;/code>
class:&lt;/p>
&lt;ul>
&lt;li>&lt;code>persons&lt;/code> contains 10 records for 10 persons with ids ranging from 0 to 9,&lt;/li>
&lt;li>&lt;code>persons_2&lt;/code> contains another 10 records for 10 persons with ids ranging from
5 to 14.&lt;/li>
&lt;/ul>
&lt;h4 id="encode-and-decode-of-schema-aware-pcollections">Encode and Decode of schema-aware PCollections&lt;/h4>
&lt;p>For this example, you still need one more piece of data from the first &lt;code>pcoll&lt;/code>
that you have created with instructions in this post.&lt;/p>
&lt;p>You can use the original &lt;code>pcoll&lt;/code>. Optionally, if you want to exercise using
coders explicitly with schema-aware PCollections, you can add a Text I/O into
the mix: write the content of &lt;code>pcoll&lt;/code> into a text file retaining its schema
information, then read the file back into a new schema-aware PCollection called
&lt;code>pcoll_in_file&lt;/code>, and use the new PCollection to join &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>
to find names with the common id in all three of them.&lt;/p>
&lt;p>To encode &lt;code>pcoll&lt;/code> into a file, execute:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">pcoll&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;/tmp/pcoll&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pipeline&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wait_until_finish&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="err">!&lt;/span>&lt;span class="n">cat&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">tmp&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">*&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image9.png"
alt="Beam SQL in Notebooks: write a schema-aware PCollection into a text file.">&lt;/p>
&lt;p>The above code execution writes the PCollection &lt;code>pcoll&lt;/code> (basically
&lt;code>{id: 1, str: foo, flt: 3.14}&lt;/code>) into a text file using the coder assigned by
Beam. As you can see, the file content is recorded in a binary non
human-readable format, and that’s normal.&lt;/p>
&lt;p>To decode the file content into a new PCollection, execute:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">pcoll_in_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="s1">&amp;#39;/tmp/pcoll*&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll_in_file&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image10.png"
alt="Beam SQL in Notebooks: read a schema-aware PCollection from a text file.">&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> you have to use the same coder during encoding and decoding, and
furthermore you may assign the schema explicitly to the new PCollection through
&lt;code>with_output_types()&lt;/code>.&lt;/p>
&lt;p>Reading out the encoded binary content from the text file and decoding it with
the correct coder, the content of &lt;code>pcoll&lt;/code> is recovered into &lt;code>pcoll_in_file&lt;/code>. You
can use this technique to save and share your data through any Beam I/O (not
necessarily a text file) with collaborators who work on their own pipelines (not
just in your notebook session or pipelines).&lt;/p>
&lt;h4 id="schema-in-beam_sql-magic">Schema in &lt;code>beam_sql&lt;/code> magic&lt;/h4>
&lt;p>The &lt;code>beam_sql&lt;/code> magic automatically registers a &lt;code>RowCoder&lt;/code> for your &lt;code>NamedTuple&lt;/code>
schema so that you only need to focus on preparing your data for query without
worrying about coders. To see more verbose details of what the &lt;code>beam_sql&lt;/code> magic
does behind the scenes, you can use the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>For example, you can look for all elements with &lt;code>id &amp;lt; 5&lt;/code> in &lt;code>persons&lt;/code> with the
below query and assign the output to &lt;code>persons_id_lt_5&lt;/code>.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o persons_id_lt_5 -v
SELECT * FROM persons WHERE id &amp;lt; 5
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image11.png"
alt="Beam SQL in Notebooks: beam_sql registers a schema for a PCollection.">&lt;/p>
&lt;p>Since this is the first time running this query, you might see a warning message
about:&lt;/p>
&lt;blockquote>
&lt;p>Schema Person has not been registered to use a RowCoder. Automatically
registering it by running:
beam.coders.registry.register_coder(Person, beam.coders.RowCoder)&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>beam_sql&lt;/code> magic helps registering a &lt;code>RowCoder&lt;/code> for each schema you define
and use whenever it finds one. You can also explicitly run the same code to do
so.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> the output element type is &lt;code>Person(id: int, name: str)&lt;/code> instead of
&lt;code>BeamSchema_…&lt;/code> because you have selected all the fields from a single
PCollection of the known type &lt;code>Person(id: int, name: str)&lt;/code>.&lt;/p>
&lt;p>Another example, you can query for all names from &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code> with
the same ids and assign the output to &lt;code>persons_with_common_id&lt;/code>:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o persons_with_common_id -v
SELECT * FROM persons JOIN persons_2 USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image12.png"
alt="Beam SQL in Notebooks: beam_sql creates a schema for a query.">&lt;/p>
&lt;p>Note the output element type is now some
&lt;code>BeamSchema_...(id: int64, name: str, name0: str)&lt;/code>. Because you have selected
columns from both PCollections, there is no known schema to hold the result.
Beam automatically creates a schema and differentiates the conflicted field
&lt;code>name&lt;/code> by suffixing 0 to one of them.&lt;/p>
&lt;p>And since &lt;code>Person&lt;/code> is already previously registered with a &lt;code>RowCoder&lt;/code>, there is
no more warning about registering it even with the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>Additionally, you can do a join with &lt;code>pcoll_in_file&lt;/code>, &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o entry_with_common_id
SELECT pcoll_in_file.id, persons.name AS name_1, persons_2.name AS name_2
FROM pcoll_in_file JOIN persons ON pcoll_in_file.id = persons.id
JOIN persons_2 ON pcoll_in_file.id = persons_2.id
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image13.png"
alt="Beam SQL in Notebooks: rename fields in a query.">&lt;/p>
&lt;p>The schema generated reflects the column renaming you have done in the SQL.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>You will go through an example to find out the US state with the most COVID
positive cases on a specific day with data provided by the
&lt;a href="https://covidtracking.com/">covid tracking project&lt;/a>.&lt;/p>
&lt;h3 id="get-the-data">Get the data&lt;/h3>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">json&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">requests&lt;/span>
&lt;span class="c1"># The covidtracking project has stopped collecting new data, current data ends on 2021-03-07&lt;/span>
&lt;span class="n">json_current&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;https://covidtracking.com/api/v1/states/current.json&amp;#39;&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">with&lt;/span> &lt;span class="n">requests&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">loads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">data&lt;/span>
&lt;span class="n">current_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">json_current&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">current_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image14.png"
alt="Beam SQL in Notebooks: preview example data.">&lt;/p>
&lt;p>The data is dated as 2021-03-07. It contains many details about COVID cases for
different states in the US. &lt;code>current_data[0]&lt;/code> is just one of the data points.&lt;/p>
&lt;p>You can get rid of most of the columns of the data. For example, just focus on
“date”, “state”, “positive” and “negative”, and then define a schema
&lt;code>UsCovidData&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="c1"># Remember to str(e[&amp;#39;date&amp;#39;]).&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">positive&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;span class="n">negative&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>date&lt;/code> is a keyword in (Calcite)SQL, use a different field name such as
&lt;code>partition_date&lt;/code>;&lt;/li>
&lt;li>&lt;code>date&lt;/code> from the data is an &lt;code>int&lt;/code> type, not &lt;code>str&lt;/code>. Make sure you convert the
data using &lt;code>str()&lt;/code> or use &lt;code>date: int&lt;/code>.&lt;/li>
&lt;li>&lt;code>negative&lt;/code> has missing values and the default is &lt;code>None&lt;/code>. So instead of
&lt;code>negative: int&lt;/code>, it should be &lt;code>negative: Optional[int]&lt;/code>. Or you can convert
&lt;code>None&lt;/code> into 0 when using the schema.&lt;/li>
&lt;/ul>
&lt;p>Then parse the json data into a PCollection with the schema:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">p_sql&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_sql&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Create PCollection from json&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">positive&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;positive&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">negative&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;negative&amp;#39;&lt;/span>&lt;span class="p">]))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">covid_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image15.png"
alt="Beam SQL in Notebooks: parse example data with a schema.">&lt;/p>
&lt;h3 id="query">Query&lt;/h3>
&lt;p>You can now find the biggest positive on the “current day” (2021-03-07).&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o max_positive
SELECT partition_date, MAX(positive) AS positive
FROM covid_data
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image16.png"
alt="Beam SQL in Notebooks: find the biggest positive from the data.">&lt;/p>
&lt;p>However, this is just the positive number. You cannot observe the state that has
this maximum number nor the negative case number for the state.&lt;/p>
&lt;p>To enrich your result, you have to join this data back to the original data set
you have parsed.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o entry_with_max_positive
SELECT covid_data.partition_date, covid_data.state, covid_data.positive, {fn IFNULL(covid_data.negative, 0)} AS negative
FROM covid_data JOIN max_positive
USING (partition_date, positive)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image17.png"
alt="Beam SQL in Notebooks: enriched data with biggest positive.">&lt;/p>
&lt;p>Now you can see all columns of the data with the maximum positive case on
2021-03-07.
&lt;strong>Note&lt;/strong>: to handle missing values of the negative column in the original data,
you can use &lt;code>{fn IFNULL(covid_data.negative, 0)}&lt;/code> to set null values to 0.&lt;/p>
&lt;p>When you&amp;rsquo;re ready to scale up, you can translate the SQLs into a pipeline with
&lt;code>SqlTransform&lt;/code>s and run your pipeline on a distributed runner like Flink or
Spark. This post demonstrates it by launching a one-shot job on Dataflow from
the notebook with the help of &lt;code>beam_sql&lt;/code> magic.&lt;/p>
&lt;h3 id="run-on-dataflow">Run on Dataflow&lt;/h3>
&lt;p>Now that you have a pipeline that parses US COVID data from json to find
positive/negative/state information for the state with the most positive cases
on each day, you can try applying it to all historical daily data and running it
on Dataflow.&lt;/p>
&lt;p>The new data source you will use is a public dataset from USAFacts US
Coronavirus Database that contains all historical daily summary of COVID cases
in the US.&lt;/p>
&lt;p>The schema of data is very similar to what the covid tracking project website
provides. The fields you will query are: &lt;code>date&lt;/code>, &lt;code>state&lt;/code>, &lt;code>confirmed_cases&lt;/code>, and
&lt;code>deaths&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image18.png"
alt="Beam SQL in Notebooks: schema of cloud data.">&lt;/p>
&lt;p>A preview of the data looks like below (you may skip the inspection in BigQuery
and just take a look at the screenshot):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image19.png"
alt="Beam SQL in Notebooks: preview of cloud data.">&lt;/p>
&lt;p>The format of the data is &lt;strong>slightly different&lt;/strong> from the json data you parsed
in the previous pipeline because the numbers are grouped by counties instead of
states, thus some additional aggregations need to be done in the SQLs.&lt;/p>
&lt;p>If you need a fresh execution, you may click the “Restart the kernel” button on
the top menu.&lt;/p>
&lt;p>Full code is as below, on-top of the original pipeline and queries:&lt;/p>
&lt;ul>
&lt;li>It changes the source from a single-day data to a more complete historical
data;&lt;/li>
&lt;li>It changes the I/O and schema to accommodate the new dataset;&lt;/li>
&lt;li>It changes the SQLs to include more aggregations to accommodate the new
format of the dataset.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prepare the data with schema&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;span class="c1"># Public BQ dataset.&lt;/span>
&lt;span class="n">table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;bigquery-public-data:covid19_usafacts.summary&amp;#39;&lt;/span>
&lt;span class="c1"># Replace with your project.&lt;/span>
&lt;span class="n">project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;YOUR-PROJECT-NAME-HERE&amp;#39;&lt;/span>
&lt;span class="c1"># Replace with your GCS bucket.&lt;/span>
&lt;span class="n">gcs_location&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;gs://YOUR_GCS_BUCKET_HERE&amp;#39;&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">deaths&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">p_on_dataflow&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_on_dataflow&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Read dataset&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">project&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">project&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gcs_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">deaths&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">])))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Run on Dataflow&lt;/strong>&lt;/p>
&lt;p>To run SQL on Dataflow is very simple, you just need to add the option
&lt;code>-r DataflowRunner&lt;/code>.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o data_by_state -r DataflowRunner
SELECT partition_date, state, SUM(confirmed_cases) as confirmed_cases, SUM(deaths) as deaths
FROM covid_data
GROUP BY partition_date, state
&lt;/code>&lt;/pre>&lt;p>Different from previous &lt;code>beam_sql&lt;/code> magic executions, you won’t see the result
immediately. Instead, a form like below is printed in the notebook cell output:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image20.png"
alt="Beam SQL in Notebooks: empty run-on-dataflow form.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic tries its best to guess your project id and preferred cloud
region. You still have to input additional information necessary to submit a
Dataflow job, such as a GCS bucket to stage the Dataflow job and any additional
Python dependencies the job needs.&lt;/p>
&lt;p>For now, ignore the form in the cell output, because you still need 2 more SQLs
to: 1) find the maximum confirmed cases on each day; 2) join the maximum case
data with the full data_by_state. The &lt;code>beam_sql&lt;/code> magic allows you to chain SQLs,
so chain 2 more by executing:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o max_cases -r DataflowRunner
SELECT partition_date, MAX(confirmed_cases) as confirmed_cases
FROM data_by_state
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>And&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o data_with_max_cases -r DataflowRunner
SELECT data_by_state.partition_date, data_by_state.state, data_by_state.confirmed_cases, data_by_state.deaths
FROM data_by_state JOIN max_cases
USING (partition_date, confirmed_cases)
&lt;/code>&lt;/pre>&lt;p>By default, when running &lt;code>beam_sql&lt;/code> on Dataflow, the output PCollection will be
written to a text file on GCS. The “write” is automatically provided by
&lt;code>beam_sql&lt;/code> and mainly for your inspection of the output data for this one-shot
Dataflow job. It’s lightweight and does not encode elements for further
development. To save the output and share it with others, you can add more Beam
I/Os into the mix.&lt;/p>
&lt;p>For example, you can appropriately encode elements into text files using the
technique described in the above schema-aware PCollections example.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">GoogleCloudOptions&lt;/span>
&lt;span class="n">coder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">max_data_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gcs_location&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s1">&amp;#39;/encoded_max_data&amp;#39;&lt;/span>
&lt;span class="n">data_with_max_cases&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_data_file&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Furthermore, you can create a new BQ dataset in your own project to store the
processed data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image21.png"
alt="Beam SQL in Notebooks: create a new BQ dataset.">&lt;/p>
&lt;p>You have to select the same data location as the public BigQuery data you are
reading. In this case, “us (multiple regions in United States)”.&lt;/p>
&lt;p>Once you finish creating an empty dataset, you can execute below:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">output_table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">f&lt;/span>&lt;span class="s1">&amp;#39;{project}:covid_data.max_analysis&amp;#39;&lt;/span>
&lt;span class="n">bq_schema&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s1">&amp;#39;fields&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">}]}&lt;/span>
&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;To json-like&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">partition_date&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">deaths&lt;/span>&lt;span class="p">})&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">output_table&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">schema&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bq_schema&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">method&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;STREAMING_INSERTS&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">custom_gcs_temp_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now back in the form of the last SQL cell output, you may fill in necessary
information to run the pipeline on Dataflow. An example input looks like below:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image22.png"
alt="Beam SQL in Notebooks: fill in the run-on-Dataflow form.">&lt;/p>
&lt;p>Because this pipeline doesn’t use any additional Python dependency, “Additional
Packages” is left empty. In the previous example where you have installed a
package called &lt;code>names&lt;/code>, to run that pipeline on Dataflow, you have to put
&lt;code>names&lt;/code> in this field.&lt;/p>
&lt;p>Once you finish updating your inputs, you can click the &lt;code>Show Options&lt;/code> button to
view what pipeline options have been configured based on your inputs. A variable
&lt;code>options_[YOUR_OUTPUT_PCOLL_NAME]&lt;/code> is generated, and you can supply more
pipeline options to it if the form is not enough for your execution.&lt;/p>
&lt;p>Once you are ready to submit the Dataflow job, click the &lt;code>Run on Dataflow&lt;/code>
button. It tells you where the default output would be written, and after a
while, a line with:&lt;/p>
&lt;blockquote>
&lt;p>Click here for the details of your Dataflow job.&lt;/p>
&lt;/blockquote>
&lt;p>would be displayed. You can click on the hyperlink to go to your Dataflow job
page. (Optionally, you can ignore the form and continue development to extend
your pipeline. Once you are satisfied with the state of your pipeline, you can
come back to the form and submit the job to Dataflow.)&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image23.png"
alt="Beam SQL in Notebooks: a Dataflow job graph.">&lt;/p>
&lt;p>As you can see, each transform name of the generated Dataflow job is prefixed
with a string &lt;code>[number]: &lt;/code>. This is to distinguish re-executed codes in
notebooks because Beam requires each transform to have a distinct name. Under
the hood, the &lt;code>beam_sql&lt;/code> magic also stages your schema information to Dataflow,
so you might see transforms named as &lt;code>schema_loaded_beam_sql_…&lt;/code>. This is because
the &lt;code>NamedTuple&lt;/code> defined in the notebook is likely in the &lt;code>__main__&lt;/code> scope and
Dataflow is not aware of them at all. To minimize user intervention and avoid
pickling the whole main session (and it’s infeasible to pickle the main session
when it contains unpickle-able attributes), the &lt;code>beam_sql&lt;/code> magic optimizes the
staging process by serializing your schemas, staging them to Dataflow, and then
deserialize/load them for job execution.&lt;/p>
&lt;p>Once the job succeeds, the result of the output PCollection would be written to
places instructed by your I/O transforms. &lt;strong>Note&lt;/strong>: running &lt;code>beam_sql&lt;/code> on
Dataflow generates a one-shot job and it’s not interactive.&lt;/p>
&lt;p>A simple inspection of the data from the default output location:&lt;/p>
&lt;pre>&lt;code>!gsutil cat 'gs://ningk-so-test/bq/staging/data_with_max_cases*'
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image24.png"
alt="Beam SQL in Notebooks: inspect the default output file.">&lt;/p>
&lt;p>The text file with encoded binary data written by your &lt;code>WriteToText&lt;/code>:&lt;/p>
&lt;pre>&lt;code>!gsutil cat 'gs://ningk-so-test/bq/encoded_max_data*'
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image25.png"
alt="Beam SQL in Notebooks: inspect the user-defined output file.">&lt;/p>
&lt;p>The table &lt;code>YOUR-PROJECT:covid_data.max_analysis&lt;/code> created by your
&lt;code>WriteToBigQuery&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image26.png"
alt="Beam SQL in Notebooks: inspect the output BQ dataset.">&lt;/p>
&lt;h3 id="run-on-other-oss-runners-directly-with-the-beam_sql-magic">Run on other OSS runners directly with the &lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>On the day this blog is posted, the &lt;code>beam_sql&lt;/code> magic only supports DirectRunner
(interactive) and DataflowRunner (one-shot). It&amp;rsquo;s a simple wrapper on top of
the &lt;code>SqlTransform&lt;/code> with interactive input widgets implemented by
&lt;a href="https://ipywidgets.readthedocs.io/en/stable/">ipywidgets&lt;/a>. You can implement
your own runner support or utilities by following the
&lt;a href="https://lists.apache.org/thread/psrx1xhbyjcqbhxx6trf5nvh66c6pk3y">instructions&lt;/a>.&lt;/p>
&lt;p>Additionally, support for other OSS runners are WIP, for example,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14373">support using FlinkRunner with the &lt;code>beam_sql&lt;/code> magic&lt;/a>.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>The &lt;code>beam_sql&lt;/code> magic and Apache Beam Notebooks combined is a convenient tool for
you to learn Beam SQL and mix Beam SQL into prototyping and productionizing (
e.g., to Dataflow) your Beam pipelines with minimum setups.&lt;/p>
&lt;p>For more details about the Beam SQL syntax, check out the Beam Calcite SQL
&lt;a href="https://beam.apache.org/documentation/dsls/sql/calcite/overview/">compatibility&lt;/a>
and the Apache Calcite SQL
&lt;a href="https://calcite.apache.org/docs/reference.html">syntax&lt;/a>.&lt;/p></description><link>/blog/beam-sql-with-notebooks/</link><pubDate>Thu, 28 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/beam-sql-with-notebooks/</guid><category>blog</category></item><item><title>Running Apache Hop visual pipelines with Google Cloud Dataflow</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>Apache Hop (&lt;a href="https://hop.apache.org/">https://hop.apache.org/&lt;/a>) is a visual development environment for creating data pipelines using Apache Beam. You can run your Hop pipelines in Spark, Flink or Google Cloud Dataflow.&lt;/p>
&lt;p>In this post, we will see how to install Hop, and we will run a sample pipeline in the cloud with Dataflow. To follow the steps given in this post, you should have a project in Google Cloud Platform, and you should have enough permissions to create a Google Cloud Storage bucket (or to use an existing one), as well as to run Dataflow jobs.&lt;/p>
&lt;p>Once you have your Google Cloud project ready, you will need to &lt;a href="https://cloud.google.com/sdk/docs/install">install the Google Cloud SDK&lt;/a> to trigger the Dataflow pipeline.&lt;/p>
&lt;p>Also, don&amp;rsquo;t forget to configure the Google Cloud SDK to use your account and your project.&lt;/p>
&lt;h2 id="setup-and-local-execution">Setup and local execution&lt;/h2>
&lt;p>You can run Apache Hop as a local application, or use &lt;a href="https://hop.incubator.apache.org/manual/latest/hop-gui/hop-web.html">the Hop web version&lt;/a> from a Docker container. The instructions given in this post will work for the local application, as the authentication for Cloud Dataflow would be different if Hop is running in a container. All the rest of the instructions remain valid. The UI of Hop is exactly the same either running as a local app or in the web version.&lt;/p>
&lt;p>Now it&amp;rsquo;s time to download and install Apache Hop, following these &lt;a href="https://hop.apache.org/manual/latest/getting-started/hop-download-install.html">instructions&lt;/a>.&lt;/p>
&lt;p>For this post, I have used the binaries in the apache-hop-client package, version 1.2.0, released on March 7th, 2022.&lt;/p>
&lt;p>After installing Hop, we are ready to start.&lt;/p>
&lt;p>The Zip file contains a directory &lt;code>config&lt;/code> where you will find some sample projects and some pipeline run configuration for Dataflow and other runners.&lt;/p>
&lt;p>For this example, we are going to use the pipeline located in &lt;code>config/projects/samples/beam/pipelines/input-process-output.hpl.&lt;/code>&lt;/p>
&lt;p>Let&amp;rsquo;s start by opening Apache Hop. In the directory where you have unzipped the client, run&lt;/p>
&lt;p>&lt;code>./hop/hop-gui.sh&lt;/code>&lt;/p>
&lt;p>(or &lt;code>./hop/hop-gui.bat&lt;/code> if you are on Windows).&lt;/p>
&lt;p>Once we are in Hop, let&amp;rsquo;s open the pipeline.&lt;/p>
&lt;p>We first switch from the project &lt;code>default&lt;/code> to the project &lt;code>samples&lt;/code>. Locate the &lt;code>projects&lt;/code> box in the top left corner of the window, and select the project &lt;code>samples&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image18.png"
alt="Apache Hop projects">&lt;/p>
&lt;p>Now we click the open button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image4.png"
alt="Apache Hop open project">&lt;/p>
&lt;p>Select the pipeline &lt;code>input-process-output.hpl&lt;/code> in the &lt;code>beam/pipelines&lt;/code> subdirectory:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image12.png"
alt="Apache Hop select pipeline">&lt;/p>
&lt;p>You should see a graph like the following in the main window of Hop:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image17.png"
alt="Apache Hop main window">&lt;/p>
&lt;p>This pipeline takes some customer data from a CSV file and filters out everything but the records with the column &lt;code>stateCode&lt;/code> equal to &lt;code>CA.&lt;/code>&lt;/p>
&lt;p>Then we select only some of the columns of the file, and the result is written to Google Cloud Storage.&lt;/p>
&lt;p>It is always a good idea to test the pipeline locally before submitting it to Dataflow. In Apache Hop, you can preview the output of each transform. Let&amp;rsquo;s have a look at the input &lt;code>Customers&lt;/code>.&lt;/p>
&lt;p>Click in the &lt;code>Customers&lt;/code> input transform and then in &lt;em>Preview Output&lt;/em> in the dialog box that opens after selecting the transform:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image10.png"
alt="Apache Hop Customers preview">&lt;/p>
&lt;p>Now select the option &lt;em>Quick launch&lt;/em> and you will see some of the input data:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image24.png"
alt="Apache Hop input data">&lt;/p>
&lt;p>Click &lt;em>Stop&lt;/em> when you finish reviewing the data.&lt;/p>
&lt;p>If we repeat the process right after the &lt;code>Only CA&lt;/code> transform, we will see that all the rows have the &lt;code>stateCode&lt;/code> column equal to &lt;code>CA&lt;/code>.&lt;/p>
&lt;p>The next transform selects only some of the columns of the input data and reorders the columns. Let&amp;rsquo;s have a look. Click the transform and then &lt;em>Preview Output&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image15.png"
alt="Apache Hop preview output">&lt;/p>
&lt;p>Then click _Quick Launch _again, and you should see output like the following:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image8.png"
alt="Apache Hop output">&lt;/p>
&lt;p>The column &lt;code>id&lt;/code> is now the first, and we see only a subset of the input columns. This is how the data will look once the pipeline finishes writing the full output.&lt;/p>
&lt;h2 id="using-the-beam-direct-runner">Using the Beam Direct Runner&lt;/h2>
&lt;p>Let&amp;rsquo;s run the pipeline. To run the pipeline, we need to specify a runner configuration. This is done through the Metadata tool of Apache Hop:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image6.png"
alt="Apache Hop runner configuration">&lt;/p>
&lt;p>In the &lt;code>samples&lt;/code> project, there are already several configurations created:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image9.png"
alt="Apache Hop configurations">&lt;/p>
&lt;p>The &lt;code>local&lt;/code> configuration is the one used to run the pipeline using Hop. For instance, this is the configuration that we used when we examined the previews of the output of different steps.&lt;/p>
&lt;p>The &lt;code>Direct&lt;/code> configuration uses the direct runner of Apache Beam. Let&amp;rsquo;s examine what it looks like. There are two tabs in the Pipeline Run Configurations: main and variables.&lt;/p>
&lt;p>For the direct runner, the main tab has the following options:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image28.png"
alt="Apache Hop direct runner">&lt;/p>
&lt;p>We can change the number of workers settings to match our number of CPUs, or even limit it just to 1 so the pipeline does not consume a lot of resources.&lt;/p>
&lt;p>In the variables tab, we find the configuration parameters for the pipeline itself (not for the runner): \&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image14.png"
alt="Apache Hop variables tab">&lt;/p>
&lt;p>For this pipeline, only the &lt;code>DATA_INPUT&lt;/code> and &lt;code>DATA_OUTPUT&lt;/code> variables are used. The &lt;code>STATE_INPUT&lt;/code> is used in a different example.&lt;/p>
&lt;p>If you go to the Beam transforms in the input and output nodes of the pipeline, you will see how these variables are used there:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image29.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image11.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>Since those variables are correctly set up to point to the location of data in the samples project folder, let&amp;rsquo;s try to run the pipeline using the Beam Direct Runner.&lt;/p>
&lt;p>For that, we need to go back to the pipeline view (arrow button just above the Metadata tool), and click the run button (the small &amp;ldquo;play&amp;rdquo; button in the toolbar). Then choose the Direct pipeline run configuration, and click the &lt;em>Launch&lt;/em> button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image20.png"
alt="Apache Hop launch">&lt;/p>
&lt;p>How do you know if the job has finished or not? You can check the logs at the bottom of the main window for that. You should see something like this:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image19.png"
alt="Apache Hop completed job">&lt;/p>
&lt;p>If we go to the location set by &lt;code>DATA_OUTPUT&lt;/code>, in our case &lt;code>config/projects/samples/beam/output&lt;/code>, we should see some output files there. In my case, I see these files:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image26.png"
alt="Apache Hop output files">&lt;/p>
&lt;p>The number of files depends on the number of workers that you have set in the run configuration.&lt;/p>
&lt;p>Great, so the pipeline works locally. It is time to run it in the cloud!&lt;/p>
&lt;h2 id="running-at-cloud-scale-with-dataflow">Running at cloud scale with Dataflow&lt;/h2>
&lt;p>Let&amp;rsquo;s have a look at the Dataflow Pipeline Run Configuration. Go to the metadata tool, then to Pipeline Run Configuration and select Dataflow:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image30.png"
alt="Apache Hop Pipeline Run Configuration">&lt;/p>
&lt;p>We have again the Main and the Variables tab. We will need to change some values in both. Let&amp;rsquo;s start with the Variables. Click the Variables tab, and you should see the following values:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image3.png"
alt="Apache Hop Variables tab">&lt;/p>
&lt;p>Those are Google Cloud Storage (GCS) locations that belong to the author of that sample project. We need to change them to point to our own GCS bucket.&lt;/p>
&lt;h2 id="project-setup-in-google-cloud">Project setup in Google Cloud&lt;/h2>
&lt;p>But for that, we will have to create a bucket. For the next step, you need to make sure that you have configured gcloud (the Google Cloud SDK), and that you have managed to authenticate.&lt;/p>
&lt;p>To double check, run the command &lt;code>gcloud config list&lt;/code> and check if the account and the project look correct. If they do, let&amp;rsquo;s triple check and run &lt;code>gcloud auth login&lt;/code>. That should open a tab in your web browser, to do the authentication process. Once you have done that, you can interact with your project using the SDK.&lt;/p>
&lt;p>For this example, I will use the region europe-west1 of GCP. Let&amp;rsquo;s create a regional bucket there. In my case, I am using the name &lt;code>ihr-apache-hop-blog&lt;/code> for the bucket name. Choose a different name for your bucket!&lt;/p>
&lt;pre>&lt;code>gsutil mb -c regional -l europe-west1 gs://ihr-apache-hop-blog
&lt;/code>&lt;/pre>&lt;p>Now let&amp;rsquo;s upload the sample data to the GCS bucket, to test how the pipeline would run in Dataflow. Go to the same directory where you have all the hop files (the same directory that &lt;code>hop-gui.sh&lt;/code> is in), and let&amp;rsquo;s copy the data to GCS:&lt;/p>
&lt;pre>&lt;code> gsutil cp config/projects/samples/beam/input/customers-noheader-1k.txt gs://ihr-apache-hop-blog/data/
&lt;/code>&lt;/pre>&lt;p>Notice the final slash &lt;code>/&lt;/code> in the path, indicating that you want to create a directory of name &lt;code>data&lt;/code>, with all the contents.&lt;/p>
&lt;p>To make sure that you have uploaded the data correctly, check the contents of that location:&lt;/p>
&lt;pre>&lt;code>gsutil ls gs://ihr-apache-hop-blog/data/
&lt;/code>&lt;/pre>&lt;p>You should see the file &lt;code>customer-noheader-1k.txt&lt;/code> in that location.&lt;/p>
&lt;p>Before we continue, make sure that Dataflow is enabled in your project, and that you have a service account ready to be used with Hop. Please check the instructions given at the documentation of Dataflow, in the &lt;em>&lt;a href="https://cloud.google.com/dataflow/docs/quickstarts/create-pipeline-java#before-you-begin">Before you begin section&lt;/a>&lt;/em> to see how to enable the API for Dataflow.&lt;/p>
&lt;p>Now we need to make sure that Hop can use the necessary credentials for accessing Dataflow. In the Hop documentation, you will find that it recommends creating a service account, exporting a key for that service account, and setting the GOOGLE_APPLICATION_CREDENTIALS environment variable. This is also the method given in the above link.&lt;/p>
&lt;p>Exporting the key of a service account is potentially dangerous, so we are going to use a different method, by leveraging the Google Cloud SDK. Run the following command:&lt;/p>
&lt;pre>&lt;code>gcloud auth application-default login
&lt;/code>&lt;/pre>&lt;p>That will open a tab in your web browser asking to confirm the authentication. Once you have confirmed, any application in your system that needs to access Google Cloud Platform will use those credentials for that access.&lt;/p>
&lt;p>We need also to create a service account for the Dataflow job, with certain permissions. Create the service account with&lt;/p>
&lt;pre>&lt;code>​​gcloud iam service-accounts create dataflow-hop-sa
&lt;/code>&lt;/pre>&lt;p>And now we give permissions to this service account for Dataflow:&lt;/p>
&lt;pre>&lt;code>gcloud projects add-iam-policy-binding ihr-hop-playground \
--member=&amp;quot;serviceAccount:dataflow-hop-sa@ihr-hop-playground.iam.gserviceaccount.com&amp;quot;\
--role=&amp;quot;roles/dataflow.worker&amp;quot;
&lt;/code>&lt;/pre>&lt;p>We also need to give additional permissions for Google Cloud Storage:&lt;/p>
&lt;pre>&lt;code>gcloud projects add-iam-policy-binding ihr-hop-playground \
--member=&amp;quot;serviceAccount:dataflow-hop-sa@ihr-hop-playground.iam.gserviceaccount.com&amp;quot;\
--role=&amp;quot;roles/storage.admin&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Make sure that you change the project id &lt;code>ihr-hop-playground&lt;/code> to your own project id.&lt;/p>
&lt;p>Now let&amp;rsquo;s give permissions to our user to impersonate that service account. For that, go to &lt;a href="https://console.cloud.google.com/iam-admin/serviceaccounts">Service Accounts in the Google Cloud Console&lt;/a> in your project, and click on the service account we have just created.&lt;/p>
&lt;p>Click on the &lt;em>Permissions&lt;/em> tab and then in the &lt;em>Grant Access&lt;/em> button:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image21.png"
alt="Apache Hop Permissions">&lt;/p>
&lt;p>Give your user the role &lt;em>Service Account User&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image13.png"
alt="Apache Hop Service Account User">&lt;/p>
&lt;p>You are now all set to be able to run Dataflow with that service account and your user.&lt;/p>
&lt;h2 id="updating-the-pipeline-run-configuration">Updating the Pipeline Run Configuration&lt;/h2>
&lt;p>Before we can run a pipeline in Dataflow, we need to generate the JAR package for the pipeline code. For that, you have to go to the &lt;em>Tools&lt;/em> menu (in the menu bar), and choose the option &lt;em>Generate a Hop fat jar&lt;/em>. Click ok in the dialog, and then select a location and filename for the jar, and click &lt;em>Save&lt;/em>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image5.png"
alt="Apache Hop Tools menu">&lt;/p>
&lt;p>It will take some minutes to generate the file:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image22.png"
alt="Apache Hop generate file">&lt;/p>
&lt;p>We are ready to run the pipeline in Dataflow. Or almost :).&lt;/p>
&lt;p>Go the pipeline editor, click the play button, and select &lt;em>DataFlow&lt;/em> as Pipeline run configuration, and then click the play button on the right side:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image7.png"
alt="Apache Hop pipeline editor">&lt;/p>
&lt;p>That will open the Dataflow Pipeline Run Configuration, where we can change the input variables, and other Dataflow settings.&lt;/p>
&lt;p>Click on the &lt;em>Variables&lt;/em> tab and modify only the &lt;code>DATA_INPUT&lt;/code> and &lt;code>DATA_OUTPUT&lt;/code> variables.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image2.png"
alt="Apache Hop Variables tab">&lt;/p>
&lt;p>Notice that we also need to change the filename.&lt;/p>
&lt;p>Let&amp;rsquo;s go now to the &lt;em>Main&lt;/em> tab, because there are some other options that we need to change there. We need to update:&lt;/p>
&lt;ul>
&lt;li>Project id&lt;/li>
&lt;li>Service account&lt;/li>
&lt;li>Staging location&lt;/li>
&lt;li>Region&lt;/li>
&lt;li>Temp location&lt;/li>
&lt;li>Fat jar file location&lt;/li>
&lt;/ul>
&lt;p>For project id, set your project id (the same one you see when you run &lt;code>gcloud config list&lt;/code>).&lt;/p>
&lt;p>For service account, use the address of the Service Account we have created. If you don&amp;rsquo;t remember, you can find it under S&lt;a href="https://console.cloud.google.com/iam-admin/serviceaccounts">ervice Accounts in the Google Cloud Console&lt;/a>.&lt;/p>
&lt;p>For staging and temp locations, use the same bucket that we have just created. Change the bucket address in the paths, and leave the same &amp;ldquo;binaries&amp;rdquo; and &amp;ldquo;tmp&amp;rdquo; locations that are already set in the configuration.&lt;/p>
&lt;p>For region, in this example we are using &lt;code>europe-west1&lt;/code>.&lt;/p>
&lt;p>Also, depending on your network configuration, you may want to check the box of &amp;ldquo;Use Public IPs?&amp;quot;, or alternatively leave it unchecked but enable Google Private Access in the regional subnetwork for europe-west1 in your project (for more details, please see &lt;a href="https://cloud.google.com/vpc/docs/configure-private-google-access#enabling-pga">Configuring Private Google Access | VPC&lt;/a>). In this example, I will check the box for simplicity.&lt;/p>
&lt;p>For the fat jar location, use the _Browse _button on the right side, and locate the JAR that we generated above. In summary, my &lt;em>Main&lt;/em> options look like these (your project id and locations will be different):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image27.png"
alt="Apache Hop variables">&lt;/p>
&lt;p>You may, of course, change any other option, depending on the specific settings that might be required for your project.&lt;/p>
&lt;p>When you are ready, click on the _Ok _button and then &lt;em>Launch&lt;/em> to trigger the pipeline.&lt;/p>
&lt;p>In the logging window, you should see a line like the following:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image16.png"
alt="Apache Hop logging window">&lt;/p>
&lt;h2 id="checking-the-job-in-dataflow">Checking the job in Dataflow&lt;/h2>
&lt;p>If everything has gone well, you should now see a job running at &lt;a href="https://console.cloud.google.com/dataflow/jobs">https://console.cloud.google.com/dataflow/jobs&lt;/a>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image1.png"
alt="Dataflow job list">&lt;/p>
&lt;p>If for some reason the job has failed, open the failed job page, check the _Logs _at the bottom, and click the error icon to find why the pipeline has failed. It is normally because we have set some wrong option in your configuration:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image25.png"
alt="Dataflow Logs">&lt;/p>
&lt;p>When the pipeline starts running, you should see the graph of the pipeline in the job page:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/apache-hop/image23.png"
alt="Dataflow pipeline graph">&lt;/p>
&lt;p>When the job finishes, there should be a file in the output location. You can check it out with &lt;code>gsutil&lt;/code>&lt;/p>
&lt;pre>&lt;code>% gsutil ls gs://ihr-apache-hop-blog/output
gs://ihr-apache-hop-blog/output/input-process-output-00000-of-00003.csv
gs://ihr-apache-hop-blog/output/input-process-output-00001-of-00003.csv
gs://ihr-apache-hop-blog/output/input-process-output-00002-of-00003.csv
&lt;/code>&lt;/pre>&lt;p>In my case, the job has generated three files, but the actual number will vary from run to run.&lt;/p>
&lt;p>Let&amp;rsquo;s explore the first lines of those files:&lt;/p>
&lt;pre>&lt;code>gsutil cat &amp;quot;gs://ihr-apache-hop-blog/output/*csv&amp;quot;| head
12,wha-firstname,vnaov-name,egm-city,CALIFORNIA
25,ayl-firstname,bwkoe-name,rtw-city,CALIFORNIA
26,zio-firstname,rezku-name,nvt-city,CALIFORNIA
44,rgh-firstname,wzkjq-name,hkm-city,CALIFORNIA
135,ttv-firstname,eqley-name,trs-city,CALIFORNIA
177,ahc-firstname,nltvw-name,uxf-city,CALIFORNIA
181,kxv-firstname,bxerk-name,sek-city,CALIFORNIA
272,wpy-firstname,qxjcn-name,rew-city,CALIFORNIA
304,skq-firstname,cqapx-name,akw-city,CALIFORNIA
308,sfu-firstname,ibfdt-name,kqf-city,CALIFORNIA
&lt;/code>&lt;/pre>&lt;p>We can see that all the rows have CALIFORNIA as the state, that the output contains only the columns that we selected, and that the user id is the first column. The actual output you get will probably be different, as the order in which data is processed will not be the same in each run.&lt;/p>
&lt;p>We have run this job with a small data sample, but we could have run the same job with an arbitrarily large input CSV. Dataflow would parallelize and process the data in chunks.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>Apache Hop is a visual development environment for Beam pipelines, that allows us to run the pipelines locally, inspect the data, debug, unit test and many other capabilities. Once we are happy with a pipeline that has run locally, we can deploy the same visual pipeline in the cloud by just setting the necessary parameters for using Dataflow.&lt;/p>
&lt;p>If you want to know more about Apache Hop, don&amp;rsquo;t miss &lt;a href="https://www.youtube.com/watch?v=sZSIbcPtebI">the Beam Summit talk delivered by the author of Hop&lt;/a>, and don&amp;rsquo;t forget to check out the &lt;a href="https://hop.apache.org/manual/latest/getting-started/index.html">getting started guide&lt;/a>.&lt;/p></description><link>/blog/apache-hop-with-dataflow/</link><pubDate>Fri, 22 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/apache-hop-with-dataflow/</guid><category>blog</category></item><item><title>Apache Beam 2.38.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.38.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2380-2022-04-20">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.38.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12351169">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Introduce projection pushdown optimizer to the Java SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12976">BEAM-12976&lt;/a>). The optimizer currently only works on the &lt;a href="https://beam.apache.org/documentation/io/built-in/google-bigquery/#storage-api">BigQuery Storage API&lt;/a>, but more I/Os will be added in future releases. If you encounter a bug with the optimizer, please file a JIRA and disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;li>A new IO for Neo4j graph databases was added. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-1857">BEAM-1857&lt;/a>) It has the ability to update nodes and relationships using UNWIND statements and to read data using cypher statements with parameters.&lt;/li>
&lt;li>&lt;code>amazon-web-services2&lt;/code> has reached feature parity and is finally recommended over the earlier &lt;code>amazon-web-services&lt;/code> and &lt;code>kinesis&lt;/code> modules (Java). These will be deprecated in one of the next releases (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13174">BEAM-13174&lt;/a>).
&lt;ul>
&lt;li>Long outstanding write support for &lt;code>Kinesis&lt;/code> was added (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13175">BEAM-13175&lt;/a>).&lt;/li>
&lt;li>Configuration was simplified and made consistent across all IOs, including the usage of &lt;code>AwsOptions&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13663">BEAM-13663&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13587">BEAM-13587&lt;/a>).&lt;/li>
&lt;li>Additionally, there&amp;rsquo;s a long list of recent improvements and fixes to
&lt;code>S3&lt;/code> Filesystem (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13245">BEAM-13245&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13246">BEAM-13246&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13441">BEAM-13441&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13445">BEAM-13445&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-14011">BEAM-14011&lt;/a>),
&lt;code>DynamoDB&lt;/code> IO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13009">BEAM-13209&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13209">BEAM-13209&lt;/a>),
&lt;code>SQS&lt;/code> IO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13631">BEAM-13631&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13510">BEAM-13510&lt;/a>) and others.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Pipeline dependencies supplied through &lt;code>--requirements_file&lt;/code> will now be staged to the runner using binary distributions (wheels) of the PyPI packages for linux_x86_64 platform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-4032">BEAM-4032&lt;/a>). To restore the behavior to use source distributions, set pipeline option &lt;code>--requirements_cache_only_sources&lt;/code>. To skip staging the packages at submission time, set pipeline option &lt;code>--requirements_cache=skip&lt;/code> (Python).&lt;/li>
&lt;li>The Flink runner now supports Flink 1.14.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13106">BEAM-13106&lt;/a>).&lt;/li>
&lt;li>Interactive Beam now supports remotely executing Flink pipelines on Dataproc (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14071">BEAM-14071&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>(Python) Previously &lt;code>DoFn.infer_output_types&lt;/code> was expected to return &lt;code>Iterable[element_type]&lt;/code> where &lt;code>element_type&lt;/code> is the PCollection elemnt type. It is now expected to return &lt;code>element_type&lt;/code>. Take care if you have overriden &lt;code>infer_output_type&lt;/code> in a &lt;code>DoFn&lt;/code> (this is not common). See &lt;a href="https://issues.apache.org/jira/browse/BEAM-13860">BEAM-13860&lt;/a>.&lt;/li>
&lt;li>(&lt;code>amazon-web-services2&lt;/code>) The types of &lt;code>awsRegion&lt;/code> / &lt;code>endpoint&lt;/code> in &lt;code>AwsOptions&lt;/code> changed from String to &lt;code>Region&lt;/code> / &lt;code>URI&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Beam 2.38.0 will be the last minor release to support Flink 1.11.&lt;/li>
&lt;li>(&lt;code>amazon-web-services2&lt;/code>) Client providers (&lt;code>withXYZClientProvider()&lt;/code>) as well as IO specific &lt;code>RetryConfiguration&lt;/code>s are deprecated, instead use &lt;code>withClientConfiguration()&lt;/code> or &lt;code>AwsOptions&lt;/code> to configure AWS IOs / clients.
Custom implementations of client providers shall be replaced with a respective &lt;code>ClientBuilderFactory&lt;/code> and configured through &lt;code>AwsOptions&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13563">BEAM-13563&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fix S3 copy for large objects (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14011">BEAM-14011&lt;/a>)&lt;/li>
&lt;li>Fix quadratic behavior of pipeline canonicalization (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14128">BEAM-14128&lt;/a>)
&lt;ul>
&lt;li>This caused unnecessarily long pre-processing times before job submission for large complex pipelines.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Fix &lt;code>pyarrow&lt;/code> version parsing (Python)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-14235">BEAM-14235&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.38.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.38.0 release. Thank you to all contributors!&lt;/p>
&lt;p>abhijeet-lele
Ahmet Altay
akustov
Alexander
Alexander Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
andoni-guzman
andreukus
Andy Ye
Ankur Goenka
ansh0l
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
Benjamin Gonzalez
Brian Hulette
brucearctor
bulat safiullin
bullet03
Carl Mastrangelo
Chamikara Jayalath
Chun Yang
Daniela Martín
Daniel Oliveira
Danny McCormick
daria.malkova
David Cavazos
David Huntsperger
dmitryor
Dmytro Sadovnychyi
dpcollins-google
egalpin
Elias Segundo Antonio
emily
Etienne Chauchot
Hengfeng Li
Ismaël Mejía
Israel Herraiz
Jack McCluskey
Jakub Kukul
Janek Bevendorff
Jeff Klukas
Johan Sternby
Kamil Breguła
Kenneth Knowles
Ke Wu
Kiley
Kyle Weaver
laraschmidt
Lara Schmidt
LE QUELLEC Olivier
Luka Kalinovcic
Luke Cwik
Marcin Kuthan
masahitojp
Masato Nakamura
Matt Casters
Melissa Pashniak
Michael Li
Miguel Hernandez
Moritz Mack
mosche
nancyxu123
Nathan J Mehl
Niel Markwick
Ning Kang
Pablo Estrada
paul-tlh
Pavel Avilov
Rahul Iyer
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Skraba
Ryan Thompson
Sam Whittle
Seth Vargo
sp029619
Steven Niemitz
Thiago Nunes
Udi Meiri
Valentyn Tymofieiev
Victor
vitaly.terentyev
Yichi Zhang
Yi Hu
yirutang
Zachary Houfek
Zoe&lt;/p></description><link>/blog/beam-2.38.0/</link><pubDate>Wed, 20 Apr 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.38.0/</guid><category>blog</category></item><item><title>Apache Beam 2.37.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.37.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2370-2022-03-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.37.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12351168">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Java 17 support for Dataflow (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12240">BEAM-12240&lt;/a>).
&lt;ul>
&lt;li>Users using Dataflow Runner V2 may see issues with state cache due to inaccurate object sizes (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13695">BEAM-13695&lt;/a>).&lt;/li>
&lt;li>ZetaSql is currently unsupported (&lt;a href="https://github.com/google/zetasql/issues/89">issue&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Python 3.9 support in Apache Beam (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12000">BEAM-12000&lt;/a>).
&lt;ul>
&lt;li>Dataflow support for Python 3.9 is expected to be available with 2.37.0,
but may not be fully available yet when the release is announced (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13864">BEAM-13864&lt;/a>).&lt;/li>
&lt;li>Users of Dataflow Runner V2 can run Python 3.9 pipelines with 2.37.0 release right away.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Go SDK now has wrappers for the following Cross Language Transforms from Java, along with automatic expansion service startup for each.
&lt;ul>
&lt;li>JDBCIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13293">BEAM-13293&lt;/a>).&lt;/li>
&lt;li>Debezium (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13761">BEAM-13761&lt;/a>).&lt;/li>
&lt;li>BeamSQL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13683">BEAM-13683&lt;/a>).&lt;/li>
&lt;li>BiqQuery (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13732">BEAM-13732&lt;/a>).&lt;/li>
&lt;li>KafkaIO now also has automatic expansion service startup. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13821">BEAM-13821&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>DataFrame API now supports pandas 1.4.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13605">BEAM-13605&lt;/a>).&lt;/li>
&lt;li>Go SDK DoFns can now observe trigger panes directly (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13757">BEAM-13757&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.37.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.37.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aizhamal Nurmamat kyzy
Alexander
Alexander Chermenin
Alexandr Zhuravlev
Alexey Romanenko
Anand Inguva
andoni-guzman
andreukus
Andy Ye
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
AydarZaynutdinov
Benjamin Gonzalez
Brian Hulette
Chamikara Jayalath
Daniel Oliveira
Danny McCormick
daria-malkova
daria.malkova
darshan-sj
David Huntsperger
dprieto91
emily
Etienne Chauchot
Fernando Morales
Heejong Lee
Ismaël Mejía
Jack McCluskey
Jan Lukavský
johnjcasey
Kamil Breguła
kellen
Kenneth Knowles
kileys
Kyle Weaver
Luke Cwik
Marcin Kuthan
Marco Robles
Matt Rudary
Miguel Hernandez
Milena Bukal
Moritz Mack
Mostafa Aghajani
Ning Kang
Pablo Estrada
Pavel Avilov
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Sam Whittle
Sandy Chapman
Sergey Kalinin
Thiago Nunes
thorbjorn444
Tim Robertson
Tomo Suzuki
Valentyn Tymofieiev
Victor
Victor Chen
Vitaly Ivanov
Yichi Zhang&lt;/p></description><link>/blog/beam-2.37.0/</link><pubDate>Fri, 04 Mar 2022 08:30:00 -0800</pubDate><guid>/blog/beam-2.37.0/</guid><category>blog</category></item><item><title>Upcoming Events for Beam in 2022</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are so excited to announce the upcoming Beam events for this year! We believe that events are an important mechanism to foster the community around Apache Beam as an Open Source Project. Our events are focused on a developer experience by giving spaces for the community to connect, facilitate collaboration, and enable knowledge sharing.&lt;/p>
&lt;p>Here is an overview of some upcoming events and ways for everyone to help foster additional community growth:&lt;/p>
&lt;h2 id="beam-summit">Beam Summit&lt;/h2>
&lt;p>The &lt;strong>&lt;a href="https://2022.beamsummit.org/">Beam Summit 2022&lt;/a>&lt;/strong> is approaching! The event will be in a hybrid in-person and virtual format from Austin, TX on July 18-20, 2022. The conference will include three full days of lightning talks, roadmap updates, use cases, demos, and workshops for Beam users of all levels. This is a great opportunity to collaborate, share ideas, and work together in the improvement of the project.&lt;/p>
&lt;p>Check out talks from prior editions of Beam Summit &lt;strong>&lt;a href="https://www.youtube.com/watch?v=jses0W4Zalc&amp;amp;list=PL4dEBWmGSIU8vLWF56shrSuTsLXvO6Ex3">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="the-experience">The Experience&lt;/h3>
&lt;p>We are so excited to see some of you in person again and the rest of the community online! The &lt;strong>&lt;a href="https://2022.beamsummit.org/team/">Beam Summit Steering Committee&lt;/a>&lt;/strong> in partnership with an event production company is working hard to ensure that we provide the community with the best possible experience, no matter which format you choose to attend in.&lt;/p>
&lt;p>If you have any ideas on how we can make this year’s event better, please &lt;strong>&lt;a href="mailto:contact@beamsummit.org">reach out to us&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="ways-to-help--participate">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>&lt;a href="https://sessionize.com/beam-summit-2022">Submit a proposal&lt;/a>&lt;/strong> to talk! The deadline for submissions is &lt;em>March 15th&lt;/em>.&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://2022.beamsummit.org/tickets/">Register&lt;/a>&lt;/strong> to join as an attendee in person or online.&lt;/li>
&lt;li>Consider sponsoring the event. If your company is interested in engaging with members of the community, please check out the &lt;strong>&lt;a href="https://2022.beamsummit.org/sponsors/">sponsoring prospectus&lt;/a>.&lt;/strong>&lt;/li>
&lt;li>Help us get the word out. Please make sure to let your colleagues and friends know about the Beam Summit.&lt;/li>
&lt;/ol>
&lt;p>Don’t forget to follow our Beam Summit &lt;strong>&lt;a href="https://twitter.com/BeamSummit?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Twitter&lt;/a>&lt;/strong> and &lt;strong>&lt;a href="https://www.linkedin.com/company/beam-summit/?viewAsMember=true">LinkedIn&lt;/a>&lt;/strong> pages to receive event updates!&lt;/p>
&lt;h2 id="beam-college">Beam College&lt;/h2>
&lt;p>&lt;strong>&lt;a href="https://beamcollege.dev/">Beam College 2022&lt;/a>&lt;/strong> is around the corner for the second season of training! The event will be hosted virtually from May 10-13, 2022. The training is focused on providing more hands-on experience around end-to-end code samples in an interactive environment, and helping attendees see the applications of concepts covered in other venues, such as the Beam Summit.&lt;/p>
&lt;p>Check out talks from prior editions of Beam College &lt;strong>&lt;a href="https://www.youtube.com/playlist?list=PLjYq1UNvv2UcrfapfgKrnLXtYpkvHmpIh">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;p>This year, the training will consist of learning modules such as:&lt;/p>
&lt;ul>
&lt;li>The Data movement ecosystem and distributed processing the Beam way&lt;/li>
&lt;li>Scaling, productionalizing, and developing your Beam pipelines&lt;/li>
&lt;li>Use Cases&lt;/li>
&lt;li>Beam ML Use Cases&lt;/li>
&lt;/ul>
&lt;p>Be sure to check out our &lt;strong>&lt;a href="https://beamcollege.dev/">website&lt;/a>&lt;/strong> as we continue updating the schedule and follow our &lt;strong>&lt;a href="https://twitter.com/beam_college?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Twitter&lt;/a>&lt;/strong> and &lt;strong>&lt;a href="https://www.linkedin.com/showcase/beam-college/">LinkedIn&lt;/a>&lt;/strong> pages to receive event updates!&lt;/p>
&lt;h3 id="ways-to-help--participate-1">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>Interested in instructing? Submit a &lt;strong>&lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSct6RCrKtgsvxlgngKUGwKoB_iOKihXi1OadKyBQIsi00p3cQ/viewform?usp=sf_link">proposal&lt;/a>&lt;/strong>! The deadline is: &lt;em>February 28th.&lt;/em>&lt;/li>
&lt;li>Enroll in Beam College. Registration is now open on the &lt;strong>&lt;a href="https://beamcollege.dev/step/2022/">registration page&lt;/a>&lt;/strong>.&lt;/li>
&lt;li>Consider partnering with the event. If your company is interested in helping to promote the event and being a part of the branding, please fill out this &lt;strong>form&lt;/strong>.&lt;/li>
&lt;li>Help us get the word out by letting your network know about this exciting opportunity to help users uplevel data processing skills, solve complex data applications, and optimize data pipelines!&lt;/li>
&lt;/ol>
&lt;h2 id="beam-meetups">Beam Meetups&lt;/h2>
&lt;p>In partnership with an event production company, Beam will be hosting an average of one virtual Meetup per month. These Meetups will be relaxed presentations on topics or demos followed by a Q&amp;amp;A session. The objective of our virtual meetups is to give the community an update on the most recent Beam features launched within the past six months. These meetups are free and open to the public.&lt;/p>
&lt;p>Check out recordings from previous Meetups &lt;strong>&lt;a href="https://www.youtube.com/watch?v=8fNEs7SbefM&amp;amp;list=PL4dEBWmGSIU-cQSpYP7R1lSC6e2K_pTf1">here&lt;/a>&lt;/strong>!&lt;/p>
&lt;h3 id="ways-to-help--participate-2">Ways to Help &amp;amp; Participate&lt;/h3>
&lt;ol>
&lt;li>Are you interested in sharing a feature launch or sharing a step-by-step use case for Beam? Submit a &lt;strong>&lt;a href="https://docs.google.com/forms/d/e/1FAIpQLScFg7fmOFc7fTvnJL_dmdhia4HDesW4HYxJsDeulnsHzIzqCg/viewform">talk idea&lt;/a>&lt;/strong>!&lt;/li>
&lt;li>Register for the events. Registration is now open on the &lt;strong>&lt;a href="https://clowder.space/projects/apache-beam/">registration page&lt;/a>&lt;/strong>.&lt;/li>
&lt;li>Help us get the word out by spreading the word throughout the community to enable more knowledge sharing and collaboration!&lt;/li>
&lt;/ol></description><link>/blog/upcoming-events-for-beam-in-2022/</link><pubDate>Mon, 28 Feb 2022 00:00:01 -0800</pubDate><guid>/blog/upcoming-events-for-beam-in-2022/</guid><category>blog</category></item><item><title>Apache Beam 2.36.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.36.0 release of Apache Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2360-2022-02-07">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.36.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350407">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for stopReadTime on KafkaIO SDF (Java).(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13171">BEAM-13171&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>💻 Support for ARM64 / Mac M1 out of the box. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11703">BEAM-11703&lt;/a>).&lt;/li>
&lt;li>Added support for cloudpickle as a pickling library for Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8123">BEAM-8123&lt;/a>). To use cloudpickle, set pipeline option: &amp;ndash;pickle_library=cloudpickle&lt;/li>
&lt;li>Added option to specify triggering frequency when streaming to BigQuery (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12865">BEAM-12865&lt;/a>).&lt;/li>
&lt;li>Added option to enable caching uploaded artifacts across job runs for Python Dataflow jobs (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13459">BEAM-13459&lt;/a>). To enable, set pipeline option: &amp;ndash;enable_artifact_caching, this will be enabled by default in a future release.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Updated the jedis from 3.x to 4.x to Java RedisIO. If you are using RedisIO and using jedis directly, please refer to &lt;a href="https://github.com/redis/jedis/blob/v4.0.0/docs/3to4.md">this page&lt;/a> to update it. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12092">BEAM-12092&lt;/a>).&lt;/li>
&lt;li>Datatype of timestamp fields in &lt;code>SqsMessage&lt;/code> for AWS IOs for SDK v2 was changed from &lt;code>String&lt;/code> to &lt;code>long&lt;/code>, visibility of all fields was fixed from &lt;code>package private&lt;/code> to &lt;code>public&lt;/code> &lt;a href="https://issues.apache.org/jira/browse/BEAM-13638">BEAM-13638&lt;/a>.&lt;/li>
&lt;li>Properly check output timestamps on elements output from DoFns, timers, and onWindowExpiration in Java &lt;a href="https://issues.apache.org/jira/browse/BEAM-12931">BEAM-12931&lt;/a>.&lt;/li>
&lt;li>Fixed a bug with DeferredDataFrame.xs when used with a non-tuple key
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13421%5D">BEAM-13421&lt;/a>).&lt;/li>
&lt;li>Beam Python now requires &lt;code>google-cloud-pubsub&amp;gt;=2.1.0&lt;/code>. The API surface for &lt;code>apache_beam.io.gcp.pubsub&lt;/code> has not changed, but code that uses the PubSub client directly may need to be updated.&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Users may encounter an unexpected java.lang.ArithmeticException when outputting a timestamp
for an element further than allowedSkew from an allowed DoFN skew set to a value more than
Integer.MAX_VALUE.&lt;/li>
&lt;li>S3 object metadata retrieval broken in Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13980">BEAM-13980&lt;/a>)&lt;/li>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.36.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.36.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ada Wong
Ahmet Altay
Alexander
Alexander Dahl
Alexandr Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
Andrew Pilloud
Andy Ye
Arkadiusz Gasiński
Artur Khanin
Arun Pandian
Aydar Farrakhov
Aydar Zainutdinov
AydarZaynutdinov
Benjamin Gonzalez
Brian Hulette
Chamikara Jayalath
Daniel Collins
Daniel Oliveira
Daniel Thevessen
Daniela Martín
David Hinkes
David Huntsperger
Emily Ye
Etienne Chauchot
Evan Galpin
Heejong Lee
Ilya
Ilya Kozyrev
In-Ho Yi
Jack McCluskey
Janek Bevendorff
Jarek Potiuk
Ke Wu
KevinGG
Kyle Hersey
Kyle Weaver
Luís Bianchin
Luke Cwik
Masato Nakamura
Matthias Baetens
Mehdi Drissi
Melissa Pashniak
Michel Davit
Miguel Hernandez
MiguelAnzoWizeline
Milena Bukal
Moritz Mack
Mostafa Aghajani
Nathan J Mehl
Niel Markwick
Ning Kang
Pablo Estrada
Pavel Avilov
Quentin Sommer
Reuben van Ammers
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Sayat
Sergei Lebedev
Sergey Kalinin
Steve Niemitz
Talat Uyarer
Thiago Nunes
Tianyang Hu
Tim Robertson
Valentyn Tymofieiev
Vitaly Ivanov
Yichi Zhang
Yiru Tang
Yu Feng
Yu ISHIKAWA
Zachary Houfek
blais
daria-malkova
daria.malkova
darshan-sj
dpcollins-google
emily
ewianda
johnjcasey
kileys
lam206
laraschmidt
mosche
&lt;a href="mailto:msbukal@google.com">msbukal@google.com&lt;/a>
tvalentyn&lt;/p></description><link>/blog/beam-2.36.0/</link><pubDate>Mon, 07 Feb 2022 10:11:00 -0800</pubDate><guid>/blog/beam-2.36.0/</guid><category>blog</category></item><item><title>Apache Beam 2.35.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.35.0 release of Apache Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2350-2021-12-29">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.35.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350406">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>MultiMap side inputs are now supported by the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3293">BEAM-3293&lt;/a>).&lt;/li>
&lt;li>Side inputs are supported within Splittable DoFns for Dataflow Runner V1 and Dataflow Runner V2. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12522">BEAM-12522&lt;/a>).&lt;/li>
&lt;li>Upgrades Log4j version used in test suites (Apache Beam testing environment only, not for end user consumption) to 2.17.0(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13434">BEAM-13434&lt;/a>).
Note that Apache Beam versions do not depend on the Log4j 2 dependency (log4j-core) impacted by &lt;a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228">CVE-2021-44228&lt;/a>.
However we urge users to update direct and indirect dependencies (if any) on Log4j 2 to the latest version by updating their build configuration and redeploying impacted pipelines.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>We changed the data type for ranges in &lt;code>JdbcIO.readWithPartitions&lt;/code> from &lt;code>int&lt;/code> to &lt;code>long&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13149">BEAM-13149&lt;/a>).
This is a relatively minor breaking change, which we&amp;rsquo;re implementing to improve the usability of the transform without increasing cruft.
This transform is relatively new, so we may implement other breaking changes in the future to improve its usability.&lt;/li>
&lt;li>Side inputs are supported within Splittable DoFns for Dataflow Runner V1 and Dataflow Runner V2. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12522">BEAM-12522&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added custom delimiters to Python TextIO reads (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12730">BEAM-12730&lt;/a>).&lt;/li>
&lt;li>Added escapechar parameter to Python TextIO reads (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13189">BEAM-13189&lt;/a>).&lt;/li>
&lt;li>Splittable reading is enabled by default while reading data with ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12070">BEAM-12070&lt;/a>).&lt;/li>
&lt;li>DoFn Execution Time metrics added to Go (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13001">BEAM-13001&lt;/a>).&lt;/li>
&lt;li>Cross-bundle side input caching is now available in the Go SDK for runners that support the feature by setting the EnableSideInputCache hook (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11097">BEAM-11097&lt;/a>).&lt;/li>
&lt;li>Upgraded the GCP Libraries BOM version to 24.0.0 and associated dependencies (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11205">BEAM-11205&lt;/a>). For Google Cloud client library versions set by this BOM,
see &lt;a href="https://storage.googleapis.com/cloud-opensource-java-dashboard/com.google.cloud/libraries-bom/24.0.0/artifact_details.html">this table&lt;/a>.&lt;/li>
&lt;li>Removed avro-python3 dependency in AvroIO. Fastavro has already been our Avro library of choice on Python 3. Boolean use_fastavro is left for api compatibility, but will have no effect.(&lt;a href="https://github.com/apache/beam/pull/15900">BEAM-13016&lt;/a>).&lt;/li>
&lt;li>MultiMap side inputs are now supported by the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-3293">BEAM-3293&lt;/a>).&lt;/li>
&lt;li>Remote packages can now be downloaded from locations supported by apache_beam.io.filesystems. The files will be downloaded on Stager and uploaded to staging location. For more information, see &lt;a href="https://issues.apache.org/jira/browse/BEAM-11275">BEAM-11275&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>A new URN convention was adopted for cross-language transforms and existing URNs were updated. This may break advanced use-cases, for example, if a custom expansion service is used to connect diffrent Beam Java and Python versions. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12047">BEAM-12047&lt;/a>).&lt;/li>
&lt;li>The upgrade to Calcite 1.28.0 introduces a breaking change in the SUBSTRING function in SqlTransform, when used with the Calcite dialect (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13099">BEAM-13099&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/CALCITE-4427">CALCITE-4427&lt;/a>).&lt;/li>
&lt;li>ListShards (with DescribeStreamSummary) is used instead of DescribeStream to list shards in Kinesis streams (AWS SDK v2). Due to this change, as mentioned in &lt;a href="https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html">AWS documentation&lt;/a>, for fine-grained IAM policies it is required to update them to allow calls to ListShards and DescribeStreamSummary APIs. For more information, see &lt;a href="https://docs.aws.amazon.com/streams/latest/dev/controlling-access.html">Controlling Access to Amazon Kinesis Data Streams&lt;/a> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13233">BEAM-13233&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Non-splittable reading is deprecated while reading data with ParquetIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12070">BEAM-12070&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Properly map main input windows to side input windows by default (Go)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-11087">BEAM-11087&lt;/a>).&lt;/li>
&lt;li>Fixed data loss when writing to DynamoDB without setting deduplication key names (Java)
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-13009">BEAM-13009&lt;/a>).&lt;/li>
&lt;li>Go SDK Examples now have types and functions registered. (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5378">BEAM-5378&lt;/a>)&lt;/li>
&lt;li>Fixed data loss when using Python WriteToFiles in streaming pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12950">BEAM-12950&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Users of beam-sdks-java-io-hcatalog (and beam-sdks-java-extensions-sql-hcatalog) must take care to override the transitive log4j dependency when they add a hive dependency (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13499">BEAM-13499&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.35.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay
Alexandr Zhuravlev
Alexey Romanenko
AlikRodriguez
Anand Inguva
Andrew Pilloud
Ankur Goenka
Anthony Sottile
Artur Khanin
Aydar Farrakhov
Aydar Zainutdinov
Benjamin Gonzalez
brachipa
Brian Hulette
Calvin Leung
Chamikara Jayalath
Chris Gray
Damon Douglas
Daniel Collins
Daniel Oliveira
daria.malkova
darshan-sj
David Huntsperger
David Prieto Rivera
Dmitrii Kuzin
dpcollins-google
dprieto
egalpin
Etienne Chauchot
Eugene Nikolaiev
Fernando Morales
Hector Lagos
Heejong Lee
Ilya Kozyrev
Iñigo San Jose Visiers
Jack McCluskey
Jiayang Wu
jrhy
Kenneth Knowles
KevinGG
kileys
klmilam
Kyle Weaver
Luís Bianchin
Luke Cwik
Melissa Pashniak
Michael Luckey
Miguel Hernandez
Milena Bukal
Minbo Bae
minherz
Moritz Mack
mosche
Natalie
Ning Kang
Pablo Estrada
Pavel Avilov
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Rogan Morrow
Ruslan Altynnikov
Sam Whittle
Sergey Kalinin
Slava Chernyak
Svetak Sundhar
Tianyang Hu
Tim Robertson
Tomo Suzuki
tuorhador
Udi Meiri
vachan-shetty
Valentyn Tymofieiev
Yichi Zhang
zhoufek&lt;/p></description><link>/blog/beam-2.35.0/</link><pubDate>Wed, 29 Dec 2021 10:11:00 -0800</pubDate><guid>/blog/beam-2.35.0/</guid><category>blog</category></item><item><title>Apache Beam 2.34.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.34.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2340-2021-11-11">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.34.0, check out the &lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12350405">detailed release
notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>The Beam Java API for Calcite SqlTransform is no longer experimental (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12680">BEAM-12680&lt;/a>).&lt;/li>
&lt;li>Python&amp;rsquo;s ParDo (Map, FlatMap, etc.) transforms now suport a &lt;code>with_exception_handling&lt;/code> option for easily ignoring bad records and implementing the dead letter pattern.&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>&lt;code>ReadFromBigQuery&lt;/code> and &lt;code>ReadAllFromBigQuery&lt;/code> now run queries with BATCH priority by default. The &lt;code>query_priority&lt;/code> parameter is introduced to the same transforms to allow configuring the query priority (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12913">BEAM-12913&lt;/a>).&lt;/li>
&lt;li>[EXPERIMENTAL] Support for &lt;a href="https://cloud.google.com/bigquery/docs/reference/storage">BigQuery Storage Read API&lt;/a> added to &lt;code>ReadFromBigQuery&lt;/code>. The newly introduced &lt;code>method&lt;/code> parameter can be set as &lt;code>DIRECT_READ&lt;/code> to use the Storage Read API. The default is &lt;code>EXPORT&lt;/code> which invokes a BigQuery export request. (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10917">BEAM-10917&lt;/a>).&lt;/li>
&lt;li>[EXPERIMENTAL] Added &lt;code>use_native_datetime&lt;/code> parameter to &lt;code>ReadFromBigQuery&lt;/code> to configure the return type of &lt;a href="https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#datetime_type">DATETIME&lt;/a> fields when using &lt;code>ReadFromBigQuery&lt;/code>. This parameter can &lt;em>only&lt;/em> be used when &lt;code>method = DIRECT_READ&lt;/code>(Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10917">BEAM-10917&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Upgrade to Calcite 1.26.0 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9379">BEAM-9379&lt;/a>).&lt;/li>
&lt;li>Added a new &lt;code>dataframe&lt;/code> extra to the Python SDK that tracks &lt;code>pandas&lt;/code> versions
we&amp;rsquo;ve verified compatibility with. We now recommend installing Beam with &lt;code>pip install apache-beam[dataframe]&lt;/code> when you intend to use the DataFrame API
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-12906">BEAM-12906&lt;/a>).&lt;/li>
&lt;li>Add an &lt;a href="https://github.com/cometta/python-apache-beam-spark">example&lt;/a> of deploying Python Apache Beam job with Spark Cluster&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>SQL Rows are no longer flattened (&lt;a href="https://issues.apache.org/jira/browse/BEAM-5505">BEAM-5505&lt;/a>).&lt;/li>
&lt;li>[Go SDK] beam.TryCrossLanguage&amp;rsquo;s signature now matches beam.CrossLanguage. Like other Try functions it returns an error instead of panicking. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9918">BEAM-9918&lt;/a>).&lt;/li>
&lt;li>&lt;a href="https://jira.apache.org/jira/browse/BEAM-12925">BEAM-12925&lt;/a> was fixed. It used to silently pass incorrect null data read from JdbcIO. Pipelines affected by this will now start throwing failures instead of silently passing incorrect data.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed error while writing multiple DeferredFrames to csv (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12701">BEAM-12701&lt;/a>).&lt;/li>
&lt;li>Fixed error when importing the DataFrame API with pandas 1.0.x installed (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12945">BEAM-12945&lt;/a>).&lt;/li>
&lt;li>Fixed top.SmallestPerKey implementation in the Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-12946">BEAM-12946&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.34.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay,
Aizhamal Nurmamat kyzy,
Alex Amato,
Alexander Chermenin,
Alexey Romanenko,
AlikRodriguez,
Andrew Pilloud,
Andy Xu,
Ankur Goenka,
Aydar Farrakhov,
Aydar Zainutdinov,
Aydar Zaynutdinov,
AydarZaynutdinov,
Benjamin Gonzalez,
BenWhitehead,
Brachi Packter,
Brian Hulette,
Bu Sun Kim,
Chamikara Jayalath,
Chris Gray,
Chuck Yang,
Chun Yang,
Claire McGinty,
comet,
Daniel Collins,
Daniel Oliveira,
Daniel Thevessen,
daria.malkova,
David Cavazos,
David Huntsperger,
Dmytro Kozhevin,
dpcollins-google,
Eduardo Sánchez López,
Elias Djurfeldt,
emily,
Emily Ye,
Enis Sert,
Etienne Chauchot,
Fernando Morales,
Heejong Lee,
Ihor Indyk,
Ismaël Mejía,
Israel Herraiz,
Jack McCluskey,
Jonathan Hourany,
Judah Rand,
Kenneth Knowles,
KevinGG,
Ke Wu,
kileys,
Kyle Weaver,
Luke Cwik,
masahitojp,
MiguelAnzoWizeline,
Minbo Bae,
Niels Basjes,
Ning Kang,
Pablo Estrada,
pareshsarafmdb,
Paul Féraud,
Piotr Szczepanik,
Reuven Lax,
Ritesh Ghorse,
R. Miles McCain,
Robert Bradshaw,
Robert Burke,
Rogan Morrow,
Ruwan Lambrichts,
rvballada,
Ryan Thompson,
Sam Rohde,
Sam Whittle,
Ștefan Istrate,
Steve Niemitz,
Thomas Li Fredriksen,
Tomo Suzuki,
tvalentyn,
Udi Meiri,
Vachan,
Valentyn Tymofieiev,
Vincent Marquez,
WinsonT,
Yichi Zhang,
Yifan Mai,
Yilei &amp;ldquo;Dolee&amp;rdquo; Yang,
zhoufek&lt;/p></description><link>/blog/beam-2.34.0/</link><pubDate>Thu, 11 Nov 2021 00:11:00 -0800</pubDate><guid>/blog/beam-2.34.0/</guid><category>blog</category></item><item><title>Go SDK Exits Experimental in Apache Beam 2.33.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Apache Beam’s latest release, version &lt;a href="/get-started/downloads/">2.33.0&lt;/a>, is the first official release of the long experimental Go SDK.
Built with the &lt;a href="https://golang.org/">Go Programming Language&lt;/a>, the Go SDK joins the Java and Python SDKs as the third implementation of the Beam programming model.&lt;/p>
&lt;h2 id="using-the-new-go-sdk">Using the new Go SDK.&lt;/h2>
&lt;p>New users of the Go SDK can start using it in their Go programs by importing the main beam package:&lt;/p>
&lt;pre>&lt;code>import &amp;quot;github.com/apache/beam/sdks/v2/go/pkg/beam&amp;quot;
&lt;/code>&lt;/pre>&lt;p>The next run of &lt;code>go mod tidy&lt;/code> will fetch the latest stable version of the module.
Alternatively executing &lt;code>go get github.com/apache/beam/sdks/v2/go/pkg/beam&lt;/code> will download it to the local module cache immeadiately, and add it to your &lt;code>go.mod&lt;/code> file.&lt;/p>
&lt;p>Existing users of the experimental Go SDK need to update to new &lt;code>v2&lt;/code> import paths to start using the latest versions of the SDK.
This can be done by adding &lt;code>v2&lt;/code> to the import paths, changing &lt;code>github.com/apache/beam/sdks/go/&lt;/code>&amp;hellip; to &lt;code>github.com/apache/beam/sdks/v2/go/&lt;/code>&amp;hellip; where applicable, and then running &lt;code>go mod tidy&lt;/code>.&lt;/p>
&lt;p>Further documentation on using the SDK is available in the &lt;a href="/documentation/programming-guide/">Beam Programming Guide&lt;/a>, and in the package &lt;a href="https://pkg.go.dev/github.com/apache/beam/sdks/v2/go/pkg/beam">Go Doc&lt;/a>.&lt;/p>
&lt;h2 id="feature-support">Feature Support&lt;/h2>
&lt;p>At time of writing, the Go SDK is currently &amp;ldquo;Batteries Not Included&amp;rdquo;.
This means that there are gaps or edge cases in supported IOs and transforms.
That said, the core of the SDK enables a great deal of the Beam Model for
custom user use, supporting the following features:&lt;/p>
&lt;ul>
&lt;li>PTransforms
&lt;ul>
&lt;li>Impulse&lt;/li>
&lt;li>Create&lt;/li>
&lt;li>ParDo with user DoFns
&lt;ul>
&lt;li>Iterable side inputs&lt;/li>
&lt;li>Multiple output emitters&lt;/li>
&lt;li>Receive and return key-value pairs&lt;/li>
&lt;li>SplittableDoFns&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>GroupByKey and CoGroupByKey&lt;/li>
&lt;li>Combine and CombinePerKey with user CombineFns&lt;/li>
&lt;li>Flatten&lt;/li>
&lt;li>Partition&lt;/li>
&lt;li>Composite transforms&lt;/li>
&lt;li>Cross language transforms&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Event time windowing
&lt;ul>
&lt;li>Global, Interval, Sliding, and Session windows&lt;/li>
&lt;li>Aggregating over windowed PCollections with GroupByKeys or Combines&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Coders
&lt;ul>
&lt;li>Primitive Go types (ints, string, []bytes, and more)&lt;/li>
&lt;li>Beam Schemas for Go struct types (including struct, slice, and map fields)&lt;/li>
&lt;li>Registering custom coders&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Metrics
&lt;ul>
&lt;li>PCollection metrics (element counts, size estimates)&lt;/li>
&lt;li>Custom user metrics&lt;/li>
&lt;li>Post job user metrics querying (coming in 2.34.0)&lt;/li>
&lt;li>DoFn profiling metrics (coming in 2.35.0)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Built-in transforms
&lt;ul>
&lt;li>Sum, count, min, max, top, filter&lt;/li>
&lt;li>Scalable TextIO reading&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Upcoming feature roadmap, and known issues are discussed below.
In particular, we plan to support a much richer set of IO connectors via Beam&amp;rsquo;s cross-language capabilities.&lt;/p>
&lt;h2 id="releases">Releases&lt;/h2>
&lt;p>With this release, the Go SDK now uses &lt;a href="https://golang.org/ref/mod">Go Modules&lt;/a> for dependency management.
This makes it so users, SDK authors, and the testing infrastructure can all rely on the same versions of dependencies, making builds reproducible.
This also makes &lt;a href="/blog/validate-beam-release/#configuring-a-go-build-to-validate-a-beam-release-candidate">validating Go SDK Release Candidates simple&lt;/a>.&lt;/p>
&lt;p>Versioned SDK worker containers are now built and &lt;a href="https://hub.docker.com/r/apache/beam_go_sdk/tags?page=1&amp;amp;ordering=last_updated">published&lt;/a>, with the SDK using matching tagged versions.
User jobs no longer need to specify a container to use, except when using custom containers.&lt;/p>
&lt;h2 id="compatibility">Compatibility&lt;/h2>
&lt;p>The Go SDK will largely follow suit with the Go notion of compatibility.
Some concessions are made to keep all SDKs together on the same release cycle.&lt;/p>
&lt;h3 id="language-compatibility">Language Compatibility&lt;/h3>
&lt;p>The SDK will be tested at a minimum &lt;a href="https://golang.org/doc/devel/release">Go Programming Language version of 1.16&lt;/a>, and use available language features and standard library packages accordingly.
To maintain a broad compatibility, the Go SDK will not require the latest major version of Go.
We expect to follow the 2nd newest supported release of the language, with a possible exception when Go 1.18 is released, in order to begin experimenting with &lt;a href="https://go.dev/blog/generics-proposal">Go Generics&lt;/a> in the SDK.
Release notes will call out when the minimum version of the language changes.&lt;/p>
&lt;h3 id="package-compatibility">Package Compatibility&lt;/h3>
&lt;p>The primary user packages will avoid changing in backwards incompatible ways for core features.
This is to be inline with Go&amp;rsquo;s notion of the &lt;a href="https://research.swtch.com/vgo-import">&lt;code>import compatibility rule&lt;/code>&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>If an old package and a new package have the same import path,
the new package must be backwards compatible with the old package.&lt;/p>
&lt;/blockquote>
&lt;p>Exceptions to this policy are around newer, experimental, or in development features and are subject to change.
Such features will have a doc comment noting the experimental status.
Major changes will be mentioned in the release notes.
For example, using &lt;code>beam.WindowInto&lt;/code> with Triggers is currently experimental and may have the API changed in a future release.&lt;/p>
&lt;p>Primary user packages include:&lt;/p>
&lt;ul>
&lt;li>The main beam package &lt;code>github.com/apache/beam/sdks/v2/go/pkg/beam&lt;/code>&lt;/li>
&lt;li>Sub packages under &lt;code>.../transforms&lt;/code>, &lt;code>.../io&lt;/code>, &lt;code>.../runners&lt;/code>, and &lt;code>.../testing&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Generally, packages in the module other than the primary user packages are for framework use and are at risk of changing.&lt;/p>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;h4 id="batteries-not-included">Batteries not included.&lt;/h4>
&lt;ul>
&lt;li>Current native transforms are undertested&lt;/li>
&lt;li>IOs may not be written to scale&lt;/li>
&lt;li>Go Direct Runner is incomplete and is not portable, prefer using the Python Portable runner, or Flink
&lt;ul>
&lt;li>Doesn&amp;rsquo;t support side input windowing. &lt;a href="https://issues.apache.org/jira/browse/BEAM-13075">BEAM-13075&lt;/a>&lt;/li>
&lt;li>Doesn&amp;rsquo;t serialize data, making it unlikely to catch coder issues &lt;a href="https://issues.apache.org/jira/browse/BEAM-6372">BEAM-6372&lt;/a>&lt;/li>
&lt;li>Can use other general improvements, and become portable &lt;a href="https://issues.apache.org/jira/browse/BEAM-11076">BEAM-11076&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Current Trigger API is under iteration and subject to change &lt;a href="https://issues.apache.org/jira/browse/BEAM-3304">BEAM-3304&lt;/a>
&lt;ul>
&lt;li>API has a possible breaking change between 2.33.0 and 2.34.0, and may change again&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Support of the SDK on services, like Google Cloud Dataflow, remains at the service owner&amp;rsquo;s discretion&lt;/li>
&lt;li>Need something?
&lt;ul>
&lt;li>File a ticket in the &lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20BEAM%20AND%20component%20%3D%20sdk-go">Beam JIRA&lt;/a> and,&lt;/li>
&lt;li>Email the &lt;a href="mailto:dev@beam.apache.org?subject=%5BGo%20SDK%20Feature%5D">dev@beam.apache.org&lt;/a> list!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="fixed-in-2340">Fixed in 2.34.0&lt;/h4>
&lt;ul>
&lt;li>&lt;code>top.SmallestPerKey&lt;/code> was broken &lt;a href="https://issues.apache.org/jira/browse/BEAM-12946">BEAM-12946&lt;/a>&lt;/li>
&lt;li>&lt;code>beam.TryCrossLanguage&lt;/code> API didn&amp;rsquo;t match non-Try version &lt;a href="https://issues.apache.org/jira/browse/BEAM-9918">BEAM-9918&lt;/a>
&lt;ul>
&lt;li>This is a breaking change if one was calling &lt;code>beam.TryCrossLanguage&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="fixed-in-2350">Fixed in 2.35.0&lt;/h4>
&lt;ul>
&lt;li>Non-global window side inputs don&amp;rsquo;t match (correctness bug) &lt;a href="https://issues.apache.org/jira/browse/BEAM-11087">BEAM-11087&lt;/a>
&lt;ul>
&lt;li>Until 2.35.0 it&amp;rsquo;s not recommended to use side inputs that are not using the global window.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>DoFns using side inputs accumulate memory over bundles, causing out of memory issues &lt;a href="https://issues.apache.org/jira/browse/BEAM-13130">BEAM-13130&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="roadmap">Roadmap&lt;/h2>
&lt;p>The &lt;a href="/roadmap/go-sdk/">SDK roadmap&lt;/a> has been updated.
Ongoing focus is to bolster streaming focused features, improve existing connectors, and make connectors easier to implement.&lt;/p>
&lt;p>In the nearer term this comes in the form of improvements to side inputs, and providing wrappers and improving ease-of-use for cross language transforms from Java.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>We hope you find the SDK useful, and it&amp;rsquo;s still early days.
If you make something with the Go SDK, consider &lt;a href="/community/contact-us/">sharing it with us&lt;/a>.
And remember, &lt;a href="/contribute/">contributions&lt;/a> are always welcome.&lt;/p></description><link>/blog/go-sdk-release/</link><pubDate>Thu, 04 Nov 2021 00:00:01 -0800</pubDate><guid>/blog/go-sdk-release/</guid><category>blog</category></item></channel></rss>