<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Apache Beam</title><description>Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.</description><link>/</link><generator>Hugo -- gohugo.io</generator><item><title>Improved Annotation Support for the Python SDK</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The importance of static type checking in a dynamically
typed language like Python is not up for debate. Type hints
allow developers to leverage a strong typing system to:&lt;/p>
&lt;ul>
&lt;li>write better code,&lt;/li>
&lt;li>self-document ambiguous programming logic, and&lt;/li>
&lt;li>inform intelligent code completion in IDEs like PyCharm.&lt;/li>
&lt;/ul>
&lt;p>This is why we&amp;rsquo;re excited to announce upcoming improvements to
the &lt;code>typehints&lt;/code> module of Beam&amp;rsquo;s Python SDK, including support
for typed PCollections and Python 3 style annotations on PTransforms.&lt;/p>
&lt;h1 id="improved-annotations">Improved Annotations&lt;/h1>
&lt;p>Today, you have the option to declare type hints on PTransforms using either
class decorators or inline functions.&lt;/p>
&lt;p>For instance, a PTransform with decorated type hints might look like this:&lt;/p>
&lt;pre>&lt;code>@beam.typehints.with_input_types(int)
@beam.typehints.with_output_types(str)
class IntToStr(beam.PTransform):
def expand(self, pcoll):
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr())
&lt;/code>&lt;/pre>&lt;p>Using inline functions instead, the same transform would look like this:&lt;/p>
&lt;pre>&lt;code>class IntToStr(beam.PTransform):
def expand(self, pcoll):
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr()).with_input_types(int).with_output_types(str)
&lt;/code>&lt;/pre>&lt;p>Both methods have problems. Class decorators are syntax-heavy,
requiring two additional lines of code, whereas inline functions provide type hints
that aren&amp;rsquo;t reusable across other instances of the same transform. Additionally, both
methods are incompatible with static type checkers like MyPy.&lt;/p>
&lt;p>With Python 3 annotations however, we can subvert these problems to provide a
clean and reusable type hint experience. Our previous transform now looks like this:&lt;/p>
&lt;pre>&lt;code>class IntToStr(beam.PTransform):
def expand(self, pcoll: PCollection[int]) -&amp;gt; PCollection[str]:
return pcoll | beam.Map(lambda num: str(num))
strings = numbers | beam.ParDo(IntToStr())
&lt;/code>&lt;/pre>&lt;p>These type hints will actively hook into the internal Beam typing system to
play a role in pipeline type checking, and runtime type checking.&lt;/p>
&lt;p>So how does this work?&lt;/p>
&lt;h2 id="typed-pcollections">Typed PCollections&lt;/h2>
&lt;p>You guessed it! The PCollection class inherits from &lt;code>typing.Generic&lt;/code>, allowing it to be
parameterized with either zero types (denoted &lt;code>PCollection&lt;/code>) or one type (denoted &lt;code>PCollection[T]&lt;/code>).&lt;/p>
&lt;ul>
&lt;li>A PCollection with zero types is implicitly converted to &lt;code>PCollection[Any]&lt;/code>.&lt;/li>
&lt;li>A PCollection with one type can have any nested type (e.g. &lt;code>Union[int, str]&lt;/code>).&lt;/li>
&lt;/ul>
&lt;p>Internally, Beam&amp;rsquo;s typing system makes these annotations compatible with other
type hints by removing the outer PCollection container.&lt;/p>
&lt;h2 id="pbegin-pdone-none">PBegin, PDone, None&lt;/h2>
&lt;p>Finally, besides PCollection, a valid annotation on the &lt;code>expand(...)&lt;/code> method of a PTransform is
&lt;code>PBegin&lt;/code> or &lt;code>None&lt;/code>. These are generally used for PTransforms that begin or end with an I/O operation.&lt;/p>
&lt;p>For instance, when saving data, your transform&amp;rsquo;s output type should be &lt;code>None&lt;/code>.&lt;/p>
&lt;pre>&lt;code>class SaveResults(beam.PTransform):
def expand(self, pcoll: PCollection[str]) -&amp;gt; None:
return pcoll | beam.io.WriteToBigQuery(...)
&lt;/code>&lt;/pre>&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>What are you waiting for.. start using annotations on your transforms!&lt;/p>
&lt;p>For more background on type hints in Python, see:
&lt;a href="https://beam.apache.org/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>.&lt;/p>
&lt;p>Finally, please
&lt;a href="https://beam.apache.org/community/contact-us/">let us know&lt;/a>
if you encounter any issues.&lt;/p></description><link>/blog/python-improved-annotations/</link><pubDate>Fri, 21 Aug 2020 00:00:01 -0800</pubDate><guid>/blog/python-improved-annotations/</guid><category>blog</category><category>python</category><category>typing</category></item><item><title>Performance-Driven Runtime Type Checking for the Python SDK</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>In this blog post, we&amp;rsquo;re announcing the upcoming release of a new, opt-in
runtime type checking system for Beam&amp;rsquo;s Python SDK that&amp;rsquo;s optimized for performance
in both development and production environments.&lt;/p>
&lt;p>But let&amp;rsquo;s take a step back - why do we even care about runtime type checking
in the first place? Let&amp;rsquo;s look at an example.&lt;/p>
&lt;pre>&lt;code>class MultiplyNumberByTwo(beam.DoFn):
def process(self, element: int):
return element * 2
p = Pipeline()
p | beam.Create(['1', '2'] | beam.ParDo(MultiplyNumberByTwo())
&lt;/code>&lt;/pre>&lt;p>In this code, we passed a list of strings to a DoFn that&amp;rsquo;s clearly intended for use with
integers. Luckily, this code will throw an error during pipeline construction because
the inferred output type of &lt;code>beam.Create(['1', '2'])&lt;/code> is &lt;code>str&lt;/code> which is incompatible with
the declared input type of &lt;code>MultiplyNumberByTwo.process&lt;/code> which is &lt;code>int&lt;/code>.&lt;/p>
&lt;p>However, what if we turned pipeline type checking off using the &lt;code>no_pipeline_type_check&lt;/code>
flag? Or more realistically, what if the input PCollection to &lt;code>MultiplyNumberByTwo&lt;/code> arrived
from a database, meaning that the output data type can only be known at runtime?&lt;/p>
&lt;p>In either case, no error would be thrown during pipeline construction.
And even at runtime, this code works. Each string would be multiplied by 2,
yielding a result of &lt;code>['11', '22']&lt;/code>, but that&amp;rsquo;s certainly not the outcome we want.&lt;/p>
&lt;p>So how do you debug this breed of &amp;ldquo;hidden&amp;rdquo; errors? More broadly speaking, how do you debug
any typing or serialization error in Beam?&lt;/p>
&lt;p>The answer is to use runtime type checking.&lt;/p>
&lt;h1 id="runtime-type-checking-rtc">Runtime Type Checking (RTC)&lt;/h1>
&lt;p>This feature works by checking that actual input and output values satisfy the declared
type constraints during pipeline execution. If you ran the code from before with
&lt;code>runtime_type_check&lt;/code> on, you would receive the following error message:&lt;/p>
&lt;pre>&lt;code>Type hint violation for 'ParDo(MultiplyByTwo)': requires &amp;lt;class 'int'&amp;gt; but got &amp;lt;class 'str'&amp;gt; for element
&lt;/code>&lt;/pre>&lt;p>This is an actionable error message - it tells you that either your code has a bug
or that your declared type hints are incorrect. Sounds simple enough, so what&amp;rsquo;s the catch?&lt;/p>
&lt;p>&lt;em>It is soooo slowwwwww.&lt;/em> See for yourself.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Element Size&lt;/th>
&lt;th>Normal Pipeline&lt;/th>
&lt;th>Runtime Type Checking Pipeline&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>5.3 sec&lt;/td>
&lt;td>5.6 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2,001&lt;/td>
&lt;td>9.4 sec&lt;/td>
&lt;td>57.2 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10,001&lt;/td>
&lt;td>24.5 sec&lt;/td>
&lt;td>259.8 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18,001&lt;/td>
&lt;td>38.7 sec&lt;/td>
&lt;td>450.5 sec&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In this micro-benchmark, the pipeline with runtime type checking was over 10x slower,
with the gap only increasing as our input PCollection increased in size.&lt;/p>
&lt;p>So, is there any production-friendly alternative?&lt;/p>
&lt;h1 id="performance-runtime-type-check">Performance Runtime Type Check&lt;/h1>
&lt;p>There is! We developed a new flag called &lt;code>performance_runtime_type_check&lt;/code> that
minimizes its footprint on the pipeline&amp;rsquo;s time complexity using a combination of&lt;/p>
&lt;ul>
&lt;li>efficient Cython code,&lt;/li>
&lt;li>smart sampling techniques, and&lt;/li>
&lt;li>optimized mega type-hints.&lt;/li>
&lt;/ul>
&lt;p>So what do the new numbers look like?&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Element Size&lt;/th>
&lt;th>Normal&lt;/th>
&lt;th>RTC&lt;/th>
&lt;th>Performance RTC&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>5.3 sec&lt;/td>
&lt;td>5.6 sec&lt;/td>
&lt;td>5.4 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2,001&lt;/td>
&lt;td>9.4 sec&lt;/td>
&lt;td>57.2 sec&lt;/td>
&lt;td>11.2 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10,001&lt;/td>
&lt;td>24.5 sec&lt;/td>
&lt;td>259.8 sec&lt;/td>
&lt;td>25.5 sec&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18,001&lt;/td>
&lt;td>38.7 sec&lt;/td>
&lt;td>450.5 sec&lt;/td>
&lt;td>39.4 sec&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>On average, the new Performance RTC is 4.4% slower than a normal pipeline whereas the old RTC
is over 900% slower! Additionally, as the size of the input PCollection increases, the fixed cost
of setting up the Performance RTC system is spread across each element, decreasing the relative
impact on the overall pipeline. With 18,001 elements, the difference is less than 1 second.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>There are three key factors responsible for this upgrade in performance.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Instead of type checking all values, we only type check a subset of values, known as
a sample in statistics. Initially, we sample a substantial number of elements, but as our
confidence that the element type won&amp;rsquo;t change over time increases, we reduce our
sampling rate (up to a fixed minimum).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Whereas the old RTC system used heavy wrappers to perform the type check, the new RTC system
moves the type check to a Cython-optimized, non-decorated portion of the codebase. For reference,
Cython is a programming language that gives C-like performance to Python code.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Finally, we use a single mega type hint to type-check only the output values of transforms
instead of type-checking both the input and output values separately. This mega typehint is composed of
the original transform&amp;rsquo;s output type constraints along with all consumer transforms&amp;rsquo; input type
constraints. Using this mega type hint allows us to reduce overhead while simultaneously allowing
us to throw &lt;em>more actionable errors&lt;/em>. For instance, consider the following error (which was
generated from the old RTC system):&lt;/p>
&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>Runtime type violation detected within ParDo(DownstreamDoFn): Type-hint for argument: 'element' violated. Expected an instance of &amp;lt;class ‘str’&amp;gt;, instead found 9, an instance of &amp;lt;class ‘int’&amp;gt;.
&lt;/code>&lt;/pre>&lt;p>This error tells us that the &lt;code>DownstreamDoFn&lt;/code> received an &lt;code>int&lt;/code> when it was expecting a &lt;code>str&lt;/code>, but doesn&amp;rsquo;t tell us
who created that &lt;code>int&lt;/code> in the first place. Who is the offending upstream transform that&amp;rsquo;s responsible for
this &lt;code>int&lt;/code>? Presumably, &lt;em>that&lt;/em> transform&amp;rsquo;s output type hints were too expansive (e.g. &lt;code>Any&lt;/code>) or otherwise non-existent because
no error was thrown during the runtime type check of its output.&lt;/p>
&lt;p>The problem here boils down to a lack of context. If we knew who our consumers were when type
checking our output, we could simultaneously type check our output value against our output type
constraints and every consumers&amp;rsquo; input type constraints to know whether there is &lt;em>any&lt;/em> possibility
for a mismatch. This is exactly what the mega type hint does, and it allows us to throw errors
at the point of declaration rather than the point of exception, saving you valuable time
while providing higher quality error messages.&lt;/p>
&lt;p>So what would the same error look like using Performance RTC? It&amp;rsquo;s the exact same string but with one additional line:&lt;/p>
&lt;pre>&lt;code>[while running 'ParDo(UpstreamDoFn)']
&lt;/code>&lt;/pre>&lt;p>And that&amp;rsquo;s much more actionable for an investigation :)&lt;/p>
&lt;h1 id="next-steps">Next Steps&lt;/h1>
&lt;p>Go play with the new &lt;code>performance_runtime_type_check&lt;/code> feature!&lt;/p>
&lt;p>It&amp;rsquo;s in an experimental state so please
&lt;a href="https://beam.apache.org/community/contact-us/">let us know&lt;/a>
if you encounter any issues.&lt;/p></description><link>/blog/python-performance-runtime-type-checking/</link><pubDate>Fri, 21 Aug 2020 00:00:01 -0800</pubDate><guid>/blog/python-performance-runtime-type-checking/</guid><category>blog</category><category>python</category><category>typing</category></item><item><title>Apache Beam 2.23.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.23.0 release of Apache Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2230-2020-07-29">download page&lt;/a> for this release.
For more information on changes in 2.23.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347145">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Twister2 Runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-7304">BEAM-7304&lt;/a>).&lt;/li>
&lt;li>Python 3.8 support (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8494">BEAM-8494&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Support for reading from Snowflake added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9722">BEAM-9722&lt;/a>).&lt;/li>
&lt;li>Support for writing to Splunk added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8596">BEAM-8596&lt;/a>).&lt;/li>
&lt;li>Support for assume role added (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10335">BEAM-10335&lt;/a>).&lt;/li>
&lt;li>A new transform to read from BigQuery has been added: &lt;code>apache_beam.io.gcp.bigquery.ReadFromBigQuery&lt;/code>. This transform
is experimental. It reads data from BigQuery by exporting data to Avro files, and reading those files. It also supports
reading data by exporting to JSON files. This has small differences in behavior for Time and Date-related fields. See
Pydoc for more information.&lt;/li>
&lt;li>Add dispositions for SnowflakeIO.write (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10343">BEAM-10343&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Update Snowflake JDBC dependency and add application=beam to connection URL (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10383">BEAM-10383&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>&lt;code>RowJson.RowJsonDeserializer&lt;/code>, &lt;code>JsonToRow&lt;/code>, and &lt;code>PubsubJsonTableProvider&lt;/code> now accept &amp;ldquo;implicit
nulls&amp;rdquo; by default when deserializing JSON (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-10220">BEAM-10220&lt;/a>).
Previously nulls could only be represented with explicit null values, as in
&lt;code>{&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;, &amp;quot;baz&amp;quot;: null}&lt;/code>, whereas an implicit null like &lt;code>{&amp;quot;foo&amp;quot;: &amp;quot;bar&amp;quot;}&lt;/code> would raise an
exception. Now both JSON strings will yield the same result by default. This behavior can be
overridden with &lt;code>RowJson.RowJsonDeserializer#withNullBehavior&lt;/code>.&lt;/li>
&lt;li>Fixed a bug in &lt;code>GroupIntoBatches&lt;/code> experimental transform in Python to actually group batches by key.
This changes the output type for this transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-6696">BEAM-6696&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Remove Gearpump runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9999">BEAM-9999&lt;/a>)&lt;/li>
&lt;li>Remove Apex runner. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9999">BEAM-9999&lt;/a>)&lt;/li>
&lt;li>RedisIO.readAll() is deprecated and will be removed in 2 versions, users must use RedisIO.readKeyPatterns() as a replacement (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9747">BEAM-9747&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.23.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron, Abhishek Yadav, Ahmet Altay, aiyangar, Aizhamal Nurmamat kyzy, Ajo Thomas, Akshay-Iyangar, Alan Pryor, Alex Amato, Alexey Romanenko, Allen Pradeep Xavier, Andrew Crites, Andrew Pilloud, Ankur Goenka, Anna Qin, Ashwin Ramaswami, bntnam, Borzoo Esmailloo, Boyuan Zhang, Brian Hulette, Brian Michalski, brucearctor, Chamikara Jayalath, chi-chi weng, Chuck Yang, Chun Yang, Colm O hEigeartaigh, Corvin Deboeser, Craig Chambers, Damian Gadomski, Damon Douglas, Daniel Oliveira, Dariusz Aniszewski, darshanj, darshan jani, David Cavazos, David Moravek, David Yan, Esun Kim, Etienne Chauchot, Filipe Regadas, fuyuwei, Graeme Morgan, Hannah-Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, InigoSJ, Ismaël Mejía, Israel Herraiz, Jacob Ferriero, Jan Lukavský, Jie Fan, John Mora, Jozef Vilcek, Julien Phalip, Justine Koa, Kamil Gabryjelski, Kamil Wasilewski, Kasia Kucharczyk, Kenneth Jung, Kenneth Knowles, kevingg, Kevin Sijo Puthusseri, kshivvy, Kyle Weaver, Kyoungha Min, Kyungwon Jo, Luke Cwik, Mark Liu, Mark-Zeng, Matthias Baetens, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Nam Bui, Nathan Fisher, Niel Markwick, Ning Kang, Omar Ismail, Pablo Estrada, paul fisher, Pawel Pasterz, perkss, Piotr Szuberski, pulasthi, purbanow, Rahul Patwari, Rajat Mittal, Rehman, Rehman Murad Ali, Reuben van Ammers, Reuven Lax, Reza Rokni, Rion Williams, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, sabhyankar, Sam Rohde, Sam Whittle, sclukas77, Sebastian Graca, Shoaib Zafar, Sruthi Sree Kumar, Stephen O&amp;rsquo;Kennedy, Steve Koonce, Steve Niemitz, Steven van Rossum, Ted Romer, Tesio, Thinh Ha, Thomas Weise, Tobias Kaymak, tobiaslieber-cognitedata, Tobiasz Kędzierski, Tomo Suzuki, Tudor Marian, tvs, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, xuelianhan, Yichi Zhang, Yifan Zou, Yixing Zhang, yoshiki.obata, Yueyang Qiu, Yu Feng, Yuwei Fu, Zhuo Peng, ZijieSong946.&lt;/p></description><link>/blog/beam-2.23.0/</link><pubDate>Wed, 29 Jul 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.23.0/</guid><category>blog</category></item><item><title>Apache Beam 2.22.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.22.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2220-2020-06-08">download page&lt;/a> for this release.
For more information on changes in 2.22.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347144">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Basic Kafka read/write support for DataflowRunner (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8019">BEAM-8019&lt;/a>).&lt;/li>
&lt;li>Sources and sinks for Google Healthcare APIs (Java)(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9468">BEAM-9468&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>&lt;code>--workerCacheMB&lt;/code> flag is supported in Dataflow streaming pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9964">BEAM-9964&lt;/a>)&lt;/li>
&lt;li>&lt;code>--direct_num_workers=0&lt;/code> is supported for FnApi runner. It will set the number of threads/subprocesses to number of cores of the machine executing the pipeline (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9443">BEAM-9443&lt;/a>).&lt;/li>
&lt;li>Python SDK now has experimental support for SqlTransform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8603">BEAM-8603&lt;/a>).&lt;/li>
&lt;li>Add OnWindowExpiration method to Stateful DoFn (&lt;a href="https://issues.apache.org/jira/browse/BEAM-1589">BEAM-1589&lt;/a>).&lt;/li>
&lt;li>Added PTransforms for Google Cloud DLP (Data Loss Prevention) services integration (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9723">BEAM-9723&lt;/a>):
&lt;ul>
&lt;li>Inspection of data,&lt;/li>
&lt;li>Deidentification of data,&lt;/li>
&lt;li>Reidentification of data.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Add a more complete I/O support matrix in the documentation site (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9916">BEAM-9916&lt;/a>).&lt;/li>
&lt;li>Upgrade Sphinx to 3.0.3 for building PyDoc.&lt;/li>
&lt;li>Added a PTransform for image annotation using Google Cloud AI image processing service
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9646">BEAM-9646&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Python SDK now requires &lt;code>--job_endpoint&lt;/code> to be set when using &lt;code>--runner=PortableRunner&lt;/code> (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9860">BEAM-9860&lt;/a>). Users seeking the old default behavior should set &lt;code>--runner=FlinkRunner&lt;/code> instead.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.22.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, aiyangar, Ajo Thomas, Akshay-Iyangar, Alan Pryor, Alexey Romanenko, Allen Pradeep Xavier, amaliujia, Andrew Pilloud, Ankur Goenka, Ashwin Ramaswami, bntnam, Borzoo Esmailloo, Boyuan Zhang, Brian Hulette, Chamikara Jayalath, Colm O hEigeartaigh, Craig Chambers, Damon Douglas, Daniel Oliveira, David Cavazos, David Moravek, Esun Kim, Etienne Chauchot, Filipe Regadas, Graeme Morgan, Hannah Jiang, Hannah-Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, Ismaël Mejía, Israel Herraiz, Jacob Ferriero, Jan Lukavský, John Mora, Kamil Wasilewski, Kenneth Jung, Kenneth Knowles, kevingg, Kyle Weaver, Kyoungha Min, Kyungwon Jo, Luke Cwik, Mark Liu, Matthias Baetens, Maximilian Michels, Michal Walenia, Mikhail Gryzykhin, Nam Bui, Niel Markwick, Ning Kang, Omar Ismail, omarismail94, Pablo Estrada, paul fisher, pawelpasterz, Pawel Pasterz, Piotr Szuberski, Rahul Patwari, rarokni, Rehman, Rehman Murad Ali, Reuven Lax, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Sam Rohde, Sam Whittle, Sebastian Graca, Shoaib Zafar, Sruthi Sree Kumar, Stephen O&amp;rsquo;Kennedy, Steve Koonce, Steve Niemitz, Steven van Rossum, Tesio, Thomas Weise, tobiaslieber-cognitedata, Tomo Suzuki, Tudor Marian, tvalentyn, Tyson Hamilton, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, xuelianhan, Yichi Zhang, Yifan Zou, yoshiki.obata, Yueyang Qiu, Zhuo Peng&lt;/p></description><link>/blog/beam-2.22.0/</link><pubDate>Mon, 08 Jun 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.22.0/</guid><category>blog</category></item><item><title>Announcing Beam Katas for Kotlin</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Today, we are happy to announce a new addition to the Beam Katas family: Kotlin!&lt;/p>
&lt;p>&lt;img src="/images/blog/beam-katas-kotlin-release/beam-and-kotlin.png" alt="Apache Beam and Kotlin Shaking Hands" height="330" width="800" >&lt;/p>
&lt;p>You may remember &lt;a href="https://beam.apache.org/blog/beam-kata-release">a post from last year&lt;/a> that informed everyone of the wonderful Beam Katas available on &lt;a href="https://stepik.org">Stepik&lt;/a>
for learning more about writing Apache Beam applications, working with its various APIs and programming model
hands-on, all from the comfort of your favorite IDEs. As of today, you can now work through all of the progressive
exercises to learn about the fundamentals of Beam in Kotlin.&lt;/p>
&lt;p>&lt;a href="https://kotlinlang.org">Kotlin&lt;/a> is a modern, open-source, statically typed language that targets the JVM. It is most commonly used by
Android developers, however it has recently risen in popularity due to its extensive feature set that enables
more concise and cleaner code than Java, without sacrificing performance or type safety. It recently was &lt;a href="https://insights.stackoverflow.com/survey/2020#technology-most-loved-dreaded-and-wanted-languages-loved">ranked
as one of the most beloved programming languages in the annual Stack Overflow Developer Survey&lt;/a>, so don&amp;rsquo;t take
just our word for it.&lt;/p>
&lt;p>The relationship between Apache Beam and Kotlin isn&amp;rsquo;t a new one. You can find examples scattered across the web
of engineering teams embracing the two technologies including &lt;a href="https://beam.apache.org/blog/beam-kotlin/">a series of samples announced on this very blog&lt;/a>.
If you are new to Beam or are an experienced veteran looking for a change of pace, we&amp;rsquo;d encourage you to give
Kotlin a try.&lt;/p>
&lt;p>You can find the Kotlin and the other excellent Beam Katas below (or by just searching for &amp;ldquo;Beam Katas&amp;rdquo; within
&lt;a href="https://www.jetbrains.com/education/download/#section=idea">IntelliJ&lt;/a> or &lt;a href="https://www.jetbrains.com/education/download/#section=pycharm-edu">PyCharm&lt;/a> through &lt;a href="https://plugins.jetbrains.com/plugin/10081-edutools">the EduTools plugin&lt;/a>):&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://stepik.org/course/72488">&lt;strong>Kotlin&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/54530">&lt;strong>Java&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/54532">&lt;strong>Python&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stepik.org/course/70387">&lt;strong>Go (in development)&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>I&amp;rsquo;d like to extend a very special thanks to &lt;a href="https://twitter.com/henry_ken">Henry Suryawirawan&lt;/a> for his creation of the original series of Katas
and his support during the review process and making this effort a reality.&lt;/p>
&lt;p>&lt;br />&lt;/p>
&lt;p>&lt;img src="/images/blog/beam-katas-kotlin-release/beam-katas-in-edutools.png" alt="Access Beam Katas Kotlin through a JetBrains Educational Product" height="252" width="800" >&lt;/p></description><link>/blog/beam-katas-kotlin-release/</link><pubDate>Mon, 01 Jun 2020 00:00:01 -0800</pubDate><guid>/blog/beam-katas-kotlin-release/</guid><category>blog</category></item><item><title>Python SDK Typing Changes</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Beam Python has recently increased its support and integration of Python 3 type
annotations for improved code clarity and type correctness checks.
Read on to find out what&amp;rsquo;s new.&lt;/p>
&lt;p>Python supports type annotations on functions (PEP 484). Static type checkers,
such as mypy, are used to verify adherence to these types.
For example:&lt;/p>
&lt;pre>&lt;code>def f(v: int) -&amp;gt; int:
return v[0]
&lt;/code>&lt;/pre>&lt;p>Running mypy on the above code will give the error:
&lt;code>Value of type &amp;quot;int&amp;quot; is not indexable&lt;/code>.&lt;/p>
&lt;p>We&amp;rsquo;ve recently made changes to Beam in 2 areas:&lt;/p>
&lt;p>Adding type annotations throughout Beam. Type annotations make a large and
sophisticated codebase like Beam easier to comprehend and navigate in your
favorite IDE.&lt;/p>
&lt;p>Second, we&amp;rsquo;ve added support for Python 3 type annotations. This allows SDK
users to specify a DoFn&amp;rsquo;s type hints in one place.
We&amp;rsquo;ve also expanded Beam&amp;rsquo;s support of &lt;code>typing&lt;/code> module types.&lt;/p>
&lt;p>For more background see:
&lt;a href="https://beam.apache.org/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>.&lt;/p>
&lt;h1 id="beam-is-typed">Beam Is Typed&lt;/h1>
&lt;p>In tandem with the new type annotation support within DoFns, we&amp;rsquo;ve invested a
great deal of time adding type annotations to the Beam python code itself.
With this in place, we have begun using mypy, a static type
checker, as part of Beam&amp;rsquo;s code review process, which ensures higher quality
contributions and fewer bugs.
The added context and insight that type annotations add throughout Beam is
useful for all Beam developers, contributors and end users alike, but
it is especially beneficial for developers who are new to the project.
If you use an IDE that understands type annotations, it will provide richer
type completions and warnings than before.
You&amp;rsquo;ll also be able to use your IDE to inspect the types of Beam functions and
transforms to better understand how they work, which will ease your own
development.
Finally, once Beam is fully annotated, end users will be able to benefit from
the use of static type analysis on their own pipelines and custom transforms.&lt;/p>
&lt;h1 id="new-ways-to-annotate">New Ways to Annotate&lt;/h1>
&lt;h2 id="python-3-syntax-annotations">Python 3 Syntax Annotations&lt;/h2>
&lt;p>Coming in Beam 2.21 (BEAM-8280), you will be able to use Python annotation
syntax to specify input and output types.&lt;/p>
&lt;p>For example, this new form:&lt;/p>
&lt;pre>&lt;code>class MyDoFn(beam.DoFn):
def process(self, element: int) -&amp;gt; typing.Text:
yield str(element)
&lt;/code>&lt;/pre>&lt;p>is equivalent to this:&lt;/p>
&lt;pre>&lt;code>@apache_beam.typehints.with_input_types(int)
@apache_beam.typehints.with_output_types(typing.Text)
class MyDoFn(beam.DoFn):
def process(self, element):
yield str(element)
&lt;/code>&lt;/pre>&lt;p>One of the advantages of the new form is that you may already be using it
in tandem with a static type checker such as mypy, thus getting additional
runtime type checking for free.&lt;/p>
&lt;p>This feature will be enabled by default, and there will be 2 mechanisms in
place to disable it:&lt;/p>
&lt;ol>
&lt;li>Calling &lt;code>apache_beam.typehints.disable_type_annotations()&lt;/code> before pipeline
construction will disable the new feature completely.&lt;/li>
&lt;li>Decorating a function with &lt;code>@apache_beam.typehints.no_annotations&lt;/code> will
tell Beam to ignore annotations for it.&lt;/li>
&lt;/ol>
&lt;p>Uses of Beam&amp;rsquo;s &lt;code>with_input_type&lt;/code>, &lt;code>with_output_type&lt;/code> methods and decorators will
still work and take precedence over annotations.&lt;/p>
&lt;h3 id="sidebar">Sidebar&lt;/h3>
&lt;p>You might ask: couldn&amp;rsquo;t we use mypy to type check Beam pipelines?
There are several reasons why this is not the case.&lt;/p>
&lt;ul>
&lt;li>Pipelines are constructed at runtime and may depend on information that is
only known at that time, such as a config file or database table schema.&lt;/li>
&lt;li>PCollections don&amp;rsquo;t have the necessary type information, so mypy sees them as
effectively containing any element type.
This may change in in the future.&lt;/li>
&lt;li>Transforms using lambdas (ex: &lt;code>beam.Map(lambda x: (1, x)&lt;/code>) cannot be
annotated properly using PEP 484.
However, Beam does a best-effort attempt to analyze the output type
from the bytecode.&lt;/li>
&lt;/ul>
&lt;h2 id="typing-module-support">Typing Module Support&lt;/h2>
&lt;p>Python&amp;rsquo;s &lt;a href="https://docs.python.org/3/library/typing.html">typing&lt;/a> module defines
types used in type annotations. This is what we call &amp;ldquo;native&amp;rdquo; types.
While Beam has its own typing types, it also supports native types.
While both Beam and native types are supported, for new code we encourage using
native typing types. Native types have as these are supported by additional tools.&lt;/p>
&lt;p>While working on Python 3 annotations syntax support, we&amp;rsquo;ve also discovered and
fixed issues with native type support. There may still be bugs and unsupported
native types. Please
&lt;a href="https://beam.apache.org/community/contact-us/">let us know&lt;/a> if you encounter
issues.&lt;/p></description><link>/blog/python-typing/</link><pubDate>Thu, 28 May 2020 00:00:01 -0800</pubDate><guid>/blog/python-typing/</guid><category>blog</category><category>python</category><category>typing</category></item><item><title>Apache Beam 2.21.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.21.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2210-2020-05-27">download page&lt;/a> for this release.
For more information on changes in 2.21.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12347143">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Python: Deprecated module &lt;code>apache_beam.io.gcp.datastore.v1&lt;/code> has been removed
as the client it uses is out of date and does not support Python 3
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9529">BEAM-9529&lt;/a>).
Please migrate your code to use
&lt;a href="https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.datastore.v1new.datastoreio.html">apache_beam.io.gcp.datastore.&lt;strong>v1new&lt;/strong>&lt;/a>.
See the updated
&lt;a href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/cookbook/datastore_wordcount.py">datastore_wordcount&lt;/a>
for example usage.&lt;/li>
&lt;li>Python SDK: Added integration tests and updated batch write functionality for Google Cloud Spanner transform (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8949">BEAM-8949&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Python SDK will now use Python 3 type annotations as pipeline type hints.
(&lt;a href="https://github.com/apache/beam/pull/10717">#10717&lt;/a>)&lt;/p>
&lt;p>If you suspect that this feature is causing your pipeline to fail, calling
&lt;code>apache_beam.typehints.disable_type_annotations()&lt;/code> before pipeline creation
will disable is completely, and decorating specific functions (such as
&lt;code>process()&lt;/code>) with &lt;code>@apache_beam.typehints.no_annotations&lt;/code> will disable it
for that function.&lt;/p>
&lt;p>More details can be found in
&lt;a href="https://beam.apache.org/documentation/sdks/python-type-safety/">Ensuring Python Type Safety&lt;/a>
and the Python SDK Typing Changes
&lt;a href="https://beam.apache.org/blog/python-typing/">blog post&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Java SDK: Introducing the concept of options in Beam Schema’s. These options add extra
context to fields and schemas. This replaces the current Beam metadata that is present
in a FieldType only, options are available in fields and row schemas. Schema options are
fully typed and can contain complex rows. &lt;em>Remark: Schema aware is still experimental.&lt;/em>
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9035">BEAM-9035&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Java SDK: The protobuf extension is fully schema aware and also includes protobuf option
conversion to beam schema options. &lt;em>Remark: Schema aware is still experimental.&lt;/em>
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9044">BEAM-9044&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added ability to write to BigQuery via Avro file loads (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8841">BEAM-8841&lt;/a>)&lt;/p>
&lt;p>By default, file loads will be done using JSON, but it is possible to
specify the temp_file_format parameter to perform file exports with AVRO.
AVRO-based file loads work by exporting Python types into Avro types, so
to switch to Avro-based loads, you will need to change your data types
from Json-compatible types (string-type dates and timestamp, long numeric
values as strings) into Python native types that are written to Avro
(Python&amp;rsquo;s date, datetime types, decimal, etc). For more information
see &lt;a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#avro_conversions&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added integration of Java SDK with Google Cloud AI VideoIntelligence service
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9147">BEAM-9147&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Added integration of Java SDK with Google Cloud AI natural language processing API
(&lt;a href="https://issues.apache.org/jira/browse/BEAM-9634">BEAM-9634&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>docker-pull-licenses&lt;/code> tag was introduced. Licenses/notices of third party dependencies will be added to the docker images when &lt;code>docker-pull-licenses&lt;/code> was set.
The files are added to &lt;code>/opt/apache/beam/third_party_licenses/&lt;/code>.
By default, no licenses/notices are added to the docker images. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9136">BEAM-9136&lt;/a>)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Dataflow runner now requires the &lt;code>--region&lt;/code> option to be set, unless a default value is set in the environment (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9199">BEAM-9199&lt;/a>). See &lt;a href="https://cloud.google.com/dataflow/docs/concepts/regional-endpoints">here&lt;/a> for more details.&lt;/li>
&lt;li>HBaseIO.ReadAll now requires a PCollection of HBaseIO.Read objects instead of HBaseQuery objects (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9279">BEAM-9279&lt;/a>).&lt;/li>
&lt;li>ProcessContext.updateWatermark has been removed in favor of using a WatermarkEstimator (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9430">BEAM-9430&lt;/a>).&lt;/li>
&lt;li>Coder inference for PCollection of Row objects has been disabled (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9569">BEAM-9569&lt;/a>).&lt;/li>
&lt;li>Go SDK docker images are no longer released until further notice.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Java SDK: Beam Schema FieldType.getMetadata is now deprecated and is replaced by the Beam
Schema Options, it will be removed in version &lt;code>2.23.0&lt;/code>. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9704">BEAM-9704&lt;/a>)&lt;/li>
&lt;li>The &lt;code>--zone&lt;/code> option in the Dataflow runner is now deprecated. Please use &lt;code>--worker_zone&lt;/code> instead. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-9716">BEAM-9716&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.21.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Aaron Meihm, Adrian Eka, Ahmet Altay, AldairCoronel, Alex Van Boxel, Alexey Romanenko, Andrew Crites, Andrew Pilloud, Ankur Goenka, Badrul (Taki) Chowdhury, Bartok Jozsef, Boyuan Zhang, Brian Hulette, brucearctor, bumblebee-coming, Chad Dombrova, Chamikara Jayalath, Chie Hayashida, Chris Gorgolewski, Chuck Yang, Colm O hEigeartaigh, Curtis &amp;ldquo;Fjord&amp;rdquo; Hawthorne, Daniel Mills, Daniel Oliveira, David Yan, Elias Djurfeldt, Emiliano Capoccia, Etienne Chauchot, Fernando Diaz, Filipe Regadas, Gleb Kanterov, Hai Lu, Hannah Jiang, Harch Vardhan, Heejong Lee, Henry Suryawirawan, Hk-tang, Ismaël Mejía, Jacoby, Jan Lukavský, Jeroen Van Goey, jfarr, Jozef Vilcek, Kai Jiang, Kamil Wasilewski, Kenneth Knowles, KevinGG, Kyle Weaver, Kyoungha Min, Luke Cwik, Maximilian Michels, Michal Walenia, Ning Kang, Pablo Estrada, paul fisher, Piotr Szuberski, Reuven Lax, Robert Bradshaw, Robert Burke, Rose Nguyen, Rui Wang, Sam Rohde, Sam Whittle, Spoorti Kundargi, Steve Koonce, sunjincheng121, Ted Yun, Tesio, Thomas Weise, Tomo Suzuki, Udi Meiri, Valentyn Tymofieiev, Vasu Nori, Yichi Zhang, yoshiki.obata, Yueyang Qiu&lt;/p></description><link>/blog/beam-2.21.0/</link><pubDate>Wed, 27 May 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.21.0/</guid><category>blog</category></item><item><title>Beam Summit Digital Is Coming - Register Now!</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>As some of you are already aware, the 2020 edition of the Beam Summit will be completely &lt;strong>digital and free&lt;/strong>. Beam Summit Digital will take place from &lt;strong>August 24th to 28th&lt;/strong>. The conference will be spread across the course of one week with a couple of hours of program each day.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beamsummit/beamsummit-digital-2020.png"
alt="Beam Summit Digital 2020, August 24-28">&lt;/p>
&lt;p>While we would have loved to see all of you in person, we have to accept that 2020 will not be the year for that. So, we are looking at this as an opportunity to have a bigger and more inclusive event, where people who would normally not be able to travel to the summit will now be able to join, learn and share with the rest of the community.&lt;/p>
&lt;h2 id="providing-you-the-best-experience-possible">Providing you the best experience possible&lt;/h2>
&lt;p>We are going to great lengths to ensure that we provide the Beam community with the best possible experience in an online event. From audio/video quality, to an adequate schedule for our community, to making it as easy as possible to register to the event and join the sessions, to setting up ways for the community to interact and network with each other. The team behind the organization of the Beam Summit has been working on these things, and we are also teaming up with an event production company with experience in online events who are bringing in their knowledge.&lt;/p>
&lt;p>So, what we want to say with this is: We will have a great event! And if you have any ideas on how to make it better, please let us know.&lt;/p>
&lt;h2 id="ways-to-participate-and-help">Ways to participate and help&lt;/h2>
&lt;p>As all things Beam, this is a community effort. The door is open for participation:&lt;/p>
&lt;ol>
&lt;li>Submit a proposal to talk. Please check out the &lt;strong>&lt;a href="https://sessionize.com/beam-digital-summit-2020/">Call for Papers&lt;/a>&lt;/strong> and submit a talk. The deadline for submissions is &lt;em>June 15th&lt;/em>!&lt;/li>
&lt;li>Register to join as an attendee. Registration is now open at the &lt;strong>&lt;a href="https://crowdcast.io/e/beamsummit">registration page&lt;/a>&lt;/strong>. Registration is free!&lt;/li>
&lt;li>Consider sponsoring the event. If your company is interested in engaging with members of the community please check out our &lt;a href="https://drive.google.com/open?id=1EbijvZKpkWwWyMryLY9sJfyZzZk1k44v">sponsoring prospectus&lt;/a>.&lt;/li>
&lt;li>Help us get the word out. Please make sure to let your colleagues and friends in the data engineering field (and beyond!) know about the Beam Summit.&lt;/li>
&lt;/ol>
&lt;h2 id="follow-up-and-more-information">Follow up and more information&lt;/h2>
&lt;p>While we will use the Crowdcast platform to broadcast the event, we will still have a full event website at &lt;a href="https://beamsummit.org">beamsummit.org&lt;/a> with details about the schedule, speakers, FAQ and everything else you need from an event. We are currently working on updating the website and will publish all event details in the next couple of weeks.&lt;/p>
&lt;p>Please also follow us on &lt;a href="https://twitter.com/beamsummit">Twitter&lt;/a> or &lt;a href="https://www.linkedin.com/company/beam-summit/">LinkedIn&lt;/a> to get event updates.&lt;/p></description><link>/blog/beam-summit-digital-2020/</link><pubDate>Fri, 08 May 2020 00:00:01 -0800</pubDate><guid>/blog/beam-summit-digital-2020/</guid><category>blog</category></item><item><title>Apache Beam 2.20.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.20.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2190-2020-02-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.20.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12346780">detailed release notes&lt;/a>.&lt;/p>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;p>Python SDK: . (#10223).&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8561">BEAM-8561&lt;/a> Adds support for Thrift encoded data via ThriftIO&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7310">BEAM-7310&lt;/a> KafkaIO supports schema resolution using Confluent Schema Registry&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-7246">BEAM-7246&lt;/a> Support for Google Cloud Spanner. This is an experimental module for reading and writing data from Google Cloud Spanner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8399">BEAM-8399&lt;/a> Adds support for standard HDFS URLs (with server name)&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9146">BEAM-9146&lt;/a> New AnnotateVideo &amp;amp; AnnotateVideoWithContext PTransform&amp;rsquo;s that integrates GCP Video Intelligence functionality&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9247">BEAM-9247&lt;/a> New AnnotateImage &amp;amp; AnnotateImageWithContext PTransform&amp;rsquo;s for element-wise &amp;amp; batch image annotation using Google Cloud Vision API&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9258">BEAM-9258&lt;/a> Added a PTransform for inspection and deidentification of text using Google Cloud DLP&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9248">BEAM-9248&lt;/a> New AnnotateText PTransform that integrates Google Cloud Natural Language functionality&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9305">BEAM-9305&lt;/a> ReadFromBigQuery now supports value providers for the query string&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8841">BEAM-8841&lt;/a> Added ability to write to BigQuery via Avro file loads&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9228">BEAM-9228&lt;/a> Direct runner for FnApi supports further parallelism&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8550">BEAM-8550&lt;/a> Support for @RequiresTimeSortedInput in Flink and Spark&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6857">BEAM-6857&lt;/a> Added support for dynamic timers&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-3453">BEAM-3453&lt;/a> Backwards incompatible change in ReadFromPubSub(topic=) in Python&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9310">BEAM-9310&lt;/a> SpannerAccessor in Java is now package-private to reduce API surface&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8616">BEAM-8616&lt;/a> ParquetIO hadoop dependency should be now provided by the users&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9063">BEAM-9063&lt;/a> Docker images will be deployed to apache/beam repositories from 2.20&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9579">BEAM-9579&lt;/a> Fixed numpy operators in ApproximateQuantiles&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9277">BEAM-9277&lt;/a> Fixed exception when running in IPython notebook&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-1833">BEAM-1833&lt;/a> Restructure Python pipeline construction to better follow the Runner API&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9225">BEAM-9225&lt;/a> Fixed Flink uberjar job termination bug&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9503">BEAM-9503&lt;/a> Fixed SyntaxError in process worker startup&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9322">BEAM-9322&lt;/a> Python SDK ignores manually set PCollection tags&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9445">BEAM-9445&lt;/a> Python SDK pre_optimize=all experiment may cause error&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9725">BEAM-9725&lt;/a> Python SDK performance regression for reshuffle transform&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.20.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Andrew Pilloud, Ankur Goenka, Anton Kedin, Boyuan Zhang, Brian Hulette, Brian Martin, Chamikara Jayalath
, Charles Chen, Craig Chambers, Daniel Oliveira, David Moravek, David Rieber, Dustin Rhodes, Etienne Chauchot, Gleb Kanterov, Hai Lu, Heejong Lee
, Ismaël Mejía, J Ross Thomson, Jan Lukavský, Jason Kuster, Jean-Baptiste Onofré, Jeff Klukas, João Cabrita, Juan Rael, Juta, Kasia Kucharczyk
, Kengo Seki, Kenneth Jung, Kenneth Knowles, Kyle Weaver, Kyle Winkelman, Lukas Drbal, Marek Simunek, Mark Liu, Maximilian Michels, Melissa Pashniak
, Michael Luckey, Michal Walenia, Mike Pedersen, Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Pascal Gula, Rehman Murad Ali, Reuven Lax, Rob, Robbe Sneyders
, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Williams, Sam Rohde, Sam Whittle, Scott Wegner, Shoaib Zafar, Thomas Weise, Tianyang Hu, Tyler Akidau
, Udi Meiri, Valentyn Tymofieiev, Xinyu Liu, XuMingmin, ttanay, tvalentyn, Łukasz Gajowy&lt;/p></description><link>/blog/beam-2.20.0/</link><pubDate>Wed, 15 Apr 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.20.0/</guid><category>blog</category></item><item><title>Apache Beam 2.19.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.19.0 release of Beam. This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2190-2020-02-04">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.19.0, check out the
&lt;a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12319527&amp;amp;version=12346582">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Multiple improvements made into Python SDK harness:
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8624">BEAM-8624&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8623">BEAM-8623&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-7949">BEAM-7949&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8935">BEAM-8935&lt;/a>,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-8816">BEAM-8816&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="ios">I/Os&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-1440">BEAM-1440&lt;/a> Create a BigQuery source (that implements iobase.BoundedSource) for Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-2572">BEAM-2572&lt;/a> Implement an S3 filesystem for Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5192">BEAM-5192&lt;/a> Support Elasticsearch 7.x&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8745">BEAM-8745&lt;/a> More fine-grained controls for the size of a BigQuery Load job&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8801">BEAM-8801&lt;/a> PubsubMessageToRow should not check useFlatSchema() in processElement&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8953">BEAM-8953&lt;/a> Extend ParquetIO.Read/ReadFiles.Builder to support Avro GenericData model&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8946">BEAM-8946&lt;/a> Report collection size from MongoDBIOIT&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8978">BEAM-8978&lt;/a> Report saved data size from HadoopFormatIOIT&lt;/li>
&lt;/ul>
&lt;h3 id="new-features--improvements">New Features / Improvements&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-6008">BEAM-6008&lt;/a> Improve error reporting in Java/Python PortableRunner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8296">BEAM-8296&lt;/a> Containerize the Spark job server&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8746">BEAM-8746&lt;/a> Allow the local job service to work from inside docker&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8837">BEAM-8837&lt;/a> PCollectionVisualizationTest: possible bug&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8139">BEAM-8139&lt;/a> Execute portable Spark application jar&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9019">BEAM-9019&lt;/a> Improve Spark Encoders (wrappers of beam coders)&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9053">BEAM-9053&lt;/a> Improve error message when unable to get the correct filesystem for specified path in Python SDK) Improve error message when unable to get the correct filesystem for specified path in Python SDK&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9055">BEAM-9055&lt;/a> Unify the config names of Fn Data API across languages&lt;/li>
&lt;/ul>
&lt;h3 id="sql">SQL&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5690">BEAM-5690&lt;/a> Issue with GroupByKey in BeamSql using SparkRunner&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8993">BEAM-8993&lt;/a> [SQL] MongoDb should use predicate push-down&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8844">BEAM-8844&lt;/a> [SQL] Create performance tests for BigQueryTable&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9023">BEAM-9023&lt;/a> Upgrade to ZetaSQL 2019.12.1&lt;/li>
&lt;/ul>
&lt;h3 id="breaking-changes">Breaking Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8989">BEAM-8989&lt;/a> Backwards incompatible change in ParDo.getSideInputs (caught by failure when running Apache Nemo quickstart)&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8402">BEAM-8402&lt;/a> Backwards incompatible change related to how Environments are represented in Python &lt;code>DirectRunner&lt;/code>.&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9218">BEAM-9218&lt;/a> Template staging broken on Beam 2.18.0&lt;/li>
&lt;/ul>
&lt;h3 id="dependency-changes">Dependency Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8696">BEAM-8696&lt;/a> Beam Dependency Update Request: com.google.protobuf:protobuf-java&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8701">BEAM-8701&lt;/a> Beam Dependency Update Request: commons-io:commons-io&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8716">BEAM-8716&lt;/a> Beam Dependency Update Request: org.apache.commons:commons-csv&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8717">BEAM-8717&lt;/a> Beam Dependency Update Request: org.apache.commons:commons-lang3&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8749">BEAM-8749&lt;/a> Beam Dependency Update Request: com.datastax.cassandra:cassandra-driver-mapping&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5546">BEAM-5546&lt;/a> Beam Dependency Update Request: commons-codec:commons-codec&lt;/li>
&lt;/ul>
&lt;h3 id="bugfixes">Bugfixes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9123">BEAM-9123&lt;/a> HadoopResourceId returns wrong directory name&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8962">BEAM-8962&lt;/a> FlinkMetricContainer causes churn in the JobManager and lets the web frontend malfunction&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-5495">BEAM-5495&lt;/a> PipelineResources algorithm is not working in most environments&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8025">BEAM-8025&lt;/a> Cassandra IO classMethod test is flaky&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8577">BEAM-8577&lt;/a> FileSystems may have not be initialized during ResourceId deserialization&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8582">BEAM-8582&lt;/a> Python SDK emits duplicate records for Default and AfterWatermark triggers&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8943">BEAM-8943&lt;/a> SDK harness servers don&amp;rsquo;t shut down properly when SDK harness environment cleanup fails&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8995">BEAM-8995&lt;/a> apache_beam.io.gcp.bigquery_read_it_test failing on Py3.5 PC with: TypeError: the JSON object must be str, not &amp;lsquo;bytes&amp;rsquo;&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-8999">BEAM-8999&lt;/a> PGBKCVOperation does not respect timestamp combiners&lt;/li>
&lt;li>&lt;a href="https://issues.apache.org/jira/browse/BEAM-9050">BEAM-9050&lt;/a> Beam pickler doesn&amp;rsquo;t pickle classes that have &lt;strong>module&lt;/strong> set to None.&lt;/li>
&lt;li>&lt;/li>
&lt;li>Various bug fixes and performance improvements.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.19.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmet Altay, Alex Amato, Alexey Romanenko, Andrew Pilloud, Ankur Goenka, Anton Kedin, Boyuan Zhang, Brian Hulette, Brian Martin, Chamikara Jayalath, Charles Chen, Craig Chambers, Daniel Oliveira, David Moravek, David Rieber, Dustin Rhodes, Etienne Chauchot, Gleb Kanterov, Hai Lu, Heejong Lee, Ismaël Mejía, Jan Lukavský, Jason Kuster, Jean-Baptiste Onofré, Jeff Klukas, João Cabrita, J Ross Thomson, Juan Rael, Juta, Kasia Kucharczyk, Kengo Seki, Kenneth Jung, Kenneth Knowles, Kyle Weaver, Kyle Winkelman, Lukas Drbal, Łukasz Gajowy, Marek Simunek, Mark Liu, Maximilian Michels, Melissa Pashniak, Michael Luckey, Michal Walenia, Mike Pedersen, Mikhail Gryzykhin, Niel Markwick, Pablo Estrada, Pascal Gula, Reuven Lax, Rob, Robbe Sneyders, Robert Bradshaw, Robert Burke, Rui Wang, Ruoyun Huang, Ryan Williams, Sam Rohde, Sam Whittle, Scott Wegner, Thomas Weise, Tianyang Hu, ttanay, tvalentyn, Tyler Akidau, Udi Meiri, Valentyn Tymofieiev, Xinyu Liu, XuMingmin&lt;/p></description><link>/blog/beam-2.19.0/</link><pubDate>Tue, 04 Feb 2020 00:00:01 -0800</pubDate><guid>/blog/beam-2.19.0/</guid><category>blog</category></item></channel></rss>