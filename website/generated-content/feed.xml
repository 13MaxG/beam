<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Apache Beam</title><description>Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes.</description><link>/</link><generator>Hugo -- gohugo.io</generator><item><title>Apache Beam 2.43.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.43.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2430-2022-11-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.43.0, check out the &lt;a href="https://github.com/apache/beam/milestone/5?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Python 3.10 support in Apache Beam (&lt;a href="https://github.com/apache/beam/issues/21458">#21458&lt;/a>).&lt;/li>
&lt;li>An initial implementation of a runner that allows us to run Beam pipelines on Dask. Try it out and give us feedback! (Python) (&lt;a href="https://github.com/apache/beam/issues/18962">#18962&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Decreased TextSource CPU utilization by 2.3x (Java) (&lt;a href="https://github.com/apache/beam/issues/23193">#23193&lt;/a>).&lt;/li>
&lt;li>Fixed bug when using SpannerIO with RuntimeValueProvider options (Java) (&lt;a href="https://github.com/apache/beam/issues/22146">#22146&lt;/a>).&lt;/li>
&lt;li>Fixed issue for unicode rendering on WriteToBigQuery (&lt;a href="https://github.com/apache/beam/issues/10785">#10785&lt;/a>)&lt;/li>
&lt;li>Remove obsolete variants of BigQuery Read and Write, always using Beam-native variant
(&lt;a href="https://github.com/apache/beam/issues/23564">#23564&lt;/a> and &lt;a href="https://github.com/apache/beam/issues/23559">#23559&lt;/a>).&lt;/li>
&lt;li>Bumped google-cloud-spanner dependency version to 3.x for Python SDK (&lt;a href="https://github.com/apache/beam/issues/21198">#21198&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Dataframe wrapper added in Go SDK via Cross-Language (with automatic expansion service). (Go) (&lt;a href="https://github.com/apache/beam/issues/23384">#23384&lt;/a>).&lt;/li>
&lt;li>Name all Java threads to aid in debugging (&lt;a href="https://github.com/apache/beam/issues/23049">#23049&lt;/a>).&lt;/li>
&lt;li>An initial implementation of a runner that allows us to run Beam pipelines on Dask. (Python) (&lt;a href="https://github.com/apache/beam/issues/18962">#18962&lt;/a>).&lt;/li>
&lt;li>Allow configuring GCP OAuth scopes via pipeline options. This unblocks usages of Beam IOs that require additional scopes.
For example, this feature makes it possible to access Google Drive backed tables in BigQuery (&lt;a href="https://github.com/apache/beam/issues/23290">#23290&lt;/a>).&lt;/li>
&lt;li>An example for using Python RunInference from Java (&lt;a href="https://github.com/apache/beam/pull/23619">#23290&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>CoGroupByKey transform in Python SDK has changed the output typehint. The typehint component representing grouped values changed from List to Iterable,
which more accurately reflects the nature of the arbitrarily large output collection. &lt;a href="https://github.com/apache/beam/issues/21556">#21556&lt;/a> Beam users may see an error on transforms downstream from CoGroupByKey. Users must change methods expecting a List to expect an Iterable going forward. See &lt;a href="https://docs.google.com/document/d/1RIzm8-g-0CyVsPb6yasjwokJQFoKHG4NjRUcKHKINu0">document&lt;/a> for information and fixes.&lt;/li>
&lt;li>The PortableRunner for Spark assumes Spark 3 as default Spark major version unless configured otherwise using &lt;code>--spark_version&lt;/code>.
Spark 2 support is deprecated and will be removed soon (&lt;a href="https://github.com/apache/beam/issues/23728">#23728&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Numeric/Decimal type values (&lt;a href="https://github.com/apache/beam/issues/19817">#19817&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.43.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
AlexZMLyu
Alexey Romanenko
Anand Inguva
Andrew Pilloud
Andy Ye
Arnout Engelen
Benjamin Gonzalez
Bharath Kumarasubramanian
BjornPrime
Brian Hulette
Bruno Volpato
Chamikara Jayalath
Colin Versteeg
Damon
Daniel Smilkov
Daniela Martín
Danny McCormick
Darkhan Nausharipov
David Huntsperger
Denis Pyshev
Dmitry Repin
Evan Galpin
Evgeny Antyshev
Fernando Morales
Geddy05
Harshit Mehrotra
Iñigo San Jose Visiers
Ismaël Mejía
Israel Herraiz
Jan Lukavský
Juta Staes
Kanishk Karanawat
Kenneth Knowles
KevinGG
Kiley Sok
Liam Miller-Cushon
Luke Cwik
Mc
Melissa Pashniak
Moritz Mack
Ning Kang
Pablo Estrada
Philippe Moussalli
Pranav Bhandari
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Ryohei Nagao
Sam Rohde
Sam Whittle
Sanil Jain
Seunghwan Hong
Shane Hansen
Shubham Krishna
Shunsuke Otani
Steve Niemitz
Steven van Rossum
Svetak Sundhar
Thiago Nunes
Toran Sahu
Veronica Wasson
Vitaly Terentyev
Vladislav Chunikhin
Xinyu Liu
Yi Hu
Yixiao Shen
alexeyinkin
arne-alex
azhurkevich
bulat safiullin
bullet03
coldWater
dpcollins-google
egalpin
johnjcasey
liferoad
rvballada
shaojwu
tvalentyn&lt;/p></description><link>/blog/beam-2.43.0/</link><pubDate>Thu, 17 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.43.0/</guid><category>blog</category><category>release</category></item><item><title>New Resources Available for Beam ML</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>If you&amp;rsquo;ve been paying attention, over the past year you&amp;rsquo;ve noticed that
Beam has released a number of features designed to make Machine Learning
easy. Ranging from things like the introduction of the &lt;code>RunInference&lt;/code>
transform to the continued refining of &lt;code>Beam Dataframes&lt;/code>, this has been
an area where we&amp;rsquo;ve seen Beam make huge strides. While development has
advanced quickly, however, until recently there has been a lack of
resources to help people discover and use these new features.&lt;/p>
&lt;p>Over the past several months, we&amp;rsquo;ve been hard at work building out
documentation and notebooks to make it easier to use these new features
and to show how Beam can be used to solve common Machine Learning problems.
We&amp;rsquo;re now happy to present this new and improved Beam ML experience!&lt;/p>
&lt;p>To get started, we encourage you to visit Beam&amp;rsquo;s new &lt;a href="https://beam.apache.org/documentation/ml/overview/">AI/ML landing page&lt;/a>.
We&amp;rsquo;ve got plenty of content on things like &lt;a href="https://beam.apache.org/documentation/ml/multi-model-pipelines/">multi-model pipelines&lt;/a>,
&lt;a href="https://beam.apache.org/documentation/ml/runinference-metrics/">performing inference with metrics&lt;/a>,
&lt;a href="https://beam.apache.org/documentation/ml/online-clustering/">online training&lt;/a>, and much more.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/ml-landing.png"
alt="ML landing page">&lt;/p>
&lt;p>We&amp;rsquo;ve also introduced a number of example &lt;a href="https://github.com/apache/beam/tree/master/examples/notebooks/beam-ml">Jupyter Notebooks&lt;/a>
showing how to use built in beam transforms like &lt;code>RunInference&lt;/code> and &lt;code>Beam Dataframes&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/ensemble-model-notebook.png"
alt="Example ensemble notebook with RunInference">&lt;/p>
&lt;p>Adding more examples and notebooks will be a point of emphasis going forward.
For our next round of improvements, we are planning on adding examples of
using RunInference with &amp;gt;30GB models, with multi-language pipelines, with
common Beam concepts, and with TensorRT. We will also add examples showing
other pieces of the Machine Learning lifecycle like model evaluation with TFMA,
per-entity training, and more online training.&lt;/p>
&lt;p>We hope you find this useful! As always, if you see any areas for improvement, please &lt;a href="https://github.com/apache/beam/issues/new/choose">open an issue&lt;/a>
or a &lt;a href="https://github.com/apache/beam/pulls">pull request&lt;/a>!&lt;/p></description><link>/blog/ml-resources/</link><pubDate>Wed, 09 Nov 2022 00:00:01 -0800</pubDate><guid>/blog/ml-resources/</guid><category>blog</category><category>python</category></item><item><title>Beam starter projects</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We&amp;rsquo;re happy to announce that we&amp;rsquo;re providing new Beam starter projects! 🎉&lt;/p>
&lt;p>Setting up and configuring a new project can be time consuming, and varies in different languages. We hope this will make it easier for you to get started in creating new Apache Beam projects and pipelines.&lt;/p>
&lt;p>All the starter projects come in their own GitHub repository, so you can simply clone a repo and you&amp;rsquo;re ready to go. Each project comes with a README with how to use it, a simple &amp;ldquo;Hello World&amp;rdquo; pipeline, and a test for the pipeline. The GitHub repositories come pre-configured with GitHub Actions to automatically run tests when pull requests are opened or modified, and Dependabot is enabled to make sure all the dependencies are up to date. This all comes out of the box, so you can start playing with your Beam pipeline without a hassle.&lt;/p>
&lt;p>For example, here&amp;rsquo;s how to get started with Java:&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/apache/beam-starter-java
cd beam-starter-java
# Install Java and Gradle with sdkman.
curl -s &amp;quot;https://get.sdkman.io&amp;quot; | bash
sdk install java 11.0.12-tem
sdk install gradle
# To run the pipeline.
gradle run
# To run the tests.
gradle test
&lt;/code>&lt;/pre>&lt;p>And here&amp;rsquo;s how to get started with Python:&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/apache/beam-starter-python
cd beam-starter-python
# Set up a virtual environment with the dependencies.
python -m venv env
source env/bin/activate
pip install -r requirements.txt
# To run the pipeline.
python main.py
# To run the tests.
python -m unittest
&lt;/code>&lt;/pre>&lt;p>Here are the starter projects; you can choose your favorite language:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>[Java]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-java">github.com/apache/beam-starter-java&lt;/a> – Includes both Gradle and Maven configurations.&lt;/li>
&lt;li>&lt;strong>[Python]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-python">github.com/apache/beam-starter-python&lt;/a> – Includes a setup.py file to allow multiple files in your pipeline.&lt;/li>
&lt;li>&lt;strong>[Go]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-go">github.com/apache/beam-starter-go&lt;/a> – Includes how to register different types of functions for ParDo.&lt;/li>
&lt;li>&lt;strong>[Kotlin]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-kotlin">github.com/apache/beam-starter-kotlin&lt;/a> – Adapted to idiomatic Kotlin&lt;/li>
&lt;li>&lt;strong>[Scala]&lt;/strong> &lt;a href="https://github.com/apache/beam-starter-scala">github.com/apache/beam-starter-scala&lt;/a> – Coming soon!&lt;/li>
&lt;/ul>
&lt;p>We have updated the &lt;a href="https://beam.apache.org/get-started/quickstart/java/">Java quickstart&lt;/a> to use the new starter project, and we&amp;rsquo;re working on updating the Python and Go quickstarts as well.&lt;/p>
&lt;p>We hope you find this useful. Feedback and contributions are always welcome! So feel free to create a GitHub issue, or open a Pull Request to any of the starter project repositories.&lt;/p></description><link>/blog/beam-starter-projects/</link><pubDate>Thu, 03 Nov 2022 09:00:00 -0700</pubDate><guid>/blog/beam-starter-projects/</guid><category>blog</category></item><item><title>Apache Beam 2.42.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.42.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2420-2022-10-17">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.42.0, check out the &lt;a href="https://github.com/apache/beam/milestone/4?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;li>Added support for &lt;a href="https://beam.apache.org/documentation/programming-guide/#batched-dofns">Batched
DoFns&lt;/a>
to the Python SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Added support for Zstd compression to the Python SDK.&lt;/li>
&lt;li>Added support for Google Cloud Profiler to the Go SDK.&lt;/li>
&lt;li>Added support for stateful DoFns to the Go SDK.&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go SDK&amp;rsquo;s Row Coder now uses a different single-precision float encoding for float32 types to match Java&amp;rsquo;s behavior (&lt;a href="https://github.com/apache/beam/issues/22629">#22629&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Python cross-language JDBC IO Connector cannot read or write rows containing Timestamp type values &lt;a href="https://github.com/apache/beam/issues/19817">19817&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>Go SDK doesn&amp;rsquo;t yet support Slowly Changing Side Input pattern (&lt;a href="https://github.com/apache/beam/issues/23106">#23106&lt;/a>)&lt;/li>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/4">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.42.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Abirdcfly
Ahmed Abualsaud
Alexander Zhuravlev
Alexey Inkin
Alexey Romanenko
Anand Inguva
Andrej Galad
Andrew Pilloud
Andy Ye
Balázs Németh
Brian Hulette
Bruno Volpato
bulat safiullin
bullet03
Chamikara Jayalath
ChangyuLi28
Clément Guillaume
Damon
Danny McCormick
Darkhan Nausharipov
David Huntsperger
dpcollins-google
Evgeny Antyshev
grufino
Heejong Lee
Ismaël Mejía
Jack McCluskey
johnjcasey
Jonathan Shen
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
liferoad
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manit Gupta
masahitojp
Michael Hu
Michel Davit
Moritz Mack
Naireen Hussain
nancyxu123
Nikhil Nadig
oborysevych
Pablo Estrada
Pranav Bhandari
Rajat Bhatta
Rebecca Szper
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Sergey Pronin
Shivam
Shunsuke Otani
Shunya Ueta
Steven Niemitz
Stuart
Svetak Sundhar
Valentyn Tymofieiev
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yixiao Shen&lt;/p></description><link>/blog/beam-2.42.0/</link><pubDate>Mon, 17 Oct 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.42.0/</guid><category>blog</category><category>release</category></item><item><title>Apache Hop web version with Cloud Dataflow</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>Hop is a codeless visual development environment for Apache Beam pipelines that
can run jobs in any Beam runner, such as Dataflow, Flink or Spark. &lt;a href="https://beam.apache.org/blog/apache-hop-with-dataflow/">In a
previous post&lt;/a>, we
introduced the desktop version of Apache Hop. Hop also has a web environment,
Hop Web, that you can run from a container, so you don&amp;rsquo;t have to install
anything on your computer to use it.&lt;/p>
&lt;p>In this detailed tutorial, you access Hop through the internet using a web
browser and point to a container running in a virtual machine on Google
Cloud. That container will launch jobs in Dataflow and report back the results
of those jobs. Because we don&amp;rsquo;t want just anyone to access your Hop instance,
we’re going to secure it so that only you can access that virtual machine. The
following diagram illustrates the setup:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image2.png" alt="Architecture deployed with this tutorial">&lt;/p>
&lt;p>We will show how to do the deployment described previously, creating a web and
visual development environment that builds Beam pipelines using just a web
browser. When complete, you will have a secure web environment that you can use
to create pipelines with your web browser and launch them using Google Cloud
Dataflow.&lt;/p>
&lt;h2 id="what-do-you-need-to-run-this-example">What do you need to run this example?&lt;/h2>
&lt;p>We are using Google Cloud, so the first thing you need is a Google Cloud
project. If needed, you can sign up for the free trial of Google Cloud at
&lt;a href="https://cloud.google.com/free">https://cloud.google.com/free&lt;/a>.&lt;/p>
&lt;p>When you have a project, you can use &lt;a href="https://cloud.google.com/shell">Cloud
Shell&lt;/a> in your web browser with no additional
setup. In Cloud Shell, the Google Cloud SDK is automatically configured for your
project and credentials. That&amp;rsquo;s the option we use here. Alternatively, you can
configure the Google Cloud SDK in your local computer. For instructions, see
&lt;a href="https://cloud.google.com/sdk/docs/install">https://cloud.google.com/sdk/docs/install&lt;/a>.&lt;/p>
&lt;p>To open Cloud Shell, go to the [Google Cloud console]
(&lt;a href="http://console.cloud.google.com">http://console.cloud.google.com&lt;/a>), make sure your project is selected, and click
the Cloud Shell button &lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image1.png" alt="Cloud Shellbutton">. Cloud Shell opens,
and you can use it to run the commands shown in this post.&lt;/p>
&lt;p>The commands that we are going to use in the next steps are &lt;a href="https://gist.github.com/iht/6219b227424ada477462c7b9d9d93c57">available in a Gist
in Github&lt;/a>, just
in case you prefer to run that script instead of copying the commands from this
tutorial.&lt;/p>
&lt;h2 id="permissions-and-accounts">Permissions and accounts&lt;/h2>
&lt;p>When we run a Dataflow pipeline, we can use our personal Google Cloud
credentials to run the job. But Hop web will be running in a virtual machine,
and in Google Cloud, virtual machines run using service accounts as
credentials. So we need to make sure that we have a service account that has
permission to run Dataflow jobs.&lt;/p>
&lt;p>By default, virtual machines use the service account called &lt;em>Compute Engine
default service account&lt;/em>. For the sake of simplicity, we will use this
account. Still, we need to add some permissions to run Dataflow jobs with that
service account.&lt;/p>
&lt;p>First, let&amp;rsquo;s make sure that you have enabled all the required Google Cloud
APIs. &lt;a href="https://console.cloud.google.com/flows/enableapi?apiid=dataflow,compute_component,logging,storage_component,storage_api,bigquery,pubsub">Click this link to enable Dataflow, BigQuery and
Pub/Sub&lt;/a>,
which we’ll use in this workflow. The link takes you to your project in the
Google Cloud console, where you can enable the APIs.&lt;/p>
&lt;p>Let&amp;rsquo;s now give permissions to the VM account. First, find the ID of the service
account. Open Cloud Shell, and run the following command.&lt;/p>
&lt;pre>&lt;code>gcloud iam service-accounts list | grep compute
&lt;/code>&lt;/pre>&lt;p>The output is similar to the following, with &lt;code>&amp;lt;PROJECT_NUMBER&amp;gt;&lt;/code> replaced by your
project number:&lt;/p>
&lt;pre>&lt;code>EMAIL: &amp;lt;PROJECT_NUMBER&amp;gt;-compute@developer.gserviceaccount.com
&lt;/code>&lt;/pre>&lt;p>Copy that service account ID, because we use it in the next step. Run the
following command to grant the &lt;a href="https://cloud.google.com/dataflow/docs/concepts/access-control">Dataflow Admin
role&lt;/a> to the
service account. This role is required to run jobs:&lt;/p>
&lt;pre>&lt;code>gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member=&amp;quot;serviceAccount:&amp;lt;SERVICE_ACCOUNT_ID&amp;gt;&amp;quot; --role=&amp;quot;roles/dataflow.admin&amp;quot;
&lt;/code>&lt;/pre>&lt;p>where &lt;code>&amp;lt;SERVICE_ACCOUNT_ID&amp;gt;&lt;/code> is the ID that you retrieved previously. If you are
running these commands in Cloud Shell, the environment variable
&lt;code>GOOGLE_CLOUD_PROJECT&lt;/code> is already set to your project ID. If you are running
this from any other place, set the &lt;code>$GOOGLE_CLOUD_PROJECT&lt;/code> variable with the ID
of your project.&lt;/p>
&lt;p>Now your &amp;ldquo;user&amp;rdquo; for Dataflow is that service account. If your jobs are accessing
data in BigQuery, Cloud Storage, Pub/Sub, and so on, you also need to grant
roles for those services to the service account.&lt;/p>
&lt;h2 id="disk-and-virtual-machine">Disk and virtual machine&lt;/h2>
&lt;p>Let&amp;rsquo;s create a virtual machine (VM) in Compute Engine to run the Docker
container of Apache Hop.&lt;/p>
&lt;p>In Compute Engine, it is possible to run a container directly in a VM. There are
other options to run containers in Google Cloud, but a VM is probably the
simplest and most straightforward. The full details are in the &lt;a href="https://cloud.google.com/compute/docs/containers/deploying-containers">Deploying
containers on VMs and
MIGs&lt;/a>
page of the Google Cloud documentation.&lt;/p>
&lt;p>In this tutorial, we will always be working in the zone &lt;code>europe-west1-b&lt;/code>, so you
will see that zone in a lot of the commands. However, you can choose any Google
Cloud zone; just remember to use the value for your zone instead of
&lt;code>europe-west1-b&lt;/code>. Always use the same zone for all the resources, such as disks
and VMs. To minimize the latency when using Hop web, choose a zone that is
geographically close to your location. Let&amp;rsquo;s define the zone now and use this
variable for the rest of the commands:&lt;/p>
&lt;pre>&lt;code>ZONE=europe-west1-b
&lt;/code>&lt;/pre>&lt;p>Containers have ephemeral storage: when you restart the container, the disk of
the container returns to its original state. Therefore, if we restart the Hop
web container, we lose all our precious pipelines. To avoid that, we are going
to create a persistent disk, where we will store all our work with Hop web. Run
the following command to create the disk:&lt;/p>
&lt;pre>&lt;code>gcloud compute disks create my-hop-disk \
--type=pd-balanced \
--size=10GB \
--zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>Thanks to this disk, we’re able to stop the virtual machine and still keep all
our personal files in Hop web intact.&lt;/p>
&lt;p>Let&amp;rsquo;s now create the VM. For the VM, we need to select the network (&lt;code>default&lt;/code> in
the, well, default case) so the VM will not have a public IP address. This is
important for security reasons, but it won’t stop us from using the VM from our
web browser thanks to the Identity Aware Proxy. More on this later; for now
let&amp;rsquo;s create the VM:&lt;/p>
&lt;pre>&lt;code>gcloud compute instances create-with-container my-hop-vm \
--zone=$ZONE \
--network-interface=subnet=default,no-address \
--scopes=https://www.googleapis.com/auth/cloud-platform \
--tags=http-server,https-server,ssh \
--container-image=apache/hop-web:2.0.1 \
--container-restart-policy=on-failure \
--container-mount-disk=mode=rw,mount-path=/root,name=my-hop-disk,partition=0 \
--disk=boot=no,device-name=my-hop-disk,mode=rw,name=my-hop-disk
&lt;/code>&lt;/pre>&lt;p>You might be wondering what those additional options are. They are required for
the VM to work properly with Hop web. For instance, the &lt;code>scopes&lt;/code> option is what
allows the VM to use Dataflow, and the &lt;code>tags&lt;/code> option lets your browser reach the
Hop web address through the network firewall.&lt;/p>
&lt;p>Apache Hop listens on port 8080 for HTTP connections, so if you have additional
custom firewall rules in your project, make sure you are not stopping TCP
traffic on port 8080.&lt;/p>
&lt;p>But wait a minute; we have created a machine with only private IPs. How can we
reach Hop web from the web browser on our computer? Don&amp;rsquo;t we need a public IP
address for that?&lt;/p>
&lt;p>Google Cloud has a feature called the Identity Aware Proxy (IAP) that can be
used to wrap services with an authorization layer, allowing connections to
resources with only internal IPs.&lt;/p>
&lt;p>We can use the IAP to wrap our Apache Hop web server. With the following
command, we create a tunnel listening on local port 8080 that connects to port
8080 on the VM:&lt;/p>
&lt;pre>&lt;code>gcloud compute start-iap-tunnel my-hop-vm 8080 --local-host-port=localhost:8080 --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>To keep the tunnel open, leave that command running. If the command fails right
after creating the VM, wait a few seconds and try again; the container might
still be booting up.&lt;/p>
&lt;p>We now have a tunnel that we can connect to using our web browser. If you’re
running these commands on your local computer and not in Cloud Shell, point your
browser to &lt;code>localhost:8080&lt;/code>. The Hop UI should load.&lt;/p>
&lt;p>If you are running these command in Cloud Shell, where do we point the browser
to? Cloud Shell comes with an utility for situations like this one. In Cloud
Shell, locate the &lt;strong>Web Preview&lt;/strong> button:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image3.png" alt="Web preview options">&lt;/p>
&lt;p>If the preview isn’t using port 8080, click &lt;strong>Change port&lt;/strong>, and switch to
port 8080. When you click &lt;strong>Preview on port&lt;/strong>, Cloud Shell opens a new tab in
your browser that points to the tunnel address.&lt;/p>
&lt;p>The &lt;strong>Identity Aware Proxy&lt;/strong> will ask you to identify yourself using your Google
account.&lt;/p>
&lt;p>After that, the Apache Hop web interface loads:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image4.png" alt="Hop web UI">&lt;/p>
&lt;p>That URL is authenticated using your Google account, the same one that you are
using for Google Cloud (the one you are authenticated with in the Google Cloud
SDK). So even if another person gets that URL address, they won’t be able to
access your Apache Hop instance.&lt;/p>
&lt;p>You are now ready to use Apache Hop in a web browser!&lt;/p>
&lt;p>You can try to replicate the example that was given &lt;a href="https://beam.apache.org/blog/apache-hop-with-dataflow/">in a previous
post&lt;/a> using Hop web, or
just try to launch any other project from the samples included with Hop:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image5.png" alt="Sample projects in Hop">&lt;/p>
&lt;h2 id="where-should-i-store-my-stuff">Where should I store my stuff?&lt;/h2>
&lt;p>The directories in the file system of a container are ephemeral. How can you be
sure that you store your pipelines and JARs in a persistent location?&lt;/p>
&lt;p>The home directory container is &lt;code>/root&lt;/code>, and it is the only &lt;strong>persistent&lt;/strong>
directory in the container (thanks to the disk we created previously). When you
restart the VM for whatever reason, any file included in that directory is
retained. But the rest of the directories reset to their original state. So make
sure you save your stuff, such as your pipelines, the fat JAR generated for
Dataflow, and so on, in the &lt;code>/root&lt;/code> directory or its subdirectories.&lt;/p>
&lt;p>In the Hop file dialogs, when you click the home icon, you are directed to the
&lt;code>/root&lt;/code> directory, so it is very straightforward to use it to store
everything. In the example in the picture, we clicked the &lt;strong>Home&lt;/strong> button and
are storing a JAR in that persistent directory:&lt;/p>
&lt;p>&lt;img src="/images/blog/hop-web-cloud/hop-web-cloud-image6.png" alt="Hop file dialog">&lt;/p>
&lt;h2 id="turning-off-the-virtual-machine">Turning off the virtual machine&lt;/h2>
&lt;p>If you want to save some money when you are not using the virtual machine, stop
the VM and launch it again when needed. The content of the &lt;em>/root&lt;/em> directory is
saved when you stop the virtual machine.&lt;/p>
&lt;p>To stop the VM, run the following command (or in the console, on the Compute
Engine VM page, click &lt;strong>Stop&lt;/strong>):&lt;/p>
&lt;pre>&lt;code>gcloud compute instances stop my-hop-vm --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>And to start it again, run the following command:&lt;/p>
&lt;pre>&lt;code>gcloud compute instances start my-hop-vm --zone=$ZONE
&lt;/code>&lt;/pre>&lt;p>Remember that you need to have the Identity Aware Proxy running in order to
access Hop web, so after starting the VM, don&amp;rsquo;t forget to run the command to
start the Identity Aware Proxy (and if it fails right after starting, wait a few
seconds and run it again):&lt;/p>
&lt;pre>&lt;code>gcloud compute start-iap-tunnel my-hop-vm 8080 --local-host-port=localhost:8080 --zone=$ZONE
&lt;/code>&lt;/pre>&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>This post has shown that all that you need to run Hop is a web browser. And,
well, a Google Cloud project too.&lt;/p>
&lt;p>We deployed the container to a virtual machine in Google Cloud, so you can
access Hop from anywhere, and we created a persistent disk, so you can have
permanent storage for your pipelines. Now you can use your web browser to create
your pipelines and to run Dataflow jobs without having to install anything
locally in your computer: not Java, not Docker, not the Google Cloud SDK;
nothing, just your favourite web browser.&lt;/p>
&lt;p>If you followed the instructions in this post, head over to the post &lt;a href="https://beam.apache.org/blog/apache-hop-with-dataflow/">Running
Apache Hop visual pipelines with Google Cloud
Dataflow&lt;/a> to run a
Dataflow pipeline right from your web browser!&lt;/p></description><link>/blog/hop-web-cloud/</link><pubDate>Sat, 15 Oct 2022 00:00:01 -0800</pubDate><guid>/blog/hop-web-cloud/</guid><category>blog</category></item><item><title>Apache Beam 2.41.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.41.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2410-2022-08-23">download page&lt;/a> for this release.&lt;/p>
&lt;p>For more information on changes in 2.41.0, check out the &lt;a href="https://github.com/apache/beam/milestone/3?closed=1">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Projection Pushdown optimizer is now on by default for streaming, matching the behavior of batch pipelines since 2.38.0. If you encounter a bug with the optimizer, please file an issue and disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Previously available in Java sdk, Python sdk now also supports logging level overrides per module. (&lt;a href="https://github.com/apache/beam/issues/18222">#18222&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Projection Pushdown optimizer may break Dataflow upgrade compatibility for optimized pipelines when it removes unused fields. If you need to upgrade and encounter a compatibility issue, disable the optimizer using pipeline option &lt;code>--experiments=disable_projection_pushdown&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Support for Spark 2.4.x is deprecated and will be dropped with the release of Beam 2.44.0 or soon after (Spark runner) (&lt;a href="https://github.com/apache/beam/issues/22094">#22094&lt;/a>).&lt;/li>
&lt;li>The modules &lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services">amazon-web-services&lt;/a> and
&lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/kinesis">kinesis&lt;/a> for AWS Java SDK v1 are deprecated
in favor of &lt;a href="https://github.com/apache/beam/tree/master/sdks/java/io/amazon-web-services2">amazon-web-services2&lt;/a>
and will be eventually removed after a few Beam releases (Java) (&lt;a href="https://github.com/apache/beam/issues/21249">#21249&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed a condition where retrying queries would yield an incorrect cursor in the Java SDK Firestore Connector (&lt;a href="https://github.com/apache/beam/issues/22089">#22089&lt;/a>).&lt;/li>
&lt;li>Fixed plumbing allowed lateness in Go SDK. It was ignoring the user set value earlier and always used to set to 0. (&lt;a href="https://github.com/apache/beam/issues/22474">#22474&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h3 id="known-issues">Known Issues&lt;/h3>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://github.com/apache/beam/milestone/3">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.41.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
Ahmet Altay
akashorabek
Alexey Inkin
Alexey Romanenko
Anand Inguva
andoni-guzman
Andrew Pilloud
Andrey
Andy Ye
Balázs Németh
Benjamin Gonzalez
BjornPrime
Brian Hulette
bulat safiullin
bullet03
Byron Ellis
Chamikara Jayalath
Damon Douglas
Daniel Oliveira
Daniel Thevessen
Danny McCormick
David Huntsperger
Dheeraj Gharde
Etienne Chauchot
Evan Galpin
Fernando Morales
Heejong Lee
Jack McCluskey
johnjcasey
Kenneth Knowles
Ke Wu
Kiley Sok
Liam Miller-Cushon
Lucas Nogueira
Luke Cwik
MakarkinSAkvelon
Manu Zhang
Minbo Bae
Moritz Mack
Naireen Hussain
Ning Kang
Oleh Borysevych
Pablo Estrada
pablo rodriguez defino
Pranav Bhandari
Rebecca Szper
Red Daly
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Steven Niemitz
Valentyn Tymofieiev
Vincent Marquez
Vitaly Terentyev
Vlad
Vladislav Chunikhin
Yichi Zhang
Yi Hu
yirutang
Yixiao Shen
Yu Feng&lt;/p></description><link>/blog/beam-2.41.0/</link><pubDate>Tue, 23 Aug 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.41.0/</guid><category>blog</category><category>release</category></item><item><title>Big Improvements in Beam Go's 2.40 Release</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>The 2.40 release is one of Beam Go&amp;rsquo;s biggest yet, and we wanted to highlight
some of the biggest changes coming with this important release!&lt;/p>
&lt;h1 id="native-streaming-support">Native Streaming Support&lt;/h1>
&lt;p>2.40 marks the release of one of our most anticipated feature sets yet:
native streaming Go pipelines. This includes adding support for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://beam.apache.org/documentation/programming-guide/#user-initiated-checkpoint">Self Checkpointing&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/programming-guide/#watermark-estimation">Watermark Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/programming-guide/#truncating-during-drain">Pipeline Drain/Truncation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://beam.apache.org/documentation/programming-guide/#bundle-finalization">Bundle Finalization&lt;/a> (added in 2.39)&lt;/li>
&lt;/ul>
&lt;p>With all of these features, it is now possible to write your own streaming
pipeline source DoFns in Go without relying on cross-language transforms
from Java or Python. We encourage you to try out all of these new features
in your streaming pipelines! The &lt;a href="https://beam.apache.org/documentation/programming-guide/#splittable-dofns">programming guide&lt;/a>
has additional information on getting started with native Go streaming DoFns.&lt;/p>
&lt;h1 id="generic-registration-make-your-pipelines-3x-faster">Generic Registration (Make Your Pipelines 3x Faster)&lt;/h1>
&lt;p>The release of &lt;a href="https://go.dev/blog/intro-generics">Go Generics&lt;/a> in Go 1.18
unlocked significant performance improvements for Beam Go. With generics,
we were able to add simple registration functions that can massively reduce
your pipeline&amp;rsquo;s runtime and resource consumption. For example, registering
the ParDo&amp;rsquo;s in our load tests which are designed to simulate a basic pipeline
reduced execution time from around 25 minutes to around 7 minutes on average&lt;/p>
&lt;ul>
&lt;li>an over 70% reduction!&lt;/li>
&lt;/ul>
&lt;p>&lt;img class="center-block"
src="/images/blog/go-registration.png"
alt="Beam Registration Load Tests ParDo Improvements">&lt;/p>
&lt;p>To get started with registering your own DoFns and unlocking these performance
gains, check out the &lt;a href="https://pkg.go.dev/github.com/apache/beam/sdks/go/pkg/beam/register">registration doc page&lt;/a>.&lt;/p>
&lt;h1 id="whats-next">What&amp;rsquo;s Next?&lt;/h1>
&lt;p>Moving forward, we remain focused on improving the streaming experience and
leveraging generics to improve the SDK. Specific improvements we are considering
include adding &lt;a href="https://beam.apache.org/documentation/programming-guide/#state-and-timers">State &amp;amp; Timers&lt;/a>
support, introducing a Go expansion service so that Go DoFns can be used in other
languages, and wrapping more Java and Python IOs so that they can be easily used
in Go. As always, please let us know what changes you would like to see by
&lt;a href="https://github.com/apache/beam/issues/new/choose">filing an issue&lt;/a>,
&lt;a href="dev@beam.apache.org">emailing the dev list&lt;/a>, or starting a &lt;a href="https://app.slack.com/client/T4S1WH2J3/C9H0YNP3P">slack thread&lt;/a>!&lt;/p></description><link>/blog/go-2.40/</link><pubDate>Wed, 06 Jul 2022 00:00:01 -0800</pubDate><guid>/blog/go-2.40/</guid><category>blog</category><category>go</category></item><item><title>Apache Beam 2.40.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.40.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2400-2022-06-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.40.0 check out the &lt;a href="https://github.com/apache/beam/releases/tag/v2.40.0">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="highlights">Highlights&lt;/h2>
&lt;ul>
&lt;li>Added &lt;a href="https://s.apache.org/inference-sklearn-pytorch">RunInference&lt;/a> API, a framework agnostic transform for inference. With this release, PyTorch and Scikit-learn are supported by the transform.
See also example at apache_beam/examples/inference/pytorch_image_classification.py&lt;/li>
&lt;/ul>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>Upgraded to Hive 3.1.3 for HCatalogIO. Users can still provide their own version of Hive. (Java) (&lt;a href="https://github.com/apache/beam/issues/19554">Issue-19554&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Go SDK users can now use generic registration functions to optimize their DoFn execution. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14347">BEAM-14347&lt;/a>)&lt;/li>
&lt;li>Go SDK users may now write self-checkpointing Splittable DoFns to read from streaming sources. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11104">BEAM-11104&lt;/a>)&lt;/li>
&lt;li>Go SDK textio Reads have been moved to Splittable DoFns exclusively. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14489">BEAM-14489&lt;/a>)&lt;/li>
&lt;li>Pipeline drain support added for Go SDK has now been tested. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>)&lt;/li>
&lt;li>Go SDK users can now see heap usage, sideinput cache stats, and active process bundle stats in Worker Status. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13829">BEAM-13829&lt;/a>)&lt;/li>
&lt;li>The serialization (pickling) library for Python is dill==0.3.1.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11167">BEAM-11167&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>The Go Sdk now requires a minimum version of 1.18 in order to support generics (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14347">BEAM-14347&lt;/a>).&lt;/li>
&lt;li>synthetic.SourceConfig field types have changed to int64 from int for better compatibility with Flink&amp;rsquo;s use of Logical types in Schemas (Go) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14173">BEAM-14173&lt;/a>)&lt;/li>
&lt;li>Default coder updated to compress sources used with &lt;code>BoundedSourceAsSDFWrapperFn&lt;/code> and &lt;code>UnboundedSourceAsSDFWrapper&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java expansion service to allow specific files to stage (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14160">BEAM-14160&lt;/a>).&lt;/li>
&lt;li>Fixed Elasticsearch connection when using both ssl and username/password (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14000">BEAM-14000&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>Python&amp;rsquo;s &lt;code>beam.FlatMap&lt;/code> will raise &lt;code>AttributeError: 'builtin_function_or_method' object has no attribute '__func__'&lt;/code> when
constructed with some
&lt;a href="https://docs.python.org/3/library/functions.html">built-ins&lt;/a>, like &lt;code>sum&lt;/code>
and &lt;code>len&lt;/code> (&lt;a href="https://github.com/apache/beam/issues/22091">#22091&lt;/a>).&lt;/li>
&lt;li>Java&amp;rsquo;s &lt;code>BigQueryIO.Write&lt;/code> can have an exception where it attempts to output a timestamp beyond the max timestamp range
&lt;code>Cannot output with timestamp 294247-01-10T04:00:54.776Z. Output timestamps must be no earlier than the timestamp of the current input or timer (294247-01-10T04:00:54.776Z) minus the allowed skew (0 milliseconds) and no later than 294247-01-10T04:00:54.775Z. See the DoFn#getAllowedTimestampSkew() Javadoc for details on changing the allowed skew.&lt;/code>
This happens when a sink is idle, causing the idle timeout to trigger, or when a specific table is idle long enough when using dynamic destinations.
When this happens, the job is no longer able to be drained. This has been fixed for the 2.41 release.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.40.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud
Ahmet Altay
Aizhamal Nurmamat kyzy
Alejandro Rodriguez-Morantes
Alexander Zhuravlev
Alexey Romanenko
Anand Inguva
andoni-guzman
Andy Ye
Balázs Németh
Benjamin Gonzalez
Brian Hulette
bulat safiullin
bullet03
Chamikara Jayalath
Damon Douglas
Daniel Oliveira
Danny McCormick
Darkhan Nausharipov
David Huntsperger
Diego Gomez
dpcollins-google
Ekaterina Tatanova
Elias Segundo
Etienne Chauchot
Evan Galpin
fbeevikm
Fernando Morales
Heejong Lee
Igor Krasavin
Ilion Beyst
Israel Herraiz
Jack McCluskey
Jan Kuehle
Jan Lukavský
johnjcasey
Jonathan Lui
jrmccluskey
Julien Tournay
Kenneth Knowles
Kerry Donny-Clark
Kevin Puthusseri
Kiley Sok
Kyle Weaver
kynx
Lucas Nogueira
Luke Cwik
LuNing Wang
Marco Robles
masahitojp
Minbo Bae
Moritz Mack
Naireen Hussain
Nancy Xu
Niel Markwick
Ning Kang
nishant jain
nishantjain91
Oskar Firlej
Pablo Estrada
pablo rodriguez defino
Rebecca Szper
Red Daly
Reuven Lax
Ritesh Ghorse
Robert Bradshaw
Robert Burke
Ryan Thompson
Sam Whittle
Thiago Nunes
Tom Stepp
vachan-shetty
Valentyn Tymofieiev
vikash2310
Vitaly Terentyev
Vladislav Chunikhin
Yichi Zhang
Yi Hu
Yiru Tang
yixiaoshen
zwestrick&lt;/p></description><link>/blog/beam-2.40.0/</link><pubDate>Sat, 25 Jun 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.40.0/</guid><category>blog</category><category>release</category></item><item><title>Apache Beam 2.39.0</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;p>We are happy to present the new 2.39.0 release of Beam.
This release includes both improvements and new functionality.
See the &lt;a href="/get-started/downloads/#2390-2022-05-25">download page&lt;/a> for this
release.&lt;/p>
&lt;p>For more information on changes in 2.39.0 check out the &lt;a href="https://issues.apache.org/jira/secure/ConfigureReleaseNote.jspa?projectId=12319527&amp;amp;version=12351170">detailed release notes&lt;/a>.&lt;/p>
&lt;h2 id="ios">I/Os&lt;/h2>
&lt;ul>
&lt;li>JmsIO gains the ability to map any kind of input to any subclass of &lt;code>javax.jms.Message&lt;/code> (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).&lt;/li>
&lt;li>JmsIO introduces the ability to write to dynamic topics (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>).
&lt;ul>
&lt;li>A &lt;code>topicNameMapper&lt;/code> must be set to extract the topic name from the input value.&lt;/li>
&lt;li>A &lt;code>valueMapper&lt;/code> must be set to convert the input value to JMS message.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Reduce number of threads spawned by BigqueryIO StreamingInserts (
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14283">BEAM-14283&lt;/a>).&lt;/li>
&lt;li>Implemented Apache PulsarIO (&lt;a href="https://issues.apache.org/jira/browse/BEAM-8218">BEAM-8218&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="new-features--improvements">New Features / Improvements&lt;/h2>
&lt;ul>
&lt;li>Support for flink scala 2.12, because most of the libraries support version 2.12 onwards. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14386">beam-14386&lt;/a>)&lt;/li>
&lt;li>&amp;lsquo;Manage Clusters&amp;rsquo; JupyterLab extension added for users to configure usage of Dataproc clusters managed by Interactive Beam (Python) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>Pipeline drain support added for Go SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-11106">BEAM-11106&lt;/a>). &lt;strong>Note: this feature is not yet fully validated and should be treated as experimental in this release.&lt;/strong>&lt;/li>
&lt;li>&lt;code>DataFrame.unstack()&lt;/code>, &lt;code>DataFrame.pivot() &lt;/code> and &lt;code>Series.unstack()&lt;/code>
implemented for DataFrame API (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13948">BEAM-13948&lt;/a>, &lt;a href="https://issues.apache.org/jira/browse/BEAM-13966">BEAM-13966&lt;/a>).&lt;/li>
&lt;li>Support for impersonation credentials added to dataflow runner in the Java and Python SDK (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14014">BEAM-14014&lt;/a>).&lt;/li>
&lt;li>Implemented Jupyterlab extension for managing Dataproc clusters (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14130">BEAM-14130&lt;/a>).&lt;/li>
&lt;li>ExternalPythonTransform API added for easily invoking Python transforms from
Java (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14143">BEAM-14143&lt;/a>).&lt;/li>
&lt;li>Added Add support for Elasticsearch 8.x (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14003">BEAM-14003&lt;/a>).&lt;/li>
&lt;li>Shard aware Kinesis record aggregation (AWS Sdk v2), (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14104">BEAM-14104&lt;/a>).&lt;/li>
&lt;li>Upgrade to ZetaSQL 2022.04.1 (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14348">BEAM-14348&lt;/a>).&lt;/li>
&lt;li>Fixed ReadFromBigQuery cannot be used with the interactive runner (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14112">BEAM-14112&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="breaking-changes">Breaking Changes&lt;/h2>
&lt;ul>
&lt;li>Unused functions &lt;code>ShallowCloneParDoPayload()&lt;/code>, &lt;code>ShallowCloneSideInput()&lt;/code>, and &lt;code>ShallowCloneFunctionSpec()&lt;/code> have been removed from the Go SDK&amp;rsquo;s pipelinex package (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13739">BEAM-13739&lt;/a>).&lt;/li>
&lt;li>JmsIO requires an explicit &lt;code>valueMapper&lt;/code> to be set (&lt;a href="https://issues.apache.org/jira/browse/BEAM-16308">BEAM-16308&lt;/a>). You can use the &lt;code>TextMessageMapper&lt;/code> to convert &lt;code>String&lt;/code> inputs to JMS &lt;code>TestMessage&lt;/code>s:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java"> &lt;span class="n">JmsIO&lt;/span>&lt;span class="o">.&amp;lt;&lt;/span>&lt;span class="n">String&lt;/span>&lt;span class="o">&amp;gt;&lt;/span>&lt;span class="n">write&lt;/span>&lt;span class="o">()&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">withConnectionFactory&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">jmsConnectionFactory&lt;/span>&lt;span class="o">)&lt;/span>
&lt;span class="o">.&lt;/span>&lt;span class="na">withValueMapper&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="n">TextMessageMapper&lt;/span>&lt;span class="o">());&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Coders in Python are expected to inherit from Coder. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14351">BEAM-14351&lt;/a>).&lt;/li>
&lt;li>New abstract method &lt;code>metadata()&lt;/code> added to io.filesystem.FileSystem in the
Python SDK. (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14314">BEAM-14314&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="deprecations">Deprecations&lt;/h2>
&lt;ul>
&lt;li>Flink 1.11 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14139">BEAM-14139&lt;/a>).&lt;/li>
&lt;li>Python 3.6 is no longer supported (&lt;a href="https://issues.apache.org/jira/browse/BEAM-13657">BEAM-13657&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="bugfixes">Bugfixes&lt;/h2>
&lt;ul>
&lt;li>Fixed Java Spanner IO NPE when ProjectID not specified in template executions (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14405">BEAM-14405&lt;/a>).&lt;/li>
&lt;li>Fixed potential NPE in BigQueryServicesImpl.getErrorInfo (Java) (&lt;a href="https://issues.apache.org/jira/browse/BEAM-14133">BEAM-14133&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;ul>
&lt;li>See a full list of open &lt;a href="https://issues.apache.org/jira/browse/BEAM-14412?jql=project%20%3D%20BEAM%20AND%20affectedVersion%20%3D%202.39.0%20ORDER%20BY%20priority%20DESC%2C%20updated%20DESC">issues that affect&lt;/a> this version.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-contributors">List of Contributors&lt;/h2>
&lt;p>According to git shortlog, the following people contributed to the 2.39.0 release. Thank you to all contributors!&lt;/p>
&lt;p>Ahmed Abualsaud,
Ahmet Altay,
Aizhamal Nurmamat kyzy,
Alexander Zhuravlev,
Alexey Romanenko,
Anand Inguva,
Andrei Gurau,
Andrew Pilloud,
Andy Ye,
Arun Pandian,
Arwin Tio,
Aydar Farrakhov,
Aydar Zainutdinov,
AydarZaynutdinov,
Balázs Németh,
Benjamin Gonzalez,
Brian Hulette,
Buqian Zheng,
Chamikara Jayalath,
Chun Yang,
Daniel Oliveira,
Daniela Martín,
Danny McCormick,
David Huntsperger,
Deepak Nagaraj,
Denise Case,
Esun Kim,
Etienne Chauchot,
Evan Galpin,
Hector Miuler Malpica Gallegos,
Heejong Lee,
Hengfeng Li,
Ilango Rajagopal,
Ilion Beyst,
Israel Herraiz,
Jack McCluskey,
Kamil Bregula,
Kamil Breguła,
Ke Wu,
Kenneth Knowles,
KevinGG,
Kiley,
Kiley Sok,
Kyle Weaver,
Liam Miller-Cushon,
Luke Cwik,
Marco Robles,
Matt Casters,
Michael Li,
MiguelAnzoWizeline,
Milan Patel,
Minbo Bae,
Moritz Mack,
Nick Caballero,
Niel Markwick,
Ning Kang,
Oskar Firlej,
Pablo Estrada,
Pavel Avilov,
Reuven Lax,
Reza Rokni,
Ritesh Ghorse,
Robert Bradshaw,
Robert Burke,
Ryan Thompson,
Sam Whittle,
Steven Niemitz,
Thiago Nunes,
Tomo Suzuki,
Valentyn Tymofieiev,
Victor,
Yi Hu,
Yichi Zhang,
Yiru Tang,
ahmedabu98,
andoni-guzman,
brachipa,
bulat safiullin,
bullet03,
dannymartinm,
daria.malkova,
dpcollins-google,
egalpin,
emily,
fbeevikm,
johnjcasey,
kileys,
&lt;a href="mailto:msbukal@google.com">msbukal@google.com&lt;/a>,
nguyennk92,
pablo rodriguez defino,
rszper,
rvballada,
sachinag,
tvalentyn,
vachan-shetty,
yirutang&lt;/p></description><link>/blog/beam-2.39.0/</link><pubDate>Wed, 25 May 2022 09:00:00 -0700</pubDate><guid>/blog/beam-2.39.0/</guid><category>blog</category><category>release</category></item><item><title>Running Beam SQL in notebooks</title><description>
&lt;!--
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>&lt;a href="https://beam.apache.org/documentation/dsls/sql/overview/">Beam SQL&lt;/a> allows a
Beam user to query PCollections with SQL statements.
&lt;a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/runners/interactive#interactive-beam">Interactive Beam&lt;/a>
provides an integration between Apache Beam and
&lt;a href="https://docs.jupyter.org/en/latest/">Jupyter Notebooks&lt;/a> (formerly known as
IPython Notebooks) to make pipeline prototyping and data exploration much faster
and easier.
You can set up your own notebook user interface (for example,
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html">JupyterLab&lt;/a>
or classic &lt;a href="https://docs.jupyter.org/en/latest/install.html">Jupyter Notebooks&lt;/a>)
on your own device following their documentations. Alternatively, you can
choose a hosted solution that does everything for you. You are free to select
whichever notebook user interface you prefer. For simplicity, this
post does not go through the notebook environment setup and uses
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development">Apache Beam Notebooks&lt;/a>
that provides a cloud-hosted
&lt;a href="https://jupyterlab.readthedocs.io/en/stable/">JupyterLab&lt;/a> environment and lets
a Beam user iteratively develop pipelines, inspect pipeline graphs, and parse
individual PCollections in a read-eval-print-loop (REPL) workflow.&lt;/p>
&lt;p>In this post, you will see how to use &lt;code>beam_sql&lt;/code>, a notebook
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">magic&lt;/a>, to
execute Beam SQL in notebooks and inspect the results.&lt;/p>
&lt;p>By the end of the post, it also demonstrates how to use the &lt;code>beam_sql&lt;/code> magic
with a production environment, such as running it as a one-shot job on
Dataflow. It&amp;rsquo;s optional. To follow those steps, you should have a project in
Google Cloud Platform with
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#before_you_begin">necessary APIs enabled&lt;/a>
, and you should have enough permissions to create a Google Cloud Storage bucket
(or to use an existing one), query a public Google Cloud BigQuery dataset, and
run Dataflow jobs.&lt;/p>
&lt;p>If you choose to use the cloud hosted notebook solution, once you have your
Google Cloud project ready, you will need to create an Apache Beam Notebooks
instance and open the JupyterLab web interface. Please follow the instructions
given at:
&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance">https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#launching_an_notebooks_instance&lt;/a>&lt;/p>
&lt;h2 id="getting-familiar-with-the-environment">Getting familiar with the environment&lt;/h2>
&lt;h3 id="landing-page">Landing page&lt;/h3>
&lt;p>After starting your own notebook user interface: for example, if using Apche
Beam Notebooks, after clicking the &lt;code>OPEN JUPYTERLAB&lt;/code> link, you will land on
the default launcher page of the notebook environment.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image1.png"
alt="Beam SQL in Notebooks: landing page">&lt;/p>
&lt;p>On the left side, there is a file explorer to view examples, tutorials and
assets on the notebook instance. To easily navigate the files, you may
double-click the &lt;code>00-Start_Here.md&lt;/code> (#1 in the screenshot) file to view detailed
information about the files.&lt;/p>
&lt;p>On the right side, it displays the default launcher page of JupyterLab. To
create and open a completely new notebook file and code with a selected version
of Apache Beam, click one of (#2) the items with Apache Beam &amp;gt;=2.34.0 (because
&lt;code>beam_sql&lt;/code> was introduced in 2.34.0) installed.&lt;/p>
&lt;h3 id="createopen-a-notebook">Create/open a notebook&lt;/h3>
&lt;p>For example, if you clicked the image button with Apache Beam 2.36.0, you would
see an &lt;code>Untitled.ipynb&lt;/code> file created and opened.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image2.png"
alt="Beam SQL in Notebooks: create/open a notebook ">&lt;/p>
&lt;p>In the file explorer, your new notebook file has been created as
&lt;code>Untitled.ipynb&lt;/code>.&lt;/p>
&lt;p>On the right side, in the opened notebook, there are 4 buttons on top that you
may interact most frequently with:&lt;/p>
&lt;ul>
&lt;li>#1: insert an empty code block after the selected / highlighted code block&lt;/li>
&lt;li>#2: execute the code in the block that is selected / highlighted&lt;/li>
&lt;li>#3: interrupt code execution if your code execution is stuck&lt;/li>
&lt;li>#4: “Restart the kernel”: clear all states from code executions and start
from fresh&lt;/li>
&lt;/ul>
&lt;p>There is a button on the top-right (#5) for you to choose a different Apache
Beam version if needed, so it’s not set in stone.&lt;/p>
&lt;p>You can always double-click a file from the file explorer to open it without
creating a new one.&lt;/p>
&lt;h2 id="beam-sql">Beam SQL&lt;/h2>
&lt;h3 id="beam_sql-magic">&lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>&lt;code>beam_sql&lt;/code> is an IPython
&lt;a href="https://ipython.readthedocs.io/en/stable/config/custommagics.html">custom magic&lt;/a>.
If you&amp;rsquo;re not familiar with magics, here are some
&lt;a href="https://ipython.readthedocs.io/en/stable/interactive/magics.html">built-in examples&lt;/a>.
It&amp;rsquo;s a convenient way to validate your queries locally against known/test data
sources when prototyping a Beam pipeline with SQL, before productionizing it on
remote cluster/services.&lt;/p>
&lt;p>The Apache Beam Notebooks environment has preloaded the &lt;code>beam_sql&lt;/code> magic and
basic &lt;code>apache-beam&lt;/code> modules so you can directly use them without additional
imports. You can also explicitly load the magic via
&lt;code>%load_ext apache_beam.runners.interactive.sql.beam_sql_magics&lt;/code> and
&lt;code>apache-beam&lt;/code> modules if you set up your own notebook elsewhere.&lt;/p>
&lt;p>You can type:&lt;/p>
&lt;pre>&lt;code>%beam_sql -h
&lt;/code>&lt;/pre>&lt;p>and then execute the code to learn how to use the magic:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image3.png"
alt="Beam SQL in Notebooks: beam_sql magic help message ">&lt;/p>
&lt;p>The selected/highlighted block is called a notebook cell. It mainly has 3
components:&lt;/p>
&lt;ul>
&lt;li>#1: The execution count. &lt;code>[1]&lt;/code> indicates this block is the first executed
code. It increases by 1 for each piece of code you execute even if you
re-execute the same piece of code. &lt;code>[ ]&lt;/code> indicates this block is not
executed.&lt;/li>
&lt;li>#2: The cell input: the code gets executed.&lt;/li>
&lt;li>#3: The cell output: the output of the code execution. Here it contains the
help documentation of the &lt;code>beam_sql&lt;/code> magic.&lt;/li>
&lt;/ul>
&lt;h3 id="create-a-pcollection">Create a PCollection&lt;/h3>
&lt;p>There are 3 scenarios for Beam SQL when creating a PCollection:&lt;/p>
&lt;ol>
&lt;li>Use Beam SQL to create a PCollection from constant values&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>%%beam_sql -o pcoll
SELECT CAST(1 AS INT) AS id, CAST('foo' AS VARCHAR) AS str, CAST(3.14 AS DOUBLE) AS flt
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image4.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from raw values.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic creates and outputs a PCollection named &lt;code>pcoll&lt;/code> with
element_type like &lt;code>BeamSchema_...(id: int32, str: str, flt: float64)&lt;/code>.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> that you have &lt;strong>not&lt;/strong> explicitly created a Beam pipeline. You get a
PCollection because the &lt;code>beam_sql&lt;/code> magic always implicitly creates a pipeline to
execute your SQL query. To hold the elements with each field&amp;rsquo;s type info, Beam
automatically creates a
&lt;a href="https://beam.apache.org/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>
as the &lt;code>element_type&lt;/code> for the created PCollection. You will learn more about
schema-aware PCollections later.&lt;/p>
&lt;ol start="2">
&lt;li>Use Beam SQL to query a PCollection&lt;/li>
&lt;/ol>
&lt;p>You can chain another SQL using the output from a previous SQL (or any
schema-aware PCollection produced by any normal Beam PTransforms) as the input
to produce a new PCollection.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: if you name the output PCollection, make sure that it’s unique in your
notebook to avoid overwriting a different PCollection.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o id_pcoll
SELECT id FROM pcoll
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image5.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from another.">&lt;/p>
&lt;ol start="3">
&lt;li>Use Beam SQL to join multiple PCollections&lt;/li>
&lt;/ol>
&lt;p>You can query multiple PCollections from a single query.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o str_with_same_id
SELECT id, str FROM pcoll JOIN id_pcoll USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image6.png"
alt="Beam SQL in Notebooks: beam_sql creates a PCollection from multiple PCollections.">&lt;/p>
&lt;p>Now you have learned how to use the &lt;code>beam_sql&lt;/code> magic to create PCollections and
inspect their results.&lt;/p>
&lt;p>&lt;strong>Tip&lt;/strong>: if you accidentally delete some of the notebook cell outputs, you can
always check the content of a PCollection by invoking &lt;code>ib.show(pcoll_name)&lt;/code> or
&lt;code>ib.collect(pcoll_name)&lt;/code> where &lt;code>ib&lt;/code> stands for “Interactive Beam”
(&lt;a href="https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development#reading_and_visualizing_the_data">learn more&lt;/a>).&lt;/p>
&lt;h3 id="schema-aware-pcollections">Schema-aware PCollections&lt;/h3>
&lt;p>The &lt;code>beam_sql&lt;/code> magic provides the flexibility to seamlessly mix SQL and non-SQL
Beam statements to build pipelines and even run them on Dataflow. However, each
PCollection queried by Beam SQL needs to have a
&lt;a href="https://beam.apache.org/documentation/programming-guide/#what-is-a-schema">schema&lt;/a>.
For the &lt;code>beam_sql&lt;/code> magic, it’s recommended to use &lt;code>typing.NamedTuple&lt;/code> when a
schema is desired. You can go through the below example to learn more details
about schema-aware PCollections.&lt;/p>
&lt;h4 id="setup">Setup&lt;/h4>
&lt;p>In the setup of this example, you will:&lt;/p>
&lt;ul>
&lt;li>Install PyPI package &lt;code>names&lt;/code> using the built-in &lt;code>%pip&lt;/code> magic: you will use
the module to generate some random English names as the raw data input.&lt;/li>
&lt;li>Define a schema with &lt;code>NamedTuple&lt;/code> that has 2 attributes: &lt;code>id&lt;/code> - an unique
numeric identifier of a person; &lt;code>name&lt;/code> - a string name of a person.&lt;/li>
&lt;li>Define a pipeline with an &lt;code>InteractiveRunner&lt;/code> to utilize notebook related
features of Apache Beam.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="o">%&lt;/span>&lt;span class="n">pip&lt;/span> &lt;span class="n">install&lt;/span> &lt;span class="n">names&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">names&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="nb">id&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;span class="n">name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is no visible output for the code execution.&lt;/p>
&lt;h4 id="create-schema-aware-pcollections-without-using-sql">Create schema-aware PCollections without using SQL&lt;/h4>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">persons&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image7.png"
alt="Beam SQL in Notebooks: create a schema-aware PCollection without SQL.">&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">persons_2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Person&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">name&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">names&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_full_name&lt;/span>&lt;span class="p">())&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">15&lt;/span>&lt;span class="p">)]))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">persons_2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image8.png"
alt="Beam SQL in Notebooks: create another schema-aware PCollection without SQL.">&lt;/p>
&lt;p>Now you have 2 PCollections both with the same schema defined by the &lt;code>Person&lt;/code>
class:&lt;/p>
&lt;ul>
&lt;li>&lt;code>persons&lt;/code> contains 10 records for 10 persons with ids ranging from 0 to 9,&lt;/li>
&lt;li>&lt;code>persons_2&lt;/code> contains another 10 records for 10 persons with ids ranging from
5 to 14.&lt;/li>
&lt;/ul>
&lt;h4 id="encode-and-decode-of-schema-aware-pcollections">Encode and Decode of schema-aware PCollections&lt;/h4>
&lt;p>For this example, you still need one more piece of data from the first &lt;code>pcoll&lt;/code>
that you have created with instructions in this post.&lt;/p>
&lt;p>You can use the original &lt;code>pcoll&lt;/code>. Optionally, if you want to exercise using
coders explicitly with schema-aware PCollections, you can add a Text I/O into
the mix: write the content of &lt;code>pcoll&lt;/code> into a text file retaining its schema
information, then read the file back into a new schema-aware PCollection called
&lt;code>pcoll_in_file&lt;/code>, and use the new PCollection to join &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>
to find names with the common id in all three of them.&lt;/p>
&lt;p>To encode &lt;code>pcoll&lt;/code> into a file, execute:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">pcoll&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;/tmp/pcoll&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pipeline&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">wait_until_finish&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="err">!&lt;/span>&lt;span class="n">cat&lt;/span> &lt;span class="o">/&lt;/span>&lt;span class="n">tmp&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">pcoll&lt;/span>&lt;span class="o">*&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image9.png"
alt="Beam SQL in Notebooks: write a schema-aware PCollection into a text file.">&lt;/p>
&lt;p>The above code execution writes the PCollection &lt;code>pcoll&lt;/code> (basically
&lt;code>{id: 1, str: foo, flt: 3.14}&lt;/code>) into a text file using the coder assigned by
Beam. As you can see, the file content is recorded in a binary non
human-readable format, and that’s normal.&lt;/p>
&lt;p>To decode the file content into a new PCollection, execute:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">pcoll_in_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromText&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="s1">&amp;#39;/tmp/pcoll*&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">pcoll&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pcoll_in_file&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image10.png"
alt="Beam SQL in Notebooks: read a schema-aware PCollection from a text file.">&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> you have to use the same coder during encoding and decoding, and
furthermore you may assign the schema explicitly to the new PCollection through
&lt;code>with_output_types()&lt;/code>.&lt;/p>
&lt;p>Reading out the encoded binary content from the text file and decoding it with
the correct coder, the content of &lt;code>pcoll&lt;/code> is recovered into &lt;code>pcoll_in_file&lt;/code>. You
can use this technique to save and share your data through any Beam I/O (not
necessarily a text file) with collaborators who work on their own pipelines (not
just in your notebook session or pipelines).&lt;/p>
&lt;h4 id="schema-in-beam_sql-magic">Schema in &lt;code>beam_sql&lt;/code> magic&lt;/h4>
&lt;p>The &lt;code>beam_sql&lt;/code> magic automatically registers a &lt;code>RowCoder&lt;/code> for your &lt;code>NamedTuple&lt;/code>
schema so that you only need to focus on preparing your data for query without
worrying about coders. To see more verbose details of what the &lt;code>beam_sql&lt;/code> magic
does behind the scenes, you can use the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>For example, you can look for all elements with &lt;code>id &amp;lt; 5&lt;/code> in &lt;code>persons&lt;/code> with the
below query and assign the output to &lt;code>persons_id_lt_5&lt;/code>.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o persons_id_lt_5 -v
SELECT * FROM persons WHERE id &amp;lt; 5
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image11.png"
alt="Beam SQL in Notebooks: beam_sql registers a schema for a PCollection.">&lt;/p>
&lt;p>Since this is the first time running this query, you might see a warning message
about:&lt;/p>
&lt;blockquote>
&lt;p>Schema Person has not been registered to use a RowCoder. Automatically
registering it by running:
beam.coders.registry.register_coder(Person, beam.coders.RowCoder)&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>beam_sql&lt;/code> magic helps registering a &lt;code>RowCoder&lt;/code> for each schema you define
and use whenever it finds one. You can also explicitly run the same code to do
so.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong> the output element type is &lt;code>Person(id: int, name: str)&lt;/code> instead of
&lt;code>BeamSchema_…&lt;/code> because you have selected all the fields from a single
PCollection of the known type &lt;code>Person(id: int, name: str)&lt;/code>.&lt;/p>
&lt;p>Another example, you can query for all names from &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code> with
the same ids and assign the output to &lt;code>persons_with_common_id&lt;/code>:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o persons_with_common_id -v
SELECT * FROM persons JOIN persons_2 USING (id)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image12.png"
alt="Beam SQL in Notebooks: beam_sql creates a schema for a query.">&lt;/p>
&lt;p>Note the output element type is now some
&lt;code>BeamSchema_...(id: int64, name: str, name0: str)&lt;/code>. Because you have selected
columns from both PCollections, there is no known schema to hold the result.
Beam automatically creates a schema and differentiates the conflicted field
&lt;code>name&lt;/code> by suffixing 0 to one of them.&lt;/p>
&lt;p>And since &lt;code>Person&lt;/code> is already previously registered with a &lt;code>RowCoder&lt;/code>, there is
no more warning about registering it even with the &lt;code>-v&lt;/code> option.&lt;/p>
&lt;p>Additionally, you can do a join with &lt;code>pcoll_in_file&lt;/code>, &lt;code>persons&lt;/code> and &lt;code>persons_2&lt;/code>:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o entry_with_common_id
SELECT pcoll_in_file.id, persons.name AS name_1, persons_2.name AS name_2
FROM pcoll_in_file JOIN persons ON pcoll_in_file.id = persons.id
JOIN persons_2 ON pcoll_in_file.id = persons_2.id
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image13.png"
alt="Beam SQL in Notebooks: rename fields in a query.">&lt;/p>
&lt;p>The schema generated reflects the column renaming you have done in the SQL.&lt;/p>
&lt;h2 id="an-example">An Example&lt;/h2>
&lt;p>You will go through an example to find out the US state with the most COVID
positive cases on a specific day with data provided by the
&lt;a href="https://covidtracking.com/">covid tracking project&lt;/a>.&lt;/p>
&lt;h3 id="get-the-data">Get the data&lt;/h3>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">json&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">requests&lt;/span>
&lt;span class="c1"># The covidtracking project has stopped collecting new data, current data ends on 2021-03-07&lt;/span>
&lt;span class="n">json_current&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;https://covidtracking.com/api/v1/states/current.json&amp;#39;&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">with&lt;/span> &lt;span class="n">requests&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">session&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">json&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">loads&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">session&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">url&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">data&lt;/span>
&lt;span class="n">current_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_json_data&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">json_current&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">current_data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image14.png"
alt="Beam SQL in Notebooks: preview example data.">&lt;/p>
&lt;p>The data is dated as 2021-03-07. It contains many details about COVID cases for
different states in the US. &lt;code>current_data[0]&lt;/code> is just one of the data points.&lt;/p>
&lt;p>You can get rid of most of the columns of the data. For example, just focus on
“date”, “state”, “positive” and “negative”, and then define a schema
&lt;code>UsCovidData&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span> &lt;span class="c1"># Remember to str(e[&amp;#39;date&amp;#39;]).&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">positive&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">int&lt;/span>
&lt;span class="n">negative&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>date&lt;/code> is a keyword in (Calcite)SQL, use a different field name such as
&lt;code>partition_date&lt;/code>;&lt;/li>
&lt;li>&lt;code>date&lt;/code> from the data is an &lt;code>int&lt;/code> type, not &lt;code>str&lt;/code>. Make sure you convert the
data using &lt;code>str()&lt;/code> or use &lt;code>date: int&lt;/code>.&lt;/li>
&lt;li>&lt;code>negative&lt;/code> has missing values and the default is &lt;code>None&lt;/code>. So instead of
&lt;code>negative: int&lt;/code>, it should be &lt;code>negative: Optional[int]&lt;/code>. Or you can convert
&lt;code>None&lt;/code> into 0 when using the schema.&lt;/li>
&lt;/ul>
&lt;p>Then parse the json data into a PCollection with the schema:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">p_sql&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_sql&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Create PCollection from json&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Create&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">current_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">positive&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;positive&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">negative&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;negative&amp;#39;&lt;/span>&lt;span class="p">]))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="n">ib&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">covid_data&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image15.png"
alt="Beam SQL in Notebooks: parse example data with a schema.">&lt;/p>
&lt;h3 id="query">Query&lt;/h3>
&lt;p>You can now find the biggest positive on the “current day” (2021-03-07).&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o max_positive
SELECT partition_date, MAX(positive) AS positive
FROM covid_data
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image16.png"
alt="Beam SQL in Notebooks: find the biggest positive from the data.">&lt;/p>
&lt;p>However, this is just the positive number. You cannot observe the state that has
this maximum number nor the negative case number for the state.&lt;/p>
&lt;p>To enrich your result, you have to join this data back to the original data set
you have parsed.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o entry_with_max_positive
SELECT covid_data.partition_date, covid_data.state, covid_data.positive, {fn IFNULL(covid_data.negative, 0)} AS negative
FROM covid_data JOIN max_positive
USING (partition_date, positive)
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image17.png"
alt="Beam SQL in Notebooks: enriched data with biggest positive.">&lt;/p>
&lt;p>Now you can see all columns of the data with the maximum positive case on
2021-03-07.
&lt;strong>Note&lt;/strong>: to handle missing values of the negative column in the original data,
you can use &lt;code>{fn IFNULL(covid_data.negative, 0)}&lt;/code> to set null values to 0.&lt;/p>
&lt;p>When you&amp;rsquo;re ready to scale up, you can translate the SQLs into a pipeline with
&lt;code>SqlTransform&lt;/code>s and run your pipeline on a distributed runner like Flink or
Spark. This post demonstrates it by launching a one-shot job on Dataflow from
the notebook with the help of &lt;code>beam_sql&lt;/code> magic.&lt;/p>
&lt;h3 id="run-on-dataflow">Run on Dataflow&lt;/h3>
&lt;p>Now that you have a pipeline that parses US COVID data from json to find
positive/negative/state information for the state with the most positive cases
on each day, you can try applying it to all historical daily data and running it
on Dataflow.&lt;/p>
&lt;p>The new data source you will use is a public dataset from USAFacts US
Coronavirus Database that contains all historical daily summary of COVID cases
in the US.&lt;/p>
&lt;p>The schema of data is very similar to what the covid tracking project website
provides. The fields you will query are: &lt;code>date&lt;/code>, &lt;code>state&lt;/code>, &lt;code>confirmed_cases&lt;/code>, and
&lt;code>deaths&lt;/code>.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image18.png"
alt="Beam SQL in Notebooks: schema of cloud data.">&lt;/p>
&lt;p>A preview of the data looks like below (you may skip the inspection in BigQuery
and just take a look at the screenshot):&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image19.png"
alt="Beam SQL in Notebooks: preview of cloud data.">&lt;/p>
&lt;p>The format of the data is &lt;strong>slightly different&lt;/strong> from the json data you parsed
in the previous pipeline because the numbers are grouped by counties instead of
states, thus some additional aggregations need to be done in the SQLs.&lt;/p>
&lt;p>If you need a fresh execution, you may click the “Restart the kernel” button on
the top menu.&lt;/p>
&lt;p>Full code is as below, on-top of the original pipeline and queries:&lt;/p>
&lt;ul>
&lt;li>It changes the source from a single-day data to a more complete historical
data;&lt;/li>
&lt;li>It changes the I/O and schema to accommodate the new dataset;&lt;/li>
&lt;li>It changes the SQLs to include more aggregations to accommodate the new
format of the dataset.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Prepare the data with schema&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">NamedTuple&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">typing&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Optional&lt;/span>
&lt;span class="c1"># Public BQ dataset.&lt;/span>
&lt;span class="n">table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;bigquery-public-data:covid19_usafacts.summary&amp;#39;&lt;/span>
&lt;span class="c1"># Replace with your project.&lt;/span>
&lt;span class="n">project&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;YOUR-PROJECT-NAME-HERE&amp;#39;&lt;/span>
&lt;span class="c1"># Replace with your GCS bucket.&lt;/span>
&lt;span class="n">gcs_location&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;gs://YOUR_GCS_BUCKET_HERE&amp;#39;&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NamedTuple&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">str&lt;/span>
&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">deaths&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">Optional&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">p_on_dataflow&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Pipeline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">runner&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">InteractiveRunner&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="n">covid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">p_on_dataflow&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Read dataset&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReadFromBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">project&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">project&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">table&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">gcs_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;Parse&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="k">lambda&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">partition_date&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;date&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">]),&lt;/span>
&lt;span class="n">deaths&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">])))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">with_output_types&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">UsCovidData&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Run on Dataflow&lt;/strong>&lt;/p>
&lt;p>To run SQL on Dataflow is very simple, you just need to add the option
&lt;code>-r DataflowRunner&lt;/code>.&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o data_by_state -r DataflowRunner
SELECT partition_date, state, SUM(confirmed_cases) as confirmed_cases, SUM(deaths) as deaths
FROM covid_data
GROUP BY partition_date, state
&lt;/code>&lt;/pre>&lt;p>Different from previous &lt;code>beam_sql&lt;/code> magic executions, you won’t see the result
immediately. Instead, a form like below is printed in the notebook cell output:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image20.png"
alt="Beam SQL in Notebooks: empty run-on-dataflow form.">&lt;/p>
&lt;p>The &lt;code>beam_sql&lt;/code> magic tries its best to guess your project id and preferred cloud
region. You still have to input additional information necessary to submit a
Dataflow job, such as a GCS bucket to stage the Dataflow job and any additional
Python dependencies the job needs.&lt;/p>
&lt;p>For now, ignore the form in the cell output, because you still need 2 more SQLs
to: 1) find the maximum confirmed cases on each day; 2) join the maximum case
data with the full data_by_state. The &lt;code>beam_sql&lt;/code> magic allows you to chain SQLs,
so chain 2 more by executing:&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o max_cases -r DataflowRunner
SELECT partition_date, MAX(confirmed_cases) as confirmed_cases
FROM data_by_state
GROUP BY partition_date
&lt;/code>&lt;/pre>&lt;p>And&lt;/p>
&lt;pre>&lt;code>%%beam_sql -o data_with_max_cases -r DataflowRunner
SELECT data_by_state.partition_date, data_by_state.state, data_by_state.confirmed_cases, data_by_state.deaths
FROM data_by_state JOIN max_cases
USING (partition_date, confirmed_cases)
&lt;/code>&lt;/pre>&lt;p>By default, when running &lt;code>beam_sql&lt;/code> on Dataflow, the output PCollection will be
written to a text file on GCS. The “write” is automatically provided by
&lt;code>beam_sql&lt;/code> and mainly for your inspection of the output data for this one-shot
Dataflow job. It’s lightweight and does not encode elements for further
development. To save the output and share it with others, you can add more Beam
I/Os into the mix.&lt;/p>
&lt;p>For example, you can appropriately encode elements into text files using the
technique described in the above schema-aware PCollections example.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">from&lt;/span> &lt;span class="nn">apache_beam.options.pipeline_options&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">GoogleCloudOptions&lt;/span>
&lt;span class="n">coder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">coders&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">registry&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_coder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">element_type&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">max_data_file&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gcs_location&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s1">&amp;#39;/encoded_max_data&amp;#39;&lt;/span>
&lt;span class="n">data_with_max_cases&lt;/span> &lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">textio&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToText&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_data_file&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">coder&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">coder&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Furthermore, you can create a new BQ dataset in your own project to store the
processed data.&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image21.png"
alt="Beam SQL in Notebooks: create a new BQ dataset.">&lt;/p>
&lt;p>You have to select the same data location as the public BigQuery data you are
reading. In this case, “us (multiple regions in United States)”.&lt;/p>
&lt;p>Once you finish creating an empty dataset, you can execute below:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="n">output_table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">f&lt;/span>&lt;span class="s1">&amp;#39;{project}:covid_data.max_analysis&amp;#39;&lt;/span>
&lt;span class="n">bq_schema&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s1">&amp;#39;fields&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;STRING&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">},&lt;/span>
&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;name&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;type&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s1">&amp;#39;INTEGER&amp;#39;&lt;/span>&lt;span class="p">}]}&lt;/span>
&lt;span class="p">(&lt;/span>&lt;span class="n">data_with_max_cases&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="s1">&amp;#39;To json-like&amp;#39;&lt;/span> &lt;span class="o">&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="s1">&amp;#39;partition_date&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">partition_date&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;state&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;confirmed_cases&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">confirmed_cases&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="s1">&amp;#39;deaths&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">deaths&lt;/span>&lt;span class="p">})&lt;/span>
&lt;span class="o">|&lt;/span> &lt;span class="n">beam&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">io&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">WriteToBigQuery&lt;/span>&lt;span class="p">(&lt;/span>
&lt;span class="n">table&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">output_table&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">schema&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">bq_schema&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">method&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;STREAMING_INSERTS&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="n">custom_gcs_temp_location&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">gcs_location&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now back in the form of the last SQL cell output, you may fill in necessary
information to run the pipeline on Dataflow. An example input looks like below:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image22.png"
alt="Beam SQL in Notebooks: fill in the run-on-Dataflow form.">&lt;/p>
&lt;p>Because this pipeline doesn’t use any additional Python dependency, “Additional
Packages” is left empty. In the previous example where you have installed a
package called &lt;code>names&lt;/code>, to run that pipeline on Dataflow, you have to put
&lt;code>names&lt;/code> in this field.&lt;/p>
&lt;p>Once you finish updating your inputs, you can click the &lt;code>Show Options&lt;/code> button to
view what pipeline options have been configured based on your inputs. A variable
&lt;code>options_[YOUR_OUTPUT_PCOLL_NAME]&lt;/code> is generated, and you can supply more
pipeline options to it if the form is not enough for your execution.&lt;/p>
&lt;p>Once you are ready to submit the Dataflow job, click the &lt;code>Run on Dataflow&lt;/code>
button. It tells you where the default output would be written, and after a
while, a line with:&lt;/p>
&lt;blockquote>
&lt;p>Click here for the details of your Dataflow job.&lt;/p>
&lt;/blockquote>
&lt;p>would be displayed. You can click on the hyperlink to go to your Dataflow job
page. (Optionally, you can ignore the form and continue development to extend
your pipeline. Once you are satisfied with the state of your pipeline, you can
come back to the form and submit the job to Dataflow.)&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image23.png"
alt="Beam SQL in Notebooks: a Dataflow job graph.">&lt;/p>
&lt;p>As you can see, each transform name of the generated Dataflow job is prefixed
with a string &lt;code>[number]: &lt;/code>. This is to distinguish re-executed codes in
notebooks because Beam requires each transform to have a distinct name. Under
the hood, the &lt;code>beam_sql&lt;/code> magic also stages your schema information to Dataflow,
so you might see transforms named as &lt;code>schema_loaded_beam_sql_…&lt;/code>. This is because
the &lt;code>NamedTuple&lt;/code> defined in the notebook is likely in the &lt;code>__main__&lt;/code> scope and
Dataflow is not aware of them at all. To minimize user intervention and avoid
pickling the whole main session (and it’s infeasible to pickle the main session
when it contains unpickle-able attributes), the &lt;code>beam_sql&lt;/code> magic optimizes the
staging process by serializing your schemas, staging them to Dataflow, and then
deserialize/load them for job execution.&lt;/p>
&lt;p>Once the job succeeds, the result of the output PCollection would be written to
places instructed by your I/O transforms. &lt;strong>Note&lt;/strong>: running &lt;code>beam_sql&lt;/code> on
Dataflow generates a one-shot job and it’s not interactive.&lt;/p>
&lt;p>A simple inspection of the data from the default output location:&lt;/p>
&lt;pre>&lt;code>!gsutil cat 'gs://ningk-so-test/bq/staging/data_with_max_cases*'
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image24.png"
alt="Beam SQL in Notebooks: inspect the default output file.">&lt;/p>
&lt;p>The text file with encoded binary data written by your &lt;code>WriteToText&lt;/code>:&lt;/p>
&lt;pre>&lt;code>!gsutil cat 'gs://ningk-so-test/bq/encoded_max_data*'
&lt;/code>&lt;/pre>&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image25.png"
alt="Beam SQL in Notebooks: inspect the user-defined output file.">&lt;/p>
&lt;p>The table &lt;code>YOUR-PROJECT:covid_data.max_analysis&lt;/code> created by your
&lt;code>WriteToBigQuery&lt;/code>:&lt;/p>
&lt;p>&lt;img class="center-block"
src="/images/blog/beam-sql-notebooks/image26.png"
alt="Beam SQL in Notebooks: inspect the output BQ dataset.">&lt;/p>
&lt;h3 id="run-on-other-oss-runners-directly-with-the-beam_sql-magic">Run on other OSS runners directly with the &lt;code>beam_sql&lt;/code> magic&lt;/h3>
&lt;p>On the day this blog is posted, the &lt;code>beam_sql&lt;/code> magic only supports DirectRunner
(interactive) and DataflowRunner (one-shot). It&amp;rsquo;s a simple wrapper on top of
the &lt;code>SqlTransform&lt;/code> with interactive input widgets implemented by
&lt;a href="https://ipywidgets.readthedocs.io/en/stable/">ipywidgets&lt;/a>. You can implement
your own runner support or utilities by following the
&lt;a href="https://lists.apache.org/thread/psrx1xhbyjcqbhxx6trf5nvh66c6pk3y">instructions&lt;/a>.&lt;/p>
&lt;p>Additionally, support for other OSS runners are WIP, for example,
&lt;a href="https://issues.apache.org/jira/browse/BEAM-14373">support using FlinkRunner with the &lt;code>beam_sql&lt;/code> magic&lt;/a>.&lt;/p>
&lt;h2 id="conclusions">Conclusions&lt;/h2>
&lt;p>The &lt;code>beam_sql&lt;/code> magic and Apache Beam Notebooks combined is a convenient tool for
you to learn Beam SQL and mix Beam SQL into prototyping and productionizing (
e.g., to Dataflow) your Beam pipelines with minimum setups.&lt;/p>
&lt;p>For more details about the Beam SQL syntax, check out the Beam Calcite SQL
&lt;a href="https://beam.apache.org/documentation/dsls/sql/calcite/overview/">compatibility&lt;/a>
and the Apache Calcite SQL
&lt;a href="https://calcite.apache.org/docs/reference.html">syntax&lt;/a>.&lt;/p></description><link>/blog/beam-sql-with-notebooks/</link><pubDate>Thu, 28 Apr 2022 00:00:01 -0800</pubDate><guid>/blog/beam-sql-with-notebooks/</guid><category>blog</category></item></channel></rss>