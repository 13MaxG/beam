<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Apache Beam Python Machine Learning</title><meta name=description content="Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing data processing workflows, and also data ingestion and integration flows, supporting Enterprise Integration Patterns (EIPs) and Domain Specific Languages (DSLs). Dataflow pipelines simplify the mechanics of large-scale batch and streaming data processing and can run on a number of runtimes like Apache Flink, Apache Spark, and Google Cloud Dataflow (a cloud service). Beam also brings DSL in different languages, allowing users to easily implement their data integration processes."><link href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700" rel=stylesheet><link rel=preload href=/scss/main.min.37842f23ca27d07e23cb7df297983c7990ad65ff7a262278363d3441bf9b9186.css as=style><link href=/scss/main.min.37842f23ca27d07e23cb7df297983c7990ad65ff7a262278363d3441bf9b9186.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-2.2.4.min.js></script><style>.body__contained img{max-width:100%}</style><script type=text/javascript src=/js/bootstrap.min.2979f9a6e32fc42c3e7406339ee9fe76b31d1b52059776a02b4a7fa6a4fd280a.js defer></script><script type=text/javascript src=/js/language-switch-v2.min.40f17a8b96c8538b9763c8c8bdfa878f9dc44618fb06b34bb2c0481b7af40efb.js defer></script><script type=text/javascript src=/js/fix-menu.min.fd987a7cda201b5f904e8f2e3300020c5c45c7b9f6ec4e43bf61e8f12d424717.js defer></script><script type=text/javascript src=/js/section-nav.min.8c5356fa02e287ef99bfc8e2eee4d4f770e8d16de8fb3c1ba7c755567e7d90bd.js defer></script><script type=text/javascript src=/js/page-nav.min.bf21527a035e495bbda8f8705a7f2dad5479e82146b7772bb3532106be57ed4b.js defer></script><script type=text/javascript src=/js/expandable-list.min.906430196d9dc7c180eecc10131d3e929d1ffc224d695a2e2b4c4e1d3bb11043.js defer></script><script type=text/javascript src=/js/copy-to-clipboard.min.f6c6316040b86d30e6e596205510a7799d5af2f121c3eeb55e5fd66614d7e842.js defer></script><script type=text/javascript src=/js/calendar.min.430bec36c3b3f6f39206f4abbd1ab42f75a71e557a1fe8e43dbca15cf09cace3.js defer></script><script type=text/javascript src=/js/fix-playground-nested-scroll.min.d636da77fee5f53da36a0eff0539f472c9be3c772accf1b37f64d38f09d8957c.js defer></script><script type=text/javascript src=/js/anchor-content-jump-fix.min.0e2c502a1006e8b9565bba4fb172b50f0d068cd06e7900186a7d05f0bf7fd2b0.js defer></script><link rel=alternate type=application/rss+xml title="Apache Beam" href=/feed.xml><link rel=canonical href=/documentation/sdks/python-machine-learning/ data-proofer-ignore><link rel="shortcut icon" type=image/x-icon href=/images/favicon.ico><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.4.1/css/all.css integrity=sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz crossorigin=anonymous><link rel=stylesheet href=https://unpkg.com/swiper@8/swiper-bundle.min.css><script async src=https://platform.twitter.com/widgets.js></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-73650088-1','auto');ga('send','pageview');</script><script>(function(h,o,t,j,a,r){h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};h._hjSettings={hjid:2182187,hjsv:6};a=o.getElementsByTagName('head')[0];r=o.createElement('script');r.async=1;r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;a.appendChild(r);})(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');</script></head><body class=body data-spy=scroll data-target=.page-nav data-offset=0><nav class="navigation-bar-mobile header navbar navbar-fixed-top"><div class=navbar-header><a href=/ class=navbar-brand><img alt=Brand style=height:46px;width:43px src=/images/beam_logo_navbar_mobile.png></a>
<a class=navbar-link href=/get-started/beam-overview/>Get Started</a>
<a class=navbar-link href=/documentation/>Documentation</a>
<button type=button class="navbar-toggle menu-open" aria-expanded=false aria-controls=navbar onclick=openMenu()>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button></div><div class="navbar-mask closed"></div><div id=navbar class="navbar-container closed"><button type=button class=navbar-toggle aria-expanded=false aria-controls=navbar id=closeMenu>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button><ul class="nav navbar-nav"><li><div class=searchBar-mobile><script>(function(){var cx='012923275103528129024:4emlchv9wzi';var gcse=document.createElement('script');gcse.type='text/javascript';gcse.async=true;gcse.src='https://cse.google.com/cse.js?cx='+cx;var s=document.getElementsByTagName('script')[0];s.parentNode.insertBefore(gcse,s);})();</script><gcse:search></gcse:search></div></li><li><a class=navbar-link href=/about>About</a></li><li><a class=navbar-link href=/get-started/beam-overview/>Get Started</a></li><li><span class=navbar-link>Documentation</span><ul><li><a href=/documentation/>General</a></li><li><a href=/documentation/sdks/java/>Languages</a></li><li><a href=/documentation/runners/capability-matrix/>Runners</a></li><li><a href=/documentation/io/connectors/>I/O Connectors</a></li></ul></li><li><a class=navbar-link href=/roadmap/>Roadmap</a></li><li><a class=navbar-link href=/community/>Community</a></li><li><a class=navbar-link href=/contribute/>Contribute</a></li><li><a class=navbar-link href=/blog/>Blog</a></li><li><a class=navbar-link href=/case-studies/>Case Studies</a></li></ul><ul class="nav navbar-nav navbar-right"><li><a href=https://github.com/apache/beam/edit/master/website/www/site/content/en/documentation/sdks/python-machine-learning.md data-proofer-ignore><svg xmlns="http://www.w3.org/2000/svg" width="25" height="24" fill="none" viewBox="0 0 25 24"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.75" d="M4.543 20h4l10.5-10.5c.53-.53.828-1.25.828-2s-.298-1.47-.828-2-1.25-.828-2-.828-1.47.298-2 .828L4.543 16v4zm9.5-13.5 4 4"/></svg></a></li><li class=dropdown><a href=# class=dropdown-toggle id=apache-dropdown data-toggle=dropdown role=button aria-haspopup=true aria-expanded=false><img src=https://www.apache.org/foundation/press/kit/feather_small.png alt="Apache Logo" style=height:20px>
&nbsp;Apache
<span class=arrow-icon><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="none" viewBox="0 0 20 20"><circle cx="10" cy="10" r="10" fill="#ff6d00"/><path stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8.535 5.28l4.573 4.818-4.573 4.403"/></svg></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a target=_blank href=http://www.apache.org/>ASF Homepage</a></li><li><a target=_blank href=http://www.apache.org/licenses/>License</a></li><li><a target=_blank href=http://www.apache.org/security/>Security</a></li><li><a target=_blank href=http://www.apache.org/foundation/thanks.html>Thanks</a></li><li><a target=_blank href=http://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li><li><a target=_blank href=https://www.apache.org/foundation/policies/conduct>Code of Conduct</a></li></ul></li></ul></div></nav><nav class=navigation-bar-desktop><a href=/ class=navbar-logo><img src=/images/beam_logo_navbar.png alt="Beam Logo"></a><div class=navbar-bar-left><div class=navbar-links><a class=navbar-link href=/about>About</a>
<a class=navbar-link href=/get-started/beam-overview/>Get Started</a><li class="dropdown navbar-dropdown navbar-dropdown-documentation"><a href=# class="dropdown-toggle navbar-link" role=button aria-haspopup=true aria-expanded=false>Documentation
<span><svg xmlns="http://www.w3.org/2000/svg" width="12" height="11" fill="none" viewBox="0 0 12 11"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10.666 4.535 5.847 9.108 1.444 4.535"/></svg></span></a><ul class=dropdown-menu><li><a class=navbar-dropdown-menu-link href=/documentation/>General</a></li><li><a class=navbar-dropdown-menu-link href=/documentation/sdks/java/>Languages</a></li><li><a class=navbar-dropdown-menu-link href=/documentation/runners/capability-matrix/>Runners</a></li><li><a class=navbar-dropdown-menu-link href=/documentation/io/connectors/>I/O Connectors</a></li></ul></li><a class=navbar-link href=/roadmap/>Roadmap</a>
<a class=navbar-link href=/community/>Community</a>
<a class=navbar-link href=/contribute/>Contribute</a>
<a class=navbar-link href=/blog/>Blog</a>
<a class=navbar-link href=/case-studies/>Case Studies</a></div><div id=iconsBar><a type=button onclick=showSearch()><svg xmlns="http://www.w3.org/2000/svg" width="25" height="24" fill="none" viewBox="0 0 25 24"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.75" d="M10.191 17c3.866.0 7-3.134 7-7s-3.134-7-7-7-7 3.134-7 7 3.134 7 7 7zM21.191 21l-6-6"/></svg></a><a target=_blank href=https://github.com/apache/beam/edit/master/website/www/site/content/en/documentation/sdks/python-machine-learning.md data-proofer-ignore><svg xmlns="http://www.w3.org/2000/svg" width="25" height="24" fill="none" viewBox="0 0 25 24"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.75" d="M4.543 20h4l10.5-10.5c.53-.53.828-1.25.828-2s-.298-1.47-.828-2-1.25-.828-2-.828-1.47.298-2 .828L4.543 16v4zm9.5-13.5 4 4"/></svg></a><li class="dropdown navbar-dropdown navbar-dropdown-apache"><a href=# class=dropdown-toggle role=button aria-haspopup=true aria-expanded=false><img src=https://www.apache.org/foundation/press/kit/feather_small.png alt="Apache Logo" style=height:20px>
&nbsp;Apache
<span class=arrow-icon><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" fill="none" viewBox="0 0 20 20"><circle cx="10" cy="10" r="10" fill="#ff6d00"/><path stroke="#fff" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8.535 5.28l4.573 4.818-4.573 4.403"/></svg></span></a><ul class=dropdown-menu><li><a class=navbar-dropdown-menu-link target=_blank href=http://www.apache.org/>ASF Homepage</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=http://www.apache.org/licenses/>License</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=http://www.apache.org/security/>Security</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=http://www.apache.org/foundation/thanks.html>Thanks</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=http://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li><li><a class=navbar-dropdown-menu-link target=_blank href=https://www.apache.org/foundation/policies/conduct>Code of Conduct</a></li></ul></li></div><div class="searchBar disappear"><script>(function(){var cx='012923275103528129024:4emlchv9wzi';var gcse=document.createElement('script');gcse.type='text/javascript';gcse.async=true;gcse.src='https://cse.google.com/cse.js?cx='+cx;var s=document.getElementsByTagName('script')[0];s.parentNode.insertBefore(gcse,s);})();</script><gcse:search></gcse:search><a type=button onclick=endSearch()><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" fill="none" viewBox="0 0 25 25"><path stroke="#ff6d00" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.75" d="M21.122 20.827 4.727 4.432M21.122 4.43 4.727 20.827"/></svg></a></div></div></nav><script>function showSearch(){addPlaceholder();var search=document.querySelector(".searchBar");search.classList.remove("disappear");var icons=document.querySelector("#iconsBar");icons.classList.add("disappear");}
function addPlaceholder(){$('input:text').attr('placeholder',"What are you looking for?");}
function endSearch(){var search=document.querySelector(".searchBar");search.classList.add("disappear");var icons=document.querySelector("#iconsBar");icons.classList.remove("disappear");}
function blockScroll(){$("body").toggleClass("fixedPosition");}
function openMenu(){addPlaceholder();blockScroll();}</script><div class="clearfix container-main-content"><div class="section-nav closed" data-offset-top=90 data-offset-bottom=500><span class="section-nav-back glyphicon glyphicon-menu-left"></span><nav><ul class=section-nav-list data-section-nav><li><span class=section-nav-list-main-title>Languages</span></li><li><span class=section-nav-list-title>Java</span><ul class=section-nav-list><li><a href=/documentation/sdks/java/>Java SDK overview</a></li><li><a href=https://beam.apache.org/releases/javadoc/2.41.0/ target=_blank>Java SDK API reference <img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a></li><li><a href=/documentation/sdks/java-dependencies/>Java SDK dependencies</a></li><li><a href=/documentation/sdks/java-extensions/>Java SDK extensions</a></li><li><a href=/documentation/sdks/java-thirdparty/>Java 3rd party extensions</a></li><li><a href=/documentation/sdks/java/testing/nexmark/>Nexmark benchmark suite</a></li><li><a href=/documentation/sdks/java/testing/tpcds/>TPC-DS benchmark suite</a></li><li><a href=/documentation/sdks/java-multi-language-pipelines/>Java multi-language pipelines quickstart</a></li></ul></li><li><span class=section-nav-list-title>Python</span><ul class=section-nav-list><li><a href=/documentation/sdks/python/>Python SDK overview</a></li><li><a href=https://beam.apache.org/releases/pydoc/2.41.0/ target=_blank>Python SDK API reference <img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a></li><li><a href=/documentation/sdks/python-dependencies/>Python SDK dependencies</a></li><li><a href=/documentation/sdks/python-streaming/>Python streaming pipelines</a></li><li><a href=/documentation/sdks/python-type-safety/>Ensuring Python type safety</a></li><li><a href=/documentation/sdks/python-machine-learning/>Machine Learning</a></li><li><a href=/documentation/sdks/python-pipeline-dependencies/>Managing pipeline dependencies</a></li><li><a href=/documentation/sdks/python-multi-language-pipelines/>Python multi-language pipelines quickstart</a></li></ul></li><li><span class=section-nav-list-title>Go</span><ul class=section-nav-list><li><a href=/documentation/sdks/go/>Go SDK overview</a></li><li><a href=https://pkg.go.dev/github.com/apache/beam/sdks/v2/go/pkg/beam target=_blank>Go SDK API reference <img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a><li><a href=/documentation/sdks/go-dependencies/>Go SDK dependencies</a></li><li><a href=/documentation/sdks/go-cross-compilation/>Cross compilation</a></li></li></ul></li><li><span class=section-nav-list-title>SQL</span><ul class=section-nav-list><li><a href=/documentation/dsls/sql/overview/>Overview</a></li><li><a href=/documentation/dsls/sql/walkthrough/>Walkthrough</a></li><li><a href=/documentation/dsls/sql/shell/>Shell</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Apache Calcite dialect</span><ul class=section-nav-list><li><a href=/documentation/dsls/sql/calcite/overview/>Calcite support overview</a></li><li><a href=/documentation/dsls/sql/calcite/query-syntax/>Query syntax</a></li><li><a href=/documentation/dsls/sql/calcite/lexical/>Lexical structure</a></li><li><a href=/documentation/dsls/sql/calcite/data-types/>Data types</a></li><li><a href=/documentation/dsls/sql/calcite/scalar-functions/>Scalar functions</a></li><li><a href=/documentation/dsls/sql/calcite/aggregate-functions/>Aggregate functions</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>ZetaSQL dialect</span><ul class=section-nav-list><li><a href=/documentation/dsls/sql/zetasql/overview/>ZetaSQL support overview</a></li><li><a href=/documentation/dsls/sql/zetasql/syntax/>Function call rules</a></li><li><a href=/documentation/dsls/sql/zetasql/conversion-rules/>Conversion rules</a></li><li><a href=/documentation/dsls/sql/zetasql/query-syntax/>Query syntax</a></li><li><a href=/documentation/dsls/sql/zetasql/lexical/>Lexical structure</a></li><li><a href=/documentation/dsls/sql/zetasql/data-types/>Data types</a></li><li><a href=/documentation/dsls/sql/zetasql/operators/>Operators</a></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Scalar functions</span><ul class=section-nav-list><li><a href=/documentation/dsls/sql/zetasql/string-functions/>String functions</a></li><li><a href=/documentation/dsls/sql/zetasql/math-functions/>Mathematical functions</a></li><li><a href=/documentation/dsls/sql/zetasql/conditional-expressions/>Conditional expressions</a></li></ul></li><li><a href=/documentation/dsls/sql/zetasql/aggregate-functions/>Aggregate functions</a></li></ul></li><li class=section-nav-item--collapsible><span class=section-nav-list-title>Beam SQL extensions</span><ul class=section-nav-list><li><a href=/documentation/dsls/sql/extensions/create-external-table/>CREATE EXTERNAL TABLE</a></li><li><a href=/documentation/dsls/sql/extensions/windowing-and-triggering/>Windowing & triggering</a></li><li><a href=/documentation/dsls/sql/extensions/joins/>Joins</a></li><li><a href=/documentation/dsls/sql/extensions/user-defined-functions/>User-defined functions</a></li><li><a href=/documentation/dsls/sql/extensions/set/>SET pipeline options</a></li></ul></li></ul></li><li><span class=section-nav-list-title>DataFrames</span><ul class=section-nav-list><li><a href=/documentation/dsls/dataframes/overview/>Overview</a></li><li><a href=/documentation/dsls/dataframes/differences-from-pandas/>Differences from pandas</a></li><li><a href=https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/dataframe target=_blank>Example pipelines <img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a></li><li><a href=https://beam.apache.org/releases/pydoc/2.41.0/apache_beam.dataframe.html target=_blank>DataFrame API reference <img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a></li></ul></li></ul></nav></div><nav class="page-nav clearfix" data-offset-top=90 data-offset-bottom=500><nav id=TableOfContents><ul><li><a href=#why-use-the-runinference-api>Why use the RunInference API?</a><ul><li><a href=#batchelements-ptransform>BatchElements PTransform</a></li><li><a href=#shared-helper-class>Shared helper class</a></li><li><a href=#multi-model-pipelines>Multi-model pipelines</a></li></ul></li><li><a href=#modify-a-pipeline-to-use-an-ml-model>Modify a pipeline to use an ML model</a><ul><li><a href=#use-pre-trained-models>Use pre-trained models</a><ul><li><a href=#pytorch>PyTorch</a></li><li><a href=#scikit-learn>Scikit-learn</a></li></ul></li><li><a href=#use-multiple-models>Use multiple models</a><ul><li><a href=#ab-pattern>A/B Pattern</a></li><li><a href=#ensemble-pattern>Ensemble Pattern</a></li></ul></li><li><a href=#use-a-keyed-modelhandler>Use a keyed ModelHandler</a></li><li><a href=#use-the-predictionresults-object>Use the PredictionResults object</a></li></ul></li><li><a href=#run-a-machine-learning-pipeline>Run a machine learning pipeline</a></li><li><a href=#beam-java-sdk-support>Beam Java SDK support</a></li><li><a href=#tensorflow-support>TensorFlow support</a></li><li><a href=#troubleshooting>Troubleshooting</a><ul><li><a href=#incorrect-inferences-in-the-predictionresult-object>Incorrect inferences in the PredictionResult object</a></li><li><a href=#unable-to-batch-tensor-elements>Unable to batch tensor elements</a></li></ul></li><li><a href=#related-links>Related links</a></li></ul></nav></nav><div class="body__contained body__section-nav"><h1 id=machine-learning>Machine Learning</h1><table align=left style=margin-right:1em><td><a class=button target=_blank href=https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.html#apache_beam.ml.inference.RunInference><img src=https://beam.apache.org/images/logos/sdks/python.png width=32px height=32px alt=Pydoc>
Pydoc</a></td></table><p><br><br><br></p><p>You can use Apache Beam with the RunInference API to use machine learning (ML) models to do local and remote inference with batch and streaming pipelines. Starting with Apache Beam 2.40.0, PyTorch and Scikit-learn frameworks are supported. You can create multiple types of transforms using the RunInference API: the API takes multiple types of setup parameters from model handlers, and the parameter type determines the model implementation.</p><h2 id=why-use-the-runinference-api>Why use the RunInference API?</h2><p>RunInference takes advantage of existing Apache Beam concepts, such as the <code>BatchElements</code> transform and the <code>Shared</code> class, to enable you to use models in your pipelines to create transforms optimized for machine learning inferences. The ability to create arbitrarily complex workflow graphs also allows you to build multi-model pipelines.</p><h3 id=batchelements-ptransform>BatchElements PTransform</h3><p>To take advantage of the optimizations of vectorized inference that many models implement, we added the <code>BatchElements</code> transform as an intermediate step before making the prediction for the model. This transform batches elements together. The batched elements are then applied with a transformation for the particular framework of RunInference. For example, for numpy <code>ndarrays</code>, we call <code>numpy.stack()</code>, and for torch <code>Tensor</code> elements, we call <code>torch.stack()</code>.</p><p>To customize the settings for <code>beam.BatchElements</code>, in <code>ModelHandler</code>, override the <code>batch_elements_kwargs</code> function. For example, use <code>min_batch_size</code> to set the lowest number of elements per batch or <code>max_batch_size</code> to set the highest number of elements per batch.</p><p>For more information, see the <a href=https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.util.html#apache_beam.transforms.util.BatchElements><code>BatchElements</code> transform documentation</a>.</p><h3 id=shared-helper-class>Shared helper class</h3><p>Using the <code>Shared</code> class within the RunInference implementation makes it possible to load the model only once per process and share it with all DoFn instances created in that process. This feature reduces memory consumption and model loading time. For more information, see the
<a href=https://github.com/apache/beam/blob/master/sdks/python/apache_beam/utils/shared.py#L20><code>Shared</code> class documentation</a>.</p><h3 id=multi-model-pipelines>Multi-model pipelines</h3><p>The RunInference API can be composed into multi-model pipelines. Multi-model pipelines can be useful for A/B testing or for building out ensembles made up of models that perform tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, language detection, coreference resolution, and more.</p><h2 id=modify-a-pipeline-to-use-an-ml-model>Modify a pipeline to use an ML model</h2><p>To use the RunInference transform, add the following code to your pipeline:</p><pre><code>from apache_beam.ml.inference.base import RunInference
with pipeline as p:
   predictions = ( p |  'Read' &gt;&gt; beam.ReadFromSource('a_source')
                     | 'RunInference' &gt;&gt; RunInference(&lt;model_handler&gt;)
</code></pre><p>Where <code>model_handler</code> is the model handler setup code.</p><p>To import models, you need to configure a <code>ModelHandler</code> object that wraps the underlying model. Which <code>ModelHandler</code> you import depends on the framework and type of data structure that contains the inputs. The following examples show some ModelHandlers that you might want to import.</p><pre><code>from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy
from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerPandas
from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor
from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerKeyedTensor
</code></pre><h3 id=use-pre-trained-models>Use pre-trained models</h3><p>The section provides requirements for using pre-trained models with PyTorch and Scikit-learn</p><h4 id=pytorch>PyTorch</h4><p>You need to provide a path to a file that contains the model&rsquo;s saved weights. This path must be accessible by the pipeline. To use pre-trained models with the RunInference API and the PyTorch framework, complete the following steps:</p><ol><li>Download the pre-trained weights and host them in a location that the pipeline can access.</li><li>Pass the path of the model weights to the PyTorch <code>ModelHandler</code> by using the following code: <code>state_dict_path=&lt;path_to_weights></code>.</li></ol><h4 id=scikit-learn>Scikit-learn</h4><p>You need to provide a path to a file that contains the pickled Scikit-learn model. This path must be accessible by the pipeline. To use pre-trained models with the RunInference API and the Scikit-learn framework, complete the following steps:</p><ol><li>Download the pickled model class and host it in a location that the pipeline can access.</li><li>Pass the path of the model to the Sklearn <code>ModelHandler</code> by using the following code:
<code>model_uri=&lt;path_to_pickled_file></code> and <code>model_file_type: &lt;ModelFileType></code>, where you can specify
<code>ModelFileType.PICKLE</code> or <code>ModelFileType.JOBLIB</code>, depending on how the model was serialized.</li></ol><h3 id=use-multiple-models>Use multiple models</h3><p>You can also use the RunInference transform to add multiple inference models to your pipeline.</p><h4 id=ab-pattern>A/B Pattern</h4><pre><code>with pipeline as p:
   data = p | 'Read' &gt;&gt; beam.ReadFromSource('a_source')
   model_a_predictions = data | RunInference(&lt;model_handler_A&gt;)
   model_b_predictions = data | RunInference(&lt;model_handler_B&gt;)
</code></pre><p>Where <code>model_handler_A</code> and <code>model_handler_B</code> are the model handler setup code.</p><h4 id=ensemble-pattern>Ensemble Pattern</h4><pre><code>with pipeline as p:
   data = p | 'Read' &gt;&gt; beam.ReadFromSource('a_source')
   model_a_predictions = data | RunInference(&lt;model_handler_A&gt;)
   model_b_predictions = model_a_predictions | beam.Map(some_post_processing) | RunInference(&lt;model_handler_B&gt;)
</code></pre><p>Where <code>model_handler_A</code> and <code>model_handler_B</code> are the model handler setup code.</p><h3 id=use-a-keyed-modelhandler>Use a keyed ModelHandler</h3><p>If a key is attached to the examples, wrap the <code>KeyedModelHandler</code> around the <code>ModelHandler</code> object:</p><pre><code>from apache_beam.ml.inference.base import KeyedModelHandler
keyed_model_handler = KeyedModelHandler(PytorchModelHandlerTensor(...))
with pipeline as p:
   data = p | beam.Create([
      ('img1', torch.tensor([[1,2,3],[4,5,6],...])),
      ('img2', torch.tensor([[1,2,3],[4,5,6],...])),
      ('img3', torch.tensor([[1,2,3],[4,5,6],...])),
   ])
   predictions = data | RunInference(keyed_model_handler)
</code></pre><h3 id=use-the-predictionresults-object>Use the PredictionResults object</h3><p>When doing a prediction in Apache Beam, the output <code>PCollection</code> includes both the keys of the input examples and the inferences. Including both these items in the output allows you to find the input that determined the predictions.</p><p>The <code>PredictionResult</code> is a <code>NamedTuple</code> object that contains both the input and the inferences, named <code>example</code> and <code>inference</code>, respectively. When keys are passed with the input data to the RunInference transform, the output <code>PCollection</code> returns a <code>Tuple[str, PredictionResult]</code>, which is the key and the <code>PredictionResult</code> object. Your pipeline interacts with a <code>PredictionResult</code> object in steps after the RunInference transform.</p><pre><code>class PostProcessor(beam.DoFn):
    def process(self, element: Tuple[str, PredictionResult]):
       key, prediction_result = element
       inputs = prediction_result.example
       predictions = prediction_result.inference

       # Post-processing logic
       result = ...

       yield (key, result)

with pipeline as p:
    output = (
        p | 'Read' &gt;&gt; beam.ReadFromSource('a_source')
                | 'PyTorchRunInference' &gt;&gt; RunInference(&lt;keyed_model_handler&gt;)
                | 'ProcessOutput' &gt;&gt; beam.ParDo(PostProcessor()))
</code></pre><p>If you need to use this object explicitly, include the following line in your pipeline to import the object:</p><pre><code>from apache_beam.ml.inference.base import PredictionResult
</code></pre><p>For more information, see the <a href=https://github.com/apache/beam/blob/master/sdks/python/apache_beam/ml/inference/base.py#L65><code>PredictionResult</code> documentation</a>.</p><h2 id=run-a-machine-learning-pipeline>Run a machine learning pipeline</h2><p>For detailed instructions explaining how to build and run a pipeline that uses ML models, see the
<a href=https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/inference>Example RunInference API pipelines</a> on GitHub.</p><h2 id=beam-java-sdk-support>Beam Java SDK support</h2><p>The RunInference API is available with the Beam Java SDK versions 2.41.0 and later through Apache Beam&rsquo;s <a href=https://beam.apache.org/documentation/programming-guide/#multi-language-pipelines>Multi-language Pipelines framework</a>. For information about the Java wrapper transform, see <a href=https://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/main/java/org/apache/beam/sdk/extensions/python/transforms/RunInference.java>RunInference.java</a>. For example pipelines, see <a href=https://github.com/apache/beam/blob/master/sdks/java/extensions/python/src/test/java/org/apache/beam/sdk/extensions/python/transforms/RunInferenceTransformTest.java>RunInferenceTransformTest.java</a>.</p><h2 id=tensorflow-support>TensorFlow support</h2><p>To use TensorFlow with the RunInference API, you need to do the following:</p><ul><li>Use <code>tfx_bsl</code> version 1.10.0 or later.</li><li>Create a model handler using <code>tfx_bsl.public.beam.run_inference.CreateModelHandler()</code>.</li><li>Use the model handler with the <a href=/releases/pydoc/current/apache_beam.ml.inference.base.html><code>apache_beam.ml.inference.base.RunInference</code></a> transform.</li></ul><p>A sample pipeline might look like the following example:</p><pre><code>import apache_beam as beam
from apache_beam.ml.inference.base import RunInference
from tensorflow_serving.apis import prediction_log_pb2
from tfx_bsl.public.proto import model_spec_pb2
from tfx_bsl.public.tfxio import TFExampleRecord
from tfx_bsl.public.beam.run_inference import CreateModelHandler

pipeline = beam.Pipeline()
tfexample_beam_record = TFExampleRecord(file_pattern='/path/to/examples')
saved_model_spec = model_spec_pb2.SavedModelSpec(model_path='/path/to/model')
inference_spec_type = model_spec_pb2.InferenceSpecType(saved_model_spec=saved_model_spec)
model_handler = CreateModelHandler(inference_spec_type)
with pipeline as p:
    _ = (p | tfexample_beam_record.RawRecordBeamSource()
           | RunInference(model_handler)
           | beam.Map(print)
        )
</code></pre><p>The model handler that is created with <code>CreateModelHander()</code> is always unkeyed. To make a keyed model handler, wrap the unkeyed model handler in the keyed model handler, which would then take the <code>tfx-bsl</code> model handler as a parameter. For example:</p><pre><code>from apache_beam.ml.inference.base import RunInference
from apache_beam.ml.inference.base import KeyedModelHandler
RunInference(KeyedModelHandler(tf_handler))
</code></pre><p>If you are unsure if your data is keyed, you can also use <code>MaybeKeyedModelHandler</code>.</p><p>For more information, see <a href=https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.KeyedModelHandler><code>KeyedModelHander</code></a>.</p><h2 id=troubleshooting>Troubleshooting</h2><p>If you run into problems with your pipeline or job, this section lists issues that you might encounter and provides suggestions for how to fix them.</p><h3 id=incorrect-inferences-in-the-predictionresult-object>Incorrect inferences in the PredictionResult object</h3><p>In some cases, the <code>PredictionResults</code> output might not include the correct predictions in the <code>inferences</code> field. This issue occurs when you use a model whose inferences return a dictionary that maps keys to predictions and other metadata. An example return type is <code>Dict[str, Tensor]</code>.</p><p>The RunInference API currently expects outputs to be an <code>Iterable[Any]</code>. Example return types are <code>Iterable[Tensor]</code> or <code>Iterable[Dict[str, Tensor]]</code>. When RunInference zips the inputs with the predictions, the predictions iterate over the dictionary keys instead of the batch elements. The result is that the key name is preserved but the prediction tensors are discarded. For more information, see the <a href=https://github.com/apache/beam/issues/22240>Pytorch RunInference PredictionResult is a Dict</a> issue in the Apache Beam GitHub project.</p><p>To work with the current RunInference implementation, you can create a wrapper class that overrides the <code>model(input)</code> call. In PyTorch, for example, your wrapper would override the <code>forward()</code> function and return an output with the appropriate format of <code>List[Dict[str, torch.Tensor]]</code>. For more information, see the <a href=https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/inference/pytorch_language_modeling.py#L49>HuggingFace language modeling example</a>.</p><h3 id=unable-to-batch-tensor-elements>Unable to batch tensor elements</h3><p>RunInference uses dynamic batching. However, the RunInference API cannot batch tensor elements of different sizes, so samples passed to the RunInferene transform must be the same dimension or length. If you provide images of different sizes or word embeddings of different lengths, the following error might occur:</p><p><code>File "/beam/sdks/python/apache_beam/ml/inference/pytorch_inference.py", line 232, in run_inference batched_tensors = torch.stack(key_to_tensor_list[key]) RuntimeError: stack expects each tensor to be equal size, but got [12] at entry 0 and [10] at entry 1 [while running 'PyTorchRunInference/ParDo(_RunInferenceDoFn)']</code></p><p>To avoid this issue, either use elements of the same size, or disable batching.</p><p><strong>Option 1: Use elements of the same size</strong></p><p>Use elements of the same size or resize the inputs. For computer vision applications, resize image inputs so that they have the same dimensions. For natural language processing (NLP) applications that have text of varying length, resize the text or word embeddings to make them the same length. When working with texts of varying length, resizing might not be possible. In this scenario, you could disable batching (see option 2).</p><p><strong>Option 2: Disable batching</strong></p><p>Disable batching by overriding the <code>batch_elements_kwargs</code> function in your ModelHandler and setting the maximum batch size (<code>max_batch_size</code>) to one: <code>max_batch_size=1</code>. For more information, see
<a href=/documentation/sdks/python-machine-learning/#batchelements-ptransform>BatchElements PTransforms</a>. For an example, see our <a href=https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/inference/pytorch_language_modeling.py>language modeling example</a>.</p><h2 id=related-links>Related links</h2><ul><li><a href=/documentation/transforms/python/elementwise/runinference>RunInference transforms</a></li><li><a href=https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/inference>RunInference API pipeline examples</a></li><li><a href=https://colab.sandbox.google.com/github/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_basic.ipynb>RunInference public codelab</a></li><li><a href=https://github.com/apache/beam/tree/master/examples/notebooks/beam-ml>RunInference notebooks</a></li></ul><table align=left style=margin-right:1em><td><a class=button target=_blank href=https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.html#apache_beam.ml.inference.RunInference><img src=https://beam.apache.org/images/logos/sdks/python.png width=32px height=32px alt=Pydoc>
Pydoc</a></td></table><p><br><br><br></p></div></div><footer class=footer><div class=footer__contained><div class=footer__cols><div class="footer__cols__col footer__cols__col__logos"><div class=footer__cols__col__logo><img src=/images/beam_logo_circle.svg class=footer__logo alt="Beam logo"></div><div class=footer__cols__col__logo><img src=/images/apache_logo_circle.svg class=footer__logo alt="Apache logo"></div></div><div class=footer-wrapper><div class=wrapper-grid><div class=footer__cols__col><div class=footer__cols__col__title>Start</div><div class=footer__cols__col__link><a href=/get-started/beam-overview/>Overview</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-java/>Quickstart (Java)</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-py/>Quickstart (Python)</a></div><div class=footer__cols__col__link><a href=/get-started/quickstart-go/>Quickstart (Go)</a></div><div class=footer__cols__col__link><a href=/get-started/downloads/>Downloads</a></div></div><div class=footer__cols__col><div class=footer__cols__col__title>Docs</div><div class=footer__cols__col__link><a href=/documentation/programming-guide/>Concepts</a></div><div class=footer__cols__col__link><a href=/documentation/pipelines/design-your-pipeline/>Pipelines</a></div><div class=footer__cols__col__link><a href=/documentation/runners/capability-matrix/>Runners</a></div></div><div class=footer__cols__col><div class=footer__cols__col__title>Community</div><div class=footer__cols__col__link><a href=/contribute/>Contribute</a></div><div class=footer__cols__col__link><a href=https://projects.apache.org/committee.html?beam target=_blank>Team<img src=/images/external-link-icon.png width=14 height=14 alt="External link."></a></div><div class=footer__cols__col__link><a href=/community/presentation-materials/>Media</a></div><div class=footer__cols__col__link><a href=/community/in-person/>Events/Meetups</a></div></div><div class=footer__cols__col><div class=footer__cols__col__title>Resources</div><div class=footer__cols__col__link><a href=/blog/>Blog</a></div><div class=footer__cols__col__link><a href=/community/contact-us/>Contact Us</a></div><div class=footer__cols__col__link><a href=https://github.com/apache/beam>GitHub</a></div></div></div><div class=footer__bottom>&copy;
<a href=http://www.apache.org>The Apache Software Foundation</a>
| <a href=/privacy_policy>Privacy Policy</a>
| <a href=/feed.xml>RSS Feed</a><br><br>Apache Beam, Apache, Beam, the Beam logo, and the Apache feather logo are either registered trademarks or trademarks of The Apache Software Foundation. All other products or name brands are trademarks of their respective holders, including The Apache Software Foundation.</div></div><div class="footer__cols__col footer__cols__col__logos"><div class=footer__cols__col--group><div class=footer__cols__col__logo><a href=https://github.com/apache/beam><img src=/images/logos/social-icons/github-logo-150.png class=footer__logo alt="Github logo"></a></div><div class=footer__cols__col__logo><a href=https://www.linkedin.com/company/apache-beam/><img src=/images/logos/social-icons/linkedin-logo-150.png class=footer__logo alt="Linkedin logo"></a></div></div><div class=footer__cols__col--group><div class=footer__cols__col__logo><a href=https://twitter.com/apachebeam><img src=/images/logos/social-icons/twitter-logo-150.png class=footer__logo alt="Twitter logo"></a></div><div class=footer__cols__col__logo><a href=https://www.youtube.com/channel/UChNnb_YO_7B0HlW6FhAXZZQ><img src=/images/logos/social-icons/youtube-logo-150.png class=footer__logo alt="Youtube logo"></a></div></div></div></div></div></footer></body></html>