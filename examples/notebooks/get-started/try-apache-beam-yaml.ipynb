{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Try Apache Beam - Python",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python2",
   "display_name": "Python 2"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/get-started/try-apache-beam-yaml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
    "\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements. See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership. The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License. You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied. See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License."
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form"
   }
  },
  {
   "metadata": {
    "id": "lNKIMlEDZ_Vw",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "# Try Apache Beam - YAML\n",
    "\n",
    "While Beam provides powerful APIs for authoring sophisticated data processing pipelines, it still has a high barrier for getting started and authoring simple pipelines. Even setting up the environment, installing the dependencies, and setting up the project can be a challenge.\n",
    "\n",
    "Here we provide a simple YAML syntax for describing pipelines that does not require coding experience or learning how to use an SDK. You can use any text editor.\n",
    "\n",
    "Please note: YAML API is still EXPERIMENTAL and subject to change.\n",
    "\n",
    "In this notebook, you set up your development environment and write a simple pipeline using YAML. Then you run it locally, using the [DirectRunner](https://beam.apache.org/documentation/runners/direct/). You can explore other runners with the [Beam Capatibility Matrix](https://beam.apache.org/documentation/runners/capability-matrix/).\n",
    "\n",
    "To navigate through different sections, use the table of contents. From **View**  drop-down list, select **Table of contents**.\n",
    "\n",
    "To run a code cell, click the **Run cell** button at the top left of the cell, or select it and press **`Shift+Enter`**. Try modifying a code cell and re-running it to see what happens.\n",
    "\n",
    "To learn more about Colab, see [Welcome to Colaboratory!](https://colab.sandbox.google.com/notebooks/welcome.ipynb)."
   ]
  },
  {
   "metadata": {
    "id": "Fz6KSQ13_3Rr",
    "colab_type": "text"
   },
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "First, you need to set up your environment. The following code installs `apache-beam` and downloads some text files from Cloud Storage to your local file system. We'll use these text files as input to the pipelines."
   ]
  },
  {
   "metadata": {
    "id": "GOOk81Jj_yUy",
    "colab_type": "code",
    "outputId": "d283dfb2-4f51-4fec-816b-f57b0cb9b71c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    }
   },
   "cell_type": "code",
   "source": [
    "# Run and print a shell command.\n",
    "def run(cmd):\n",
    "  print('>> {}'.format(cmd))\n",
    "  !{cmd}\n",
    "  print('')\n",
    "\n",
    "def save_to_file(content, file_name):\n",
    "  with open(file_name, 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "# Install apache-beam.\n",
    "run('pip install --quiet apache-beam')\n",
    "\n",
    "# Copy the input files into the local file system.\n",
    "run('mkdir -p data')\n",
    "run('wget -O data/kinglear.txt https://storage.googleapis.com/dataflow-samples/shakespeare/kinglear.txt')\n",
    "run('wget -O data/SMSSpamCollection.csv https://storage.googleapis.com/apache-beam-samples/SMSSpamCollection/SMSSpamCollection')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspect the data\n",
    "We'll be working with two datasets. We'll use `kinglear.txt` for the first example, and `SMSSpamCollection.csv` for the second and third examples.\n",
    "Let's first take a look at the `kinglear.txt` dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run('head data/kinglear.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is just a `txt` file that contains lines of text.\n",
    "Let's take a look at the other dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run('head data/SMSSpamCollection.csv')\n",
    "run('wc -l data/SMSSpamCollection.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This dataset is a `csv` file that contains 5,574 rows of SMS messages labeled either spam or not-spam (\"ham\"). Each row contains two columns separated by a tab character:\n",
    "1. `Column 1`: The label, either `ham` or `spam`\n",
    "2. `Column 2`: The SMS message as raw text (type `string`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 1: Word count\n",
    "This example is a version of the [WordCount](https://beam.apache.org/get-started/wordcount-example/)). It reads lines of text from the input dataset `kinglear.txt` and counts the number of times each word appears in the text.\n",
    "To start, we'll create a `.yaml` file specifying our pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = '''\n",
    "pipeline:\n",
    "  # Read input data. Each line from the txt file is a String.\n",
    "  - type: ReadFromText\n",
    "    name: InputText\n",
    "    config:\n",
    "      file_pattern: data/kinglear.txt\n",
    "\n",
    "  # Using a regex, we'll split the content of the message (one long string) into words (list of strings).\n",
    "  # The 'fn' parameter accepts functions written in Python\n",
    "  - type: PyFlatMap\n",
    "    name: FindWords\n",
    "    input: InputText\n",
    "    config:\n",
    "      fn: |\n",
    "        import re\n",
    "        lambda line: re.findall(r\"[a-zA-Z]+\", line)\n",
    "\n",
    "  # Transforming each word to lower case and combining it with a '1'. Result of this step are pairs (word: 1).\n",
    "  - type: PyMap\n",
    "    name: PairWordsWith1\n",
    "    input: FindWords\n",
    "    config:\n",
    "      fn: 'lambda word: (word, 1)'\n",
    "\n",
    "  # Using CombinePerKey transform with the 'sum' function as a combine function,\n",
    "  # we'll calculate the occurrence of each word.\n",
    "  - type: CombinePerKey\n",
    "    config:\n",
    "      combine_fn: sum\n",
    "    name: GroupAndSum\n",
    "    input: PairWordsWith1\n",
    "\n",
    "  # Format results - each record should be represented as 'word: count'.\n",
    "  # The 'fn' parameter accepts functions written in Python\n",
    "  - type: PyMap\n",
    "    name: FormatResults\n",
    "    input: GroupAndSum\n",
    "    config:\n",
    "      fn: \"lambda word_count_tuple: f'{word_count_tuple[0]}: {word_count_tuple[1]}'\"\n",
    "\n",
    "  # Save results to a text file.\n",
    "  - type: WriteToText\n",
    "    name: SaveToText\n",
    "    input: FormatResults\n",
    "    config:\n",
    "      file_path_prefix: \"data/result-pipeline-01\"\n",
    "      file_name_suffix: \".txt\"\n",
    "'''\n",
    "save_to_file(pipeline, 'pipeline-01.yaml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each pipeline specification must start with a `pipeline` key on the first line.\n",
    "The `pipeline` keyh is followed by a list of transforms. For example, the first transform reads the input file:\n",
    "```\n",
    "  # Read input data. Each line from the csv file is a String.\n",
    "  - type: ReadFromText\n",
    "    name: InputText\n",
    "    config:\n",
    "      file_pattern: data/kinglear.txt\n",
    "```\n",
    "Note: The indentation is important, because it specifies object hierarchy.\n",
    "YAML supports comments. Everything after the `#` is always treated as a comment. Use them to improve readability.\n",
    "\n",
    "Each operation must specify the `type` descriptor and other fields, such as `name` and other transform-specific parameters.\n",
    "For a list of available transforms and their parameters, see the YAML API documentation. # todo(yaml) add link\n",
    "\n",
    "To link two operations, use the `input` field. The `input` field specifies the name of another transform.\n",
    "For example, the third operation in this pipeline takes the `FindWords` transform as input:\n",
    "```\n",
    "  # Transforming each word to lower case and combining it with a '1'. Result of this step are pairs (word: 1).\n",
    "  - type: PyMap\n",
    "    name: PairWordsWith1\n",
    "    input: FindWords\n",
    "    config:\n",
    "      fn: 'lambda word: (word, 1)'\n",
    "```\n",
    "This particular operation takes `fn` (which stands for function) as an argument. Currently only Python functions are supported.\n",
    "\n",
    "For more complicated functions, you can take advantage of YAML's multiline feature, as show in the second operation:\n",
    "```\n",
    "  # Using a regex, we'll split the content of the message (one long string) into words (list of strings).\n",
    "  - type: PyFlatMap\n",
    "    name: FindWords\n",
    "    input: InputText\n",
    "    config:\n",
    "      fn: |\n",
    "        import re\n",
    "        lambda line: re.findall(r\"[a-zA-Z]+\", line)\n",
    "```\n",
    "In this trasnform, we need to import Python's regex package, `re`. To do that, we use the '|' character to start a multiline string.\n",
    "This lets us write the function across two lines.\n",
    "\n",
    "Let's run the pipeline executing the Python entry-point script (`apache_beam.yaml.main`) with our pipeline file as an argument:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run('python -m apache_beam.yaml.main --pipeline_spec_file=pipeline-01.yaml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's inspect the results. Each line contains a word and an associated count."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run('head data/result-pipeline-01-00000-of-00001.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 2: Load data, filter unwanted lines, and save results to a text file.\n",
    "This example creates a pipeline that loads a `.csv` file containing SMS messages, filters out valid messages, and saves the spam messages to a file.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = '''\n",
    "pipeline:\n",
    "  # Read input data. Each line from the csv file is a String.\n",
    "  - type: ReadFromText\n",
    "    name: SmsData\n",
    "    config:\n",
    "      file_pattern: data/SMSSpamCollection.csv\n",
    "\n",
    "  # Split each line into an array, where the first element is message label (ham or spam) and the second is the content of the message.\n",
    "  - type: PyMap\n",
    "    name: SplitLine\n",
    "    input: SmsData\n",
    "    config:\n",
    "      fn: 'lambda line: line.split(\"\\\\t\")'\n",
    "\n",
    "  # Keep only the rows that contain spam messages, based on the first element in the array - the label.\n",
    "  - type: PyFilter\n",
    "    name: KeepSpam\n",
    "    input: SplitLine\n",
    "    config:\n",
    "      keep: 'lambda row: row[0] == \"spam\"' # this is a function in Python, similar to the 'fn' in the previous example.\n",
    "\n",
    "  # Save only the rows from the input file that are classified as spam.\n",
    "  - type: WriteToText\n",
    "    name: SaveToText\n",
    "    input: KeepSpam\n",
    "    config:\n",
    "      file_path_prefix: \"data/result-pipeline-02\"\n",
    "      file_name_suffix: \".txt\"\n",
    "'''\n",
    "save_to_file(pipeline, 'pipeline-02.yaml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's run the pipeline with our `.yaml` file as an input:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run('python -m apache_beam.yaml.main --pipeline_spec_file=pipeline-02.yaml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verify the results and see the content of the output file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run('head data/result-pipeline-02-00000-of-00001.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should see only spam messages from the input dataset. Congratulations, onto the next example!\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example 3: Count words in spam messages, select the top 10 popular words, and write the results to a file.\n",
    "\n",
    "This example counts words occurring in spam messages, selects the most popular words, and writes the result to a file.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline = '''\n",
    "pipeline:\n",
    "  # Read input data. Each line from the csv file is a String.\n",
    "  - type: ReadFromText\n",
    "    name: SmsData\n",
    "    config:\n",
    "      file_pattern: data/SMSSpamCollection.csv\n",
    "\n",
    "  # Split each line into an array, where the first element is message label (ham or spam) and the second is the content of the message.\n",
    "  - type: PyMap\n",
    "    name: SplitLine\n",
    "    input: SmsData\n",
    "    config:\n",
    "      fn: 'lambda line: line.split(\"\\\\t\")'\n",
    "\n",
    "  # Keep only the rows that contain spam messages, based on the first element in the array - the label.\n",
    "  - type: PyFilter\n",
    "    name: SpamMessages\n",
    "    input: SplitLine\n",
    "    config:\n",
    "      keep: 'lambda row: row[0] == \"spam\"'\n",
    "\n",
    "  # Using a regex, we'll split the content of the message (one long string) into words (list of strings)\n",
    "  - type: PyFlatMap\n",
    "    name: FindWords\n",
    "    input: SpamMessages\n",
    "    config:\n",
    "      fn: |\n",
    "        import re\n",
    "        lambda line: re.findall(r\"[a-zA-Z]+\", line[1])\n",
    "\n",
    "  # Transforming each word to lower case and combining it with a '1'. Result of this step are pairs (word: 1).\n",
    "  - type: PyMap\n",
    "    name: PairLoweredWordsWith1\n",
    "    input: FindWords\n",
    "    config:\n",
    "      fn: 'lambda word: (word.lower(), 1)'\n",
    "\n",
    "  # Using CombinePerKey transform with the 'sum' function as a combine function,\n",
    "  # we'll calculate the occurrence of each word.\n",
    "  - type: CombinePerKey\n",
    "    config:\n",
    "      combine_fn: sum\n",
    "    name: GroupAndSum\n",
    "    input: PairLoweredWordsWith1\n",
    "\n",
    "  # Select 10 most popular words. Input format to this step is a tuple (word: count),\n",
    "  # so we provide the count (row[1]) as the key to compare the numbers\n",
    "  - type: TopNLargest\n",
    "    name: MostPopular\n",
    "    input: GroupAndSum\n",
    "    config:\n",
    "      n: 10\n",
    "      key: 'lambda row: row[1]'\n",
    "\n",
    "  # Save results to a text file.\n",
    "  - type: WriteToText\n",
    "    name: SaveToText\n",
    "    input: MostPopular\n",
    "    config:\n",
    "      file_path_prefix: \"data/result-pipeline-03\"\n",
    "      file_name_suffix: \".txt\"\n",
    "'''\n",
    "save_to_file(pipeline, 'pipeline-03.yaml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the pipeline:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run('python -m apache_beam.yaml.main --pipeline_spec_file=pipeline-03.yaml')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, view the output:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run('head data/result-pipeline-03-00000-of-00001.txt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "Congratulations! You've just run Apache Beam pipelines using YAML.\n",
    "\n",
    "For all the available operations visit the documentation: # todo(yaml) add url\n",
    "\n",
    "For a list of available transforms, visit # todo(yaml) add url\n",
    "\n",
    "To run your pipeline in Dataflow, you'll need to set up your Google Cloud and run the pipeline with the `DataflowRunner`. For more information, follow https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#run-on-dataflow"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
